<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>宽客人生：从物理学家到数量金融大师的传奇</title>
      <link href="/2018/07/30/%E5%AE%BD%E5%AE%A2%E4%BA%BA%E7%94%9F%EF%BC%9A%E4%BB%8E%E7%89%A9%E7%90%86%E5%AD%A6%E5%AE%B6%E5%88%B0%E6%95%B0%E9%87%8F%E9%87%91%E8%9E%8D%E5%A4%A7%E5%B8%88%E7%9A%84%E4%BC%A0%E5%A5%87/"/>
      <url>/2018/07/30/%E5%AE%BD%E5%AE%A2%E4%BA%BA%E7%94%9F%EF%BC%9A%E4%BB%8E%E7%89%A9%E7%90%86%E5%AD%A6%E5%AE%B6%E5%88%B0%E6%95%B0%E9%87%8F%E9%87%91%E8%9E%8D%E5%A4%A7%E5%B8%88%E7%9A%84%E4%BC%A0%E5%A5%87/</url>
      <content type="html"><![CDATA[<p>当你研究物理学的时候，你的对手是上帝；当你研究金融学时，你的对手是上帝创造的人类。</p><p>——伊曼纽尔.德曼<br><a id="more"></a></p><p><img src="/2018/07/30/宽客人生：从物理学家到数量金融大师的传奇/宽客人生.jpg" style="zoom:50%"></p><p>序言 两种文化</p><p>宽客从事的主要工作的是“金融工程“（quantitative finance），这个学科是跨学科的混合体，包括物理学模型、数据技巧和计算机科学等，目的是为了对金融证券进行估值。最佳的数量金融学时间洞察了证券价值与不确定性之间的关系，并接近真正的科学；糟糕的实践则是缺乏有效论证的复杂数学模型的伪科学大杂烩。</p><p>物理学家大批涌入其他领域就职的部分原因在于，20世纪70年代他们传统的就业市场——学术领域工作萎缩了。……</p><p>最成功的的理论<br>目前金融世界中最著名也是应用最广泛的模型就是布莱克-斯科尔斯（Black-Scholes）期权定价模型。</p><p>第一章 因缘际会</p><p>1965年末</p><blockquote><p>尽管为了完成博士论文花费了很长时间，但我并没有真的遗憾过；从某种程度上来讲，我为其中的努力而骄傲。我在那些年学到的东西——百折不挠的韧性和数学知识，对我无论是在华尔街还是在学术界都非常有帮助。在任何领域内，只要有人先验发现新大陆，他就要花费多年去思考、不断试错。在歧途中徘徊、在误区中跌跌撞撞，最终还要站起来继续前行。从这个角度来说，获得博士学位是一个很好、很痛苦的过程。</p><p>尽管我把大部分的精力都花在物理上，但整整奋斗了7年才得以被人注意。我用爱因斯坦67岁写在自传体笔记中的几句话来安慰自己，这几句话是关于他参加期末考试所带来的后续影响的，“（对于我来说）这种强迫具有一种阻碍作用，我发现通过期末考试之后，我在真正一年内对任何科学问题的思考都让我觉得反感。“</p><p>博士后生活是一种“返祖现象”，是很久以前一个时代留下的印迹。创设博士后研究职位是为了给那些已经研究生毕业但还没有当上教授的人提供一个短暂的过渡。但在苏联抢先发射卫星上天后，美国将科学看成是精神上同苏联的另一场战争，结果就是产生了大量年轻科学家，这些科学家已经取得了终身教职，而且他们至少在30年内不可能退休。教师需要学生，于是就鼓励有抱负的物理学研究者进入博士生培养流程，但当这些学生从教育流程末端涌现出来的毕业的时候，几乎没有地方可去了。博士后职位暂时填补了这一空缺，只需要两年时间，而且报酬微薄。但这种机制对大学而言非常有效，大学每年都能新招收一批年轻的物理学研究者，一旦有空缺出来的教师职位，大学就可以在这群年轻的学者中挑选非常出色的人来填补。</p><p>我的一些博士朋友，对留在物理学界从事研究充满热情且基督热爱，即使拿不到任何薪水，成为“免费赠品”（freebies）。这个词表示那些在任何地方都没有找到工作，于是在一流研究机构中申请一张桌子和基本研究设备、不要求任何报酬做研究的人。他们这么做的目的就是处在一个激励人的环境中，与其他学者保持紧密的联系，然后完成一篇足以让他们获得带薪职位的论文。甚至我有一个朋友，居然拒绝了一份二流学术机构提供给他的带薪博士后的职位，而成为哈佛大学的“免费赠品”。在哈佛，他顺利完成了一些研究，这些研究使他得到了一流研究机构——斯坦福大学加速器中心的带薪职位。</p></blockquote><p>人智学会（Anthropological Society）《更高世界的知识》<br>《弗洛伊德与人的灵魂》<br>史代纳</p><blockquote><p>就像艺术和音乐的美通常出自于在已接受的框架的限制内创造新事物的张力一样，很多理论物理学的深刻理论都来自于非常通用的指导原则限制下试图描述自然界的法则。如果一个人非常幸运又非常聪明，通用指导原则将排除其他一切，只剩下一个真实的理论。</p><p>我形单影只的个人生活强化了理论物理学家和学者研究工作的孤独感。为了进行持续的研究或计算，你可能老是拿脑袋往墙上撞，试图压制需要人陪伴的想法。我厌恶独处，并羡慕那些日常工作中可以和其他人打交道的人。多年以后，我发现宽客的日常生活十分富足且不像学者生活那么孤单。可以想象出作为宽客每天的工作：要与其他宽客讨论，阅读理论，与交易员交流，进行软件编程，与客户谈话，向聪明但不懂数学的交易员讲解=复杂的概念。直到我经历了这种生活，我才开始相信投资银行比大学还想大学。</p><p>根据托尔斯泰的说法，“业‘是指有着因果关系的对罪孽的赎罪行为，就在那时我想我想明白了”业“的含义。命运要你放弃虚荣、野心和奥曼，来信奉”上帝“。心甘情愿地主动做这件事是最好的。但如果你没有，那么”业“，也就是命运日常的运行方式，就会慢慢地、固执地磨平你的虚荣，剥掉你虚荣与自以为是的外衣，就像自动削皮机中的土豆一样，直到你听命为止。</p><p>当你认真检视一件事物，足以将其“质“和”量”统一起来时，任何事情都是很有趣的。当你对某个领域的细微之处足够熟悉，而且开始尝试着将将它的形式和它的应用连接起来的时，任何领域都是很吸引人的。应用物理学能够提供各种各样的任务，提供一个长期理论问题和短期现实问题的组合，提供一个理论指导实践并从中得到乐趣的机会。简而言之，它能够使人在独自研究的执着境界和与人接触的鲜活世界之间随时更新，从容往来。</p><p>叔本华在他的《随笔和箴言集》（Essay and Aphorisms）中写道：“你能做你想要的，但你不能想要你想要的。”当时我正开始阅读这本书，我认为这句话是对的。我已经不能再像以前喜欢研究物理那样想要去研究物理了。在自我挣扎的过程中，我开始体会到叔本华那冷酷无情、愤世嫉俗的世界观和方法论了，所有这些内容都用凝练的短语表达得如此漂亮，读起来就像诗一样。我永远忘不了。叔本华愤世嫉俗的分析与优雅的方式远远胜于史代纳那种对真理不加解释而做出的拙朴的表达，而且这种分析与方式又给人一种更加冷静、客观的慰籍。</p><p>差不多也就是看爱德华所设计和编写的代码时，我意识到很多物理学家是如何误解非学术世界中工作和职业性质了。物理学家总是认为自己很聪明，一旦自降身段从事了“外面世界”的工作，他们的聪明才智能让他们只需要朝九晚五地工作，还能超过其他同事。但是在很多非学术工作中，总是有这样一些人：对他们来说，特定的工作并不是一种妥协，而是一种激情、一种投入，他们非常认真的看到这份工作。是他们，而不是那些聪明但漫无目的的混日子的物理学家，给卓越确立了标准。</p><p>无论如何，正是那些不可预测的“我”们，像你和我一样的人们，决定了金融价值。费希尔·布莱克曾将金融理论写成：</p><blockquote><p>理论最终被接受，并不是因为它被传统的实证检验所验证，而是因为研究者们说服了其他研究者们，使他们相信这里理论是正确的和重要的。<br>我愿意阐述的更深入一些，从交易员合作者角度来看，我喜欢将金融模型水位类似量子物理学家和相对论物理学家在20世纪早期所做的那些想象试验（gedanken）。想象试验，在德国称之为思想实验（though experiment），是想象中的研究，是一种在精神世界里对物理世界做的眼里测试。这种试验之所以在你大脑中进行，是因为实践起来太难而不可能真的操作。他们的目的就是强迫你将关于世界的概念变成一个矛盾体。哎一实验为了深入洞察信仰牛顿学说的观察者和麦克斯韦对光的描述之间的矛盾，曾想象蛋挞坐在一定光速边缘的时候，他将会看到什么。当你坐在一个波峰上时，时候光波看上去仍会是从波峰向波谷变动的？同样的，薛定谔为了强调量子力学完全与直观相反的特性，想象一只看不见的猫，被密封在一个装有放射性原子的盒子里，放射性原子持续衰变，会触发一个盖格尔计数器（Geiger counter）从而释放出毒气。那么这只猫会像不可预测到的、在不同量子形态之间不断来回转换的原子那样，在生死之间不断轮回？</p></blockquote><p>我想这才是将数学模型应用于金融学的正确方式。模型只是模型而已，并不是事情本身。因此，我们不能指望它们是真正正确的。模型最好被视为一组你能研究的平行的实现领域。每个思想领域都应该是一致的，但真实的金融和人类世界与物质世界不同，比我们用来了解它的任何模型都要无限度地更加复杂。我们总是尝试将真实世界硬塞进一个模型中，想看看这个模型是多么有用的一个近似形式。</p><p>我喜欢按照歌德的风格来思考我们在数量金融学领域所做的事情：我们尝试对我们观察到的事物做优美的、如实的描述，这个过程中涉及直觉感知、发明创造已经编制近似的规则和模式。我们在创造理解方面，融合了艺术和科学。我们运用我们的自觉、我们的科学知识已经我们的技巧来描绘出如何进行定性思考，并在一定限制内量化分析人类事务的世界，在这样的过程中，我们影响着其他人的想法，也被其他人的想法影响着。如果没有希望，人在生命中也就不会祈求太多。</p></blockquote>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
          <category> Finance </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>欢迎来到我的博客</title>
      <link href="/2018/07/30/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/07/30/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>这里写摘要。test<br><a id="more"></a></p><p>这是我的第一篇博文，测试用。</p><ul><li>43243242342</li></ul>]]></content>
      
      <categories>
          
          <category> Sports </category>
          
          <category> Baseball </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 主题测试 </tag>
            
            <tag> Injury </tag>
            
            <tag> Fight </tag>
            
            <tag> Shocking </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/07/30/hello-world/"/>
      <url>/2018/07/30/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>“存在主义”的荒诞表述</title>
      <link href="/2018/07/10/%E2%80%9C%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E2%80%9D%E7%9A%84%E8%8D%92%E8%AF%9E%E8%A1%A8%E8%BF%B0/"/>
      <url>/2018/07/10/%E2%80%9C%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E2%80%9D%E7%9A%84%E8%8D%92%E8%AF%9E%E8%A1%A8%E8%BF%B0/</url>
      <content type="html"><![CDATA[<p>在国企上的这两年班，尤其是经常性的出差，全国各地的奔波，发现这真的是一种适合思考哲学的生活状态，尤其是在飞机上透过舷窗看着平流层下云舒云展，或者高铁上看着阡陌交通，这个时候哲学灵感尤为强烈。</p><p>单位距离单位有二十公里的距离，又因为上班的必经之路是交通要道，所以每天按点起床仍是告别学校之后一项严峻的考验，提示我们生活的第一件事就是违背自己的意愿。</p><p>一个相信绝对真理的理性主义者来到现实世界的大荒漠之后，很容易就被改造成一个存在主义的信徒。因为发现所有的书面真理遭到鲜活的生命之后就会原形毕露，正如存在主义的先驱克尔凯郭尔不满于黑格尔哲学大厦无法提供安身立命的教诲而转向了主管性，赋予自身生存状态以本体论的意义。</p><p>我的室友阿诺，最近就陷于存在主义的荒诞之中，这个自由派青年出于工具理性光荣地成为了一家百年国企的文宣员，每天都在拷问自己人生的意义，并且提出了更具有本体论色彩的问题：我为什么没有钱？是不是二十五岁的我，注定要为没钱而流眼泪？</p><p>这个时候我就会给他背一段高中课文：“盖文王拘而演《周易》；仲尼厄而作《春秋》……“只有幻灭才能成就伟大的作品，把生活经历淬炼成艺术，说不定救火了，然后实现财务自由。接着，我又背一段高中课文：”天将降大任于斯人也……“于是微信群里充满了快活的气息。</p><p>但后来我发现，为了掩饰庸常而建构的意义，并不能解决阿诺的精神问题，在进入国企前，他也曾是个深邃而富有批判精神的青年，很容易识破所有忽悠人的意义。既然建构伟大不管用，我就改变策略，开始悬念直面残酷，怂恿他辞职，去追寻我被没有选择的人生——做个好记者。但很遗憾，阿诺寻死觅活的俩个月依然没有辞职，而我终于也明白了他就像《邪不压正》里面的李天然，十几年心心念念想要报仇，但真正面对仇人的时候却无法克服内心的恐惧，虽然我觉得姜文对这一心理状态的刻画非常庸俗。阿诺不敢辞职的原因更加庸俗——户口和房租。</p><p>最近房租又涨了，舆论场上充斥着“失控的房租正在榨干年轻人”的悲情控诉，而这些控诉又反过来拔高房东和房客对于房租的预期。正如黑格尔名言所说：“一切重大的历史事件和人物，一般来说都会出现两次。”黑格尔的意思是一旦同样的事情再次发生，那就意味着，这件事情具有深层的历史必然性。</p><p>两年前我读研究生一年级的时候管我们这代大学生叫“预备中产阶级”：“其特征是受过良好的高等教育（985、211毕业生），在大众媒体时代掌握知识和话语权，却没有相应的经济支撑（焦点在房产）的青年群体，他们对中产阶层生活方式进行模仿，对大资本控制展现批判精神，对底层表达仪式性同情。而今天弥漫在这批青年中的不满，是因为预备中产‘转正为中产阶层的渠道被凝滞了。”而今天，文化资本、受教育水平、媒介话语权——它还有许许多多的名字——与经济地位之间的龃龉愈发凸显，于是北京焦虑被放大为中国问题，“北京之所以能焦虑，同样是交通和资讯发展的结果，让无数人站在这里，试图抛弃身后的中国，瞭望新的中国。”</p><p>写这篇文章的时候正值房价又一轮暴涨，但当时我毕竟住着中关村1080元/年的学生宿舍，正如金庸在《倚天屠龙记》后记里所写：“……书中写得太也肤浅了，真实人生中不是这样的。因为那时候我还不明白。”后来自己租了房，拿到第一个月惨不忍睹的工资，再看这些问题，便无法置身事外，多了一些窘迫，少了许多从容，也时不时在出租房或者通勤路上发出许知远式的质问：“读圣贤书所为何事啊！”当然许老师已经实现了财务自由，不会在这等庸俗层面上提问。</p><p>写到这里，有人可能以为，阿诺是我杜撰出来借以自我说服的人物，但实话说不是的，阿诺是我活生生黑胖胖的室友，最近在办公室坐多了可能还胖了好几斤，而我说的也都是真事，并没有用什么文学修辞。</p><p>但我确实有自我说服的工作要做：全中国最好的高等教育不但不能让你月薪两万，而且不能让你自信地面对生活与未来，反而可能成为困扰的来源，让财务理想和薪资现实之间的落差更加扎眼。所有的哲学词汇遭遇生活的时候都很可能完败，存在主义的终极表达就是：X你X的！<br>虽然我清醒地认识到，当初无论我选择哪条路，读博士也好，做记者也好，做公务员也好，进国企也好，都会面临今天的困境。个人的困扰总是和时代的症候铰接在一起的，何况你站立的地方是北京。</p><p>而所有高级低级的鸡汤故事都在告诉我们，人生起伏，有崖边跌落的挫折，也有插画游街的风光，沉潜而不气馁，鱼跃只需尽力，总有办法突围前行。</p><p>我采取的第一个突围方式是做菜，寄情于庖厨之间。工作两个月后，我已经学会了熟练煲火腿汤，做剁椒鱼头，煎牛排，还有功臻化境的蛋炒饭。从今天起，关心粮食和蔬菜，土豆0.99元/斤，西瓜前段时间是0.79元/斤，现在天气转冷是1.39元/斤。拥抱生活，虽然我刚X他妈的。</p><p>第二个突围方式是读书、写稿、写论文、跟人讨论问题，假装自己从未远离知识和媒体。我永攀文学高峰，读完了腰封上号称“超越《百年孤独》的惊世之作”《2666》，用我读博同学的话说，读完这本书基本属于“国家顶尖人才”了；我乐此不疲地在微信群里跟我导师争论，仿佛自己仍然是那个在讨论课上剑拔弩张的思想青年；我研究起新时代的通三统，搞一搞具有汉唐气象的马克思主义儒学（简称唐马儒）；我为媒体撰稿补贴家用，和朝九晚五的健康作息抗衡，抵抗下班回家胖瘫的诱惑，在午夜敲打着并不伟大的作品。而这些努力的意义都如阿兰·德波顿所言：“借由那些更为沉重和骇人的事件，我们得以将自己从琐事中抽离，让更大的命题盖过我们方寸前的忧虑和疑惑。”</p><p>第三个突围方式是理解并且热爱自己的本职工作。我开始理解国企的工作方式，这是一个看起来并不美好的科层制体系，设定目标，拆解任务，然后各人完成各人的部分，连写稿亦如是，并不需要什么天才，但如此反而是一种高效且差错最小的工作方式，与工业社会相契合，这是一个人力不可抵抗的理性化过程。但正是这个生产制度推进了工业化，提升了人均寿命和粮食产量，完成了最本质的“社会进步”。相比之下，学院里的论文写作实在像是小农社会的知识生产，而许多智识优越的教授可能是一群小农知识分子，而大学，至少是文科，可能是农业社会最后的堡垒，它们守卫着那些伟大的灵魂和文本，等待它们命定的读者顺手拿来激活我们时代的困扰。</p><p>最后也是俗气而无用的突围就是规划未来。料你出走半生，终将归来读博。但我依然不觉得毕业决心走出学校是一个错误，尽管立志做个知识青年，书读到我当时那个阶段，已经遭遇了瓶颈，未曾经历的生活不值得审察，所以需要来这花花世界历练一番。</p><p>这里要引用我很喜欢的赖特·米尔斯的名言，他说：“具有社会学的想象力的人，就更有能力在理解更大的历史景观时，思考它对于形形色色个体的内在生命和外在生涯的意义。”</p><p>国企的工作让我慢慢觉得，社会科学最难的题目，恐怕是搞清楚五百块钱对形形色色的个体来说意味着什么？对通勤地铁上哈欠连连的白领意味着什么？对出租房楼下收废品乐呵呵的大妈意味着什么？对于永辉超市里的收银员意味着什么？它涵盖了经济运行、阶级利益、社会分配、群体心理、媒介感知等一系列社会科学问题，搞清楚了就是一部出色的《中国社会各阶级的分析》。换而言之，能够比较准确衡量五百块钱对于某个个体或群体的分量，是社会学的终极想象力之一。这个问题有很多现实应用领域，比如个税起征点改革、社保计提比例调整到底会导向什么样的社会情绪。之所以想这个问题也并不仅仅是因为我真的很穷。</p><p>但我至今还没问过楼下收废品大妈五百块钱意味着什么，“社会学的想象力”止步于想象，无法给出任何具有扎实调查基础的解释和结论。如果我真的是一个社会科学家，这就意味着一种社会科学的失能。但遗憾的是，这种失能症弥漫于当代知识界，喋喋不休争吵各方的批判与想象总是先于理解和解释，而这成为社会科学的阿克琉斯之踵，把社会科学家变成理工科口中“聒噪的文人”，也时时威胁着社会科学的存在基础。所以我一头扎入了生活。</p><p>我依然怀念2016年冬天和阿诺在南方某城市参加学术会议的日子，那时候的我经过了几个月的苦闷，突然用“预备中产之殇”打开了所有的思路，好像恨不得一下子燃烧尽自己的才华与创造力，一天能写出五千字的论文、随笔或者讲稿。我记得那时候我和阿诺说，以后我们要做中国的默顿和拉扎斯菲尔德，你负责调查研究，我负责理论建构。广东的12月居然可以穿短袖。</p><p>希望我们归来依然是少年。</p>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>解读量化投资：西蒙斯用公式打败市场的故事</title>
      <link href="/2018/07/05/%E8%A7%A3%E8%AF%BB%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%EF%BC%9A%E8%A5%BF%E8%92%99%E6%96%AF%E7%94%A8%E5%85%AC%E5%BC%8F%E6%89%93%E8%B4%A5%E5%B8%82%E5%9C%BA%E7%9A%84%E6%95%85%E4%BA%8B/"/>
      <url>/2018/07/05/%E8%A7%A3%E8%AF%BB%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%EF%BC%9A%E8%A5%BF%E8%92%99%E6%96%AF%E7%94%A8%E5%85%AC%E5%BC%8F%E6%89%93%E8%B4%A5%E5%B8%82%E5%9C%BA%E7%9A%84%E6%95%85%E4%BA%8B/</url>
      <content type="html"><![CDATA[<p>《解读量化投资：西蒙斯用公式打败市场的故事》由机械工业出版社于2010年1月1日出版。本书讲述詹姆斯·西蒙斯，基金领域的拓扑学大腕，成功取代保尔森的对冲之王，20年内最佳赚钱基金经理，在投资界掀起了一场量化投资的狂潮。《解读量化投资：西蒙斯用公式打败市场的故事》用轻松、幽默的讲故事手法，解读了西蒙斯量化投资“黑箱”之内的秘密。通过深入浅出地回顾西蒙斯的投资布阵，比较西蒙斯与巴菲特投资模式的迥异，分析投资领域技术分析方法和宏观分析方法的优劣，《解读量化投资：西蒙斯用公式打败市场的故事》带我们走近了20年中平均每年总回报为80%的大奖章基金，看看它如何能将1万元变成1亿元。用数学公式打败市场，投资并非悬而未决的事情——这就是《解读量化投资：西蒙斯用公式打败市场的故事》揭示的投资之道。<br><a id="more"></a></p>]]></content>
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>高频交易员</title>
      <link href="/2018/06/11/%E9%AB%98%E9%A2%91%E4%BA%A4%E6%98%93%E5%91%98%EF%BC%88Flash%20Boy%EF%BC%89/"/>
      <url>/2018/06/11/%E9%AB%98%E9%A2%91%E4%BA%A4%E6%98%93%E5%91%98%EF%BC%88Flash%20Boy%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>《说谎者的扑克牌》《大空头》作者迈克尔•刘易斯最新作品</p><p>英国《金融时报》和麦肯锡2014年度商业类入围好书</p><p>美国上市一周，狂销13万册！</p><p>2014年苹果电子书店下载，非虚构类第一名！</p><p>一场运用网络交易技术与精密算法的大对决</p><p>一场抗衡超级金融系统与权贵人士的大挑战</p><p>一场直面股市交易内幕与监管之惑的大辩论</p><p>千分之十三秒能做什么？你还来不及眨一次眼睛，但对于高频交易员而言，足够完成一次交易。</p><p>美国有多少个股票交易所？你很可能以为，只有两家：纽约证交所和纳斯达克。错！今天，美国股市有超过13个公开交易所，此外，几乎各大券商都有属于自己的秘密交易平台，俗称“暗池”。</p><p>这是一个隐蔽且被操控的市场，高频交易员利用纳秒级（十亿分之一秒）的时间差，捕捉“猎物”，快速买入卖出，谋取暴利。这是“暗池”里的“游戏”，速度就是一切，为此，他们甚至开山破土，建起了一条有史以来最笔直的数据传输光缆。</p><p>一群华尔街的奇异人士发现了这个秘密，他们是一群智力超群的反叛者，他们密谋揭秘金融市场的疯狂，揭露华尔街上那些大银行、股票交易所、高频交易公司赚取暴利的新把戏。《高频交易员》曝光了全球金融市场中不为人知的内幕，对当今世界快速演变的金融工具进行反思，并回答了这样一个问题：到底是谁在操纵市场？<br><a id="more"></a></p>]]></content>
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>纳西姆</title>
      <link href="/2018/05/30/%E7%BA%B3%E8%A5%BF%E5%A7%86/"/>
      <url>/2018/05/30/%E7%BA%B3%E8%A5%BF%E5%A7%86/</url>
      <content type="html"><![CDATA[<p>Nassim Nicholas Taleb（NNT）</p><p><strong>NNT的精髓是“求知” - Epistemology</strong></p><p>NNT是一个什么样的人？</p><p><strong>黎巴嫩名门之后</strong>：父母的家族都是显赫的政治和学术世家；<br><strong>自学成才(Autodidact)</strong>：虽然他有很高的学历，但年轻时代痴迷的是古典著作而不是数理统计，据他自己讲，数理统计主要都是自学的。<br><strong>多语言掌握者(Polyglot)</strong>：能讲法语，英语，阿拉伯语，西班牙语，意大利语；至少还能额外看懂古希腊语，拉丁语，亚拉姆语，古希伯来语；<br><strong>成功的，不按常理出牌的交易员(Doer)</strong>：20出头开始做交易员，靠自己的研究成功：1987年市场崩盘时，27岁的NNT就已经实现财务自由。他为交易者提供黑天鹅事件的保护，在最近的数十年每次的大崩盘他都赚了大钱。<br><strong>癌症康复者</strong>：NNT在90年代患上喉癌，在自己研究的支持下康复。 畅销书作家：Fool by randomness与The Black Swan，不必多讲。他还有一本自己写的谚语书The Bed of Procrustes，当然还有大作Antifragile。<br><strong>学者</strong>：全职教授，无数客座教授；<br><strong>阅读狂人</strong>：从13岁起，每周读书30到60小时； </p><p>上述标签中最重要的两个，是Autodidact和Doer。第一他不是传统系统里的学者，第二他不是只讲不做，靠嘴皮子吃饭的Charlatan。（NNT说，听任何人讲理论，你要看他是不是敢对讲话的内容付私人的财富上的责任，这叫做Skin in the Game，如果不是，那他的理论要打巨大的折扣。） </p><p>现在来说Epistemology。求知，这很难写，因为他所提出过和谈到过的各种理念，一是我没这个水平，二是这会剥夺同学们跟随他进行思维飞行的乐趣。所以在这里我想以举例的形式来介绍一下NNT的求知过程，把大家谈得比较少的概念讲一讲，那就是Via Negativa, or the negative way。《黑天鹅》的作者纳西姆·塔勒布是怎样一个人？</p><p>Via Negativa是神学的一个理念，跟随这个理念的教义并不试着解释神是什么，而主要解释神不是什么。举个例子来说明，知乎上有很多人来问过我，投资是什么，PE是什么，能不能用一句话解释。这是效率很低的做法，对于提问者来讲，其实最有价值的问题是，投资不是什么。</p><p>为什么？因为负面的信息远远比正面的信息有力量。比如，一个想知道投资是什么的人，其实是想知道进了投资界会不会比较自由，有没有一夜暴富的机会，能不能变成高帅富开超跑泡女模。《黑天鹅》的作者纳西姆·塔勒布是怎样一个人？</p><p>那么你只需要知道投资界大多数人的生存状态，就能做出正确的决定，比听这个人说说投资方法论，那个人说说投资心得，第三个人八一八海天盛宴要准确得多，有效率得多。这也是”黑天鹅“的由来：就可以得到如何在不可预知的世界中生存的方法：Barbell Strategy。以Barbell的眼光看世界，就可以看到如何主动打造出Optionality，最后成功实现Antifragile：既然世界不可知，那么就让这些不可知对我们有利吧！</p>]]></content>
      
      <categories>
          
          <category> Peoplel </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>黑天鹅</title>
      <link href="/2018/05/14/%E9%BB%91%E5%A4%A9%E9%B9%85/"/>
      <url>/2018/05/14/%E9%BB%91%E5%A4%A9%E9%B9%85/</url>
      <content type="html"><![CDATA[<p>在发现澳大利亚的黑天鹅之前，欧洲人认为天鹅都是白色的，“黑天鹅”曾经是欧洲人言谈与写作中的惯用语，用来指不可能存在的事物，但这个不可动摇的信念随着第一只黑天鹅的出现而崩溃。<br>黑天鹅的存在寓意着不可预测的重大稀有事件，它在意料之外，却又改变一切。人类总是过度相信经验，而不知道一只黑天鹅的出现就足以颠覆一切。然而，无论是在对股市的预期，还是政府的决策中，黑天鹅都是无法预测的。“9·11”事件、美国的次级贷危机、我国的雪灾，都是如此。<br>生活中，随机性随处可见，在资本市场也是一样。人们总是以自己有限的生活经验和不堪一击的信念来解释不可预测的事件；即便是精于算计的专业人士，也难保不被随机性愚弄，其实我们应该做的是顺应这种不可知的未来。本书会教你改变自己的思维方式，把握黑天鹅带来的机会，采取应对策略，从中受益。这本书将改变我们对世界、人性和金钱的看法。<br>大约在四百年前，弗朗西斯·培根就曾经发出这样的警告：当心被我们自己思想的丝线丝丝束缚。但是我们老是犯这种错误，老是以为过去发生过的事情很有可能再次发生，所以免不了会凭经验办事。比如说，我们经常编出简单的理由或故事来解释我们尚不知晓（而很有可能是我们根本就不可能知道的）复杂的事情。举个简单的例子：我们无法预知在未来的某一天股市会涨还是会跌，据以推断预测的理由要么过于简单化了，要么根本就是错误。事实上，真正重大的事件是无法预知的，Nassim Nicholas Taleb把这称之为“黑天鹅”。作者的这一论点，对于那些靠预测经济发展为业的MBA甚至曾获得过诺贝尔奖的专家们而言，可以说是沉重的一击。这在 Nassim Nicholas Taleb已经不是第一次了，在他的上一部作品《被随意性愚弄》（曾被翻译成十九种语言出版，是当年的最畅销书之一）一书中，就曾发出这种声音。而今在《黑天鹅》中，他把重点进一步聚焦在自然科学最无所作为的一点——预知未来。并非只有华尔街才需要预测未来，事实上我们每个人都常常有意无意地做预测—— 当你决定买保险的时候，或者系上安全带的时候，等等。<br>“黑天鹅”有着高度不可能事件所应具备的三个特征：第一点是不可预知性；第二点是它所带来的影响是巨大的；第三点是，在此之后，人们总是试图编造理由来作解释，好让整件事情看起来不是那么的随意就发生了，而是事先能够被预测到的——通过这样那样的分析。Google公司所取得的惊人的成功就是这样的一只“黑天鹅”，美国9·11事件也是。对于Nassim Nicholas Taleb而言，黑天鹅无处不在，几乎是世界上任何事情的基础——从宗教的兴起到我们每个人的私生活。<br>那么，为什么在黑天鹅被发现之 前，没有人去设想一下其他颜色的天鹅也有可能存在呢？Nassim Nicholas Taleb 给出的答案是：在应该关注共性的时候，人类错误地关注着特性，反之亦然。受思想的束缚之害，我们总是习惯于重视已知的事物，而忘记了去想想为什么有那么多其他的事物我们还不了解。多年来，Nassim Nicholas Taleb一直致力于研究人性的弱点之一，就是自欺欺人的——我们以为自己知道的很多，而事实上我们真正知道的东西很少。在许多大事件陆续发生并改变着我们的世界的时候，我们却还像鸵鸟一样把头埋在沙堆中，禁锢着自己的思想。因此，我们总是习惯于将事物作简单的归类处理，一味将可能性比较小的概率事件归结为不可能那一类，也就难怪会有那么多“黑天鹅”出现了。然而，Nassim Nicholas Taleb毫不留情地指出：历史不是徐徐行进的，而是活蹦乱跳式的。从以往的事件中归纳总结妄图加以解释的做法，不过是为了获得心理上的满足和安慰感罢了，谈不上任何实用性。比如说9·11，比如说股票市场的突然大跌甚至股灾。<br>在《黑天鹅》这一堪称具有革命性意义的书中， Nassim Nicholas Taleb研究了高度不可能事件以及不可预期事件的强大的影响力，相当精彩且极具启发性——它很有可能改变你看待这个世界的方式——好多我们自以为非常清楚确定的事情，在看过《黑天鹅》之后你会发现实际上我们一无所知。但是也不必悲观，作者同时还告诉了我们对付“黑天鹅”的诀窍，处理得当的话还有可能变被动为主动，甚至从中获益。运气、不确定性、可能性以及认知，是Nassim Nicholas Taleb穷其一生研究的问题，其自然科学、经济学以及统计学背景，在《黑天鹅》中表现得淋漓尽致，驾驭起来显得游刃有余。不仅如此，Nassim Nicholas Taleb还是一位懂得娱乐的作家，文字一如既往地延续了其探索性的随意风格，充满着智慧、调侃和戏谑。可以说，是Nassim Nicholas Taleb成就了《黑天鹅》，而《黑天鹅》本身就是一只“黑天鹅”。<br><a id="more"></a></p>]]></content>
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>哲学家都干了什么-下</title>
      <link href="/2018/03/30/%E5%93%B2%E5%AD%A6%E5%AE%B6%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BB%80%E4%B9%88-%E4%B8%8B/"/>
      <url>/2018/03/30/%E5%93%B2%E5%AD%A6%E5%AE%B6%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BB%80%E4%B9%88-%E4%B8%8B/</url>
      <content type="html"><![CDATA[<p>【读书笔记】：哲学家们都干了些什么？ 下<br><a id="more"></a></p><p>　　第二次世界大战之后，社会格局、人类思想变得越来越多元，没有什么能统治全人类的思想，各种新思潮层出不穷。我们看不到某种形而上学统治人类思想的样子。<strong>从这个角度上说，尼采才是伟大的预言家。他要毁灭传统的价值观，写了一本《重估一切价值》。二战以后的现实确像他所说的，所有的传统价值都崩溃了，一切价值都应当重估。</strong><br>　　<br>　　在近代，建立在理性基础上的科学创造了各种人间奇迹。这是理性蕴含无穷力量的最好证据。多亏科学的成就，人类在历史上从没有像最近两百年这么自信过。在今天，谁能不相信科学的力量呢？<strong>罗素说，如果有一个国家完全不相信物理学，那么另一个国家只需要靠几个物理学家就可以把前一个国家灭了。换句话说，你要是不相信科学，你在这个世界上连生存都谈不上，就更别提其他了。</strong><br>　　<br>　　我们来看看在第一次世界大战之前，科学的新发现为我们提供了什么有用的东西。<strong>首先是物理学的进步。</strong> 力学在牛顿之后两个世纪里基本没受到什么质疑。很多物理学家认为牛顿力学已经揭示了世界的真实面目，此后的物理事业没有太多可发展的余地了，不过是修修补补、把物理数据弄得更精确点罢了。因此有科学家说过，牛顿既是天才也是幸运的，因为只有一个宇宙可以供人发现，而牛顿已经把最重要的规律都发现了。不过即便在牛顿力学的影响下，物理学仍旧有重要的发展，就是能量守恒和质量守恒定律。这两大定律进一步扩大了物理学的影响力，再次让人们发现，人体和其他无机物在物理上没有什么区别。<strong>第二个是进化论的发现。</strong>科学将触角伸向了有机体。我们待会儿专门拿出一章来讲。第三个是心理学的发展。 这回，科学直接染指精神领域了。 现代心理学使用的是科学的研究方法，研究成果很丰硕，很多原本神秘的心理活动如今也有了清晰的规律。人们已经可以适当地干涉、改变人类的内心活动。比如“巴甫洛夫的狗”，大家都知道吧，巴甫洛夫一边喂狗一边摇铃，最后训练得狗一听铃声就会流口水。这就等于可以用科学的方法去改变动物——也包括人类——的本能，使得人类对生物的干涉前进了一大步。这样一来，人们对科学的自信心变得出奇的高。人们相信，只要假以时日，科学可以解决一切问题。就算是艺术、哲学那些过去被认为科学难以碰触的领域，将来运用<strong>心理学</strong>也可以解释了。</p><p>科学的发展给哲学带来了两个影响：第一个影响是把宗教完全打趴下了。 科学打击宗教的方法主要有两个，一个是不断创造科学奇迹增加人类的自信心；一个是公布各种和《圣经》记载矛盾的科学结论。就比如日心说之于地心说，进化论之于神创论。</p><p>更何况，科学可以创造出各种人间奇迹。宗教一直号称神迹可以让人们获得丰收、治愈疾病。但在近代，是科学不断在提高粮食产量、治愈疾病，而祈祷并不是每次都管用。</p><p>科学发展的第二个影响是，随着科学的触角越来越广，机械论和决定论必然重新抬头。就像前面说的，随着科学成就的增加，人们相信科学可以解决一切问题。甚至有人提出来，以后没必要有哲学这个科目了。哲学问题不过都是人的心理活动而已，以后哲学只作为心理学的一个分支就足够了。这种用科学代替哲学的想法，被称为“科学主义”。<br><strong>达尔文</strong>之牛，和欧几里得一样，不仅因为他设计了一个超强的理论，还因为他的理论在后世几百年中不断被人们攻击、讨论。结果越讨论，证明它正确的证据就越多。</p><p>　　进化论的关键内容有这么几条：第一，生物的基因信息可以遗传给下一代： 第二，在遗传的时候，基因会发生不可控制的随机变异； 第三，整个生物种群都面临着巨大的生存压力，每一代新生物的数量却大于自然资源能够供养的数量，因此每一代新生物中的大部分都会死掉。第四，生物后天的变化在大部分情况下不能改变基因。</p><p>　　进化论把这神秘性给打破了。进化论就像牛顿力学那样，用极为简单的理论解释了复杂的世界，而且逻辑严谨，能自圆其说，不需要任何超自然力量的干涉，比牛顿力学更容易让人接受。<br>　　<br>　　达尔文在《物种起源》书里画了一张插图，画的是一棵巨大的树，树根是原始生物，越往上树的分叉越多，生物越复杂。这张图暗示了生物是从低级到高级“进化”的，生物越进化，构造越复杂。这是《物种起源》里唯一一张插图，我们的课本上过去也有这张图，然而这张图是错的。目前生物学界更喜欢的画法是把所有的生物画成一个圆形，越靠圆心的生物在地球上生存的时间越早，人类和今天所有的动植物平均分布于圆形的边缘，看不出谁比谁更高级来。<br>　　<br>　　达尔文的进化论则认为，长颈鹿并不是自己把脖子伸长的，而是每一代新出生的长颈鹿因为基因的变异，脖子有长有短，脖子较短的长颈鹿很难生存下来，都死了，只有长脖子的基因才能留下来。久而久之，长颈鹿的脖子也就越来越长了。<br>　　<br>　　那么雄孔雀的尾巴呢？这玩意儿纯粹是累赘！这个疑问乍一看的确是对进化论非常有力的攻击，所以达尔文说：“每当我凝视雄孔雀的尾羽，总感到一阵恶心！” 解释是，<strong>进化中除了生存选择外，还有生殖选择。</strong><br>　　<br>　　有两个解释可以反驳、否定进化论的倾向。第一，所有的科学理论都是一种假说。这就是为什么我们前面要费这么大力气解释进化论的细节，我是想让大家明白，达尔文进化论是目前最合理、佐证最多、反证最少，也是最简洁、最聪明的假说。第二，有越来越多的科学发现增加了进化论的可信度。除了现实中可以观察到活生生的进化过程外，最有力的证据，是在不同地质层里发现的化石都符合进化论的预言。</p><p><strong>社会达尔文主</strong>：把进化论推广到社会学领域。在接受了进化论以后，有些人试图把这一理论应用到其他领域，就像哲学家把力学应用到机械论世界观中一样。其中给人类造成最大恶果的就是社会达尔文主义了。 简单地说，社会达尔文主义的意思是，我们的社会也应该像优胜劣汰的大自然那样，有很高的淘汰率，把不适合生存的人都淘汰掉，以便达到最高效的进化。其中最具代表性的就是纳粹的种族主义。纳粹认为只有“优等”种族才有权利在资源有限的地球上生存下去，其他的“劣等”种族必须淘汰掉，以免和优等种族抢夺资源，以免他们以通婚方式“污染”优等种族的基因。这种社会达尔文主义给纳粹迫害犹太人找到了理论借口。</p><p>这是一个集体主义的价值观。然而，根据进化论，我们还可以得到另一个完全相反的结论：进化论不是说基因的遗传——即个体的生存、生殖——是最重要的吗？换句话说，人类基因的本性就是自私的，这是毋庸置疑的，因为不自私的基因都没得到遗传。那么，自私不就是人类的天性吗？这样看来，每个人都为自己的利益行事也是天经地义的。“人不为己，天诛地灭”，这便形成了和社会达尔文主义相反的、极端的个人主义的价值观了。怎么看待这两种观点呢？ 英国作家理查德・道金斯写了一本《自私的基因》，解释了生物种群中的利他行为。比如有的蚂蚁为了其他蚂蚁的生存，会牺牲掉自己。这是因为，在有的情况下，我们应该把某一个生物群体看成一个基因单位。当生物是以种群为单位生活的时候，基因中可以带有一些牺牲自己帮助他者的“利他基因”，这样更有助于整个种群基因的延续。当个体牺牲自己利于他者的时候，就等于通过他者延续了自己的基因，并不违背进化论的模式。所以，<strong>“人类天生自私”的这种观点就可以被否掉了。人类就是典型的社会型生物，人类的基因中，并不一定就不存在“利他基因”。</strong></p><p>无论是天性自私论，还是社会达尔文主义，全都犯了一个错误，<strong>那就是把“我们为何成为这样”和“我们应该怎样”等同在一起。</strong>这些观点的逻辑是，既然人类的基因是经过生存斗争而来的，那么人类就应该把这种斗争精神延续下去，继续通过竞争来筛选基因。</p><p>但是，为什么呢？进化论仅仅阐述了一套基因变化的规律，这中间并没有任何道德含义。而且正是进化论把神创说从生物界赶走了，才把生物学中的道德元素降到最低的程度。 在整个进化论学说中，有任何的观点能说明，进化论是道德的吗？是高尚的吗？是人类不可干涉的吗？实际上，就像人类利用力学改造自然一样，人类早已在插手生物的进化过程。这才有了不适合野外生存的家畜，才有了农作物。 所以，把进化论的观点和道德连接在一起，是思维混乱的表现。</p><p>按照<strong>爱因斯坦</strong>的说法，太阳的引力能够扭曲光线。在白天，我们观测太阳旁边的星星时，星星发射到地球的光线不是正好路过太阳吗？这光线就会受到太阳引力的干扰，我们所看到的星星位置会受到影响。而到了晚上没有太阳的时候②，我们观测到的星星的位置没受到太阳的影响，就和白天的不同了。但我们都知道，白天是看不到星星的，因为太阳太亮了。 只有一种情况除外：日全食。</p><p>在牛顿时代（也是咱们普通人的概念），时间和空间都是独立的，互相没有关系。就像“5分钟”和“3厘米”根本没法放在一起计算一样。 但是<strong>狭义相对论认为，</strong>时间和空间不是互相独立的，可以互相影响，不同运动状态的人观察同一个物体，观测到的时间、大小都不相同。因此时间和空间得放在一起研究，统称为时空。<strong>质量和能量也不是互相独立的，统称为质能。这也是核武器的理论基础。</strong></p><p>直到<strong>广义相对论</strong>出现后，人们对于引力才有了一个较为合理的解释。<strong>广义相对论的意思是说，当空间中存在物质和能量的时候，空间就会受到影响而弯曲，质能越大，空间弯曲得越厉害。</strong>引力就是这种空间弯曲产生的。有一个非常形象的比喻。好比我们的空间是一张抻平的床单，当我们往上放一个木球的时候，床单会被压下去，那么木球周围其他更轻的小球就会滚向木球，看上去就好像小球被木球吸引了一样。假如放的是铅球呢，床单会被压得更严重，造成的空间扭曲更大，引力也就更大。相对论对于哲学的意义在于，这进一步打击了人们对先验理性的信心。<br>我们顺便说一下，相对论的发现也正好证明了休谟怀疑论的正确。休谟说，无论我们过去看到多少重复发生的事件，我们也不能断言这事件在未来一定会再次发生。<br><strong>测不准原理：</strong>然而物理学家们在研究量子的时候发现了一个奇怪的现象。物理学家观测一个电子，越是精确地确定其位置，就越无法确定它的动量；越是想更精确地测定它的动量，就越测量不到它的位置。这并不是因为科学家的观测技术不行，而是由严格的理论决定的。这个规律叫作“海森堡测不准原理”或者“海森堡不确定性原理”。也就是说，你大可以想象每一个电子在某个瞬间有固定的位置和动量，但这对于人类是没有意义的。人类永远无法知道一个电子的运动状态，也永远无法精确预测电子的运动，只能大略猜测它的运动趋势。而且，因为不能准确预测某个电子下一刻的位置，所以我们连分辨两个电子的能力都没有。当我们观测两个相同电子的时候，我们只能看到两个电子闪来闪去，我们根本没法知道哪个是哪个。</p><p>然而量子力学说的是什么呢？<strong>在量子级别的世界里，没有决定论，也没有确定的因果律。</strong>科学家们对于一个电子的运动状态只能预测出一个概率，只能说大约、可能在哪儿。物理学成了一门缺乏确定性的学说。这一下子让整个物理学都变得可疑了。难怪爱因斯坦对这一学说特别反感。在这个问题上，爱因斯坦扮演了顽固派的角色，试图用各种办法来驳倒测不准原理，爱因斯坦有一句名言：“上帝不掷骰子。”意思是说，世界不可能真正是随机的，一切都是确定的。然而，这回是爱因斯坦错了。</p><p>经过无数次讨论，今天的科学家们普遍接受了海森堡等人的结论。人们相信，在对量子的认识上存在着不可逾越的限制，人类永远无法准确地认识量子。霍金因此说：“上帝不但掷骰子，还把骰子掷到我们看不见的地方去。”说这世界不仅存在随机性，而且人类无法更准确地了解它。这意味着，人类对世界的认识能力又受到了进一步的限制，而且只要量子力学不被推翻，这限制就永远无法超越。 那种认为“随着不断发展，科学终究能解释</p><p>量子力学还有一个问题，它和广义相对论是矛盾的。<strong>用广义相对论去研究宏观宇宙，用量子力学去解释微观世界，都没什么问题。</strong>但是这两个理论却无法相容。这里面显然有问题，但是科学家们无论是从量子力学还是广义相对论中，都还没有找到突破口。科学家们觉得，应该从更高的层次上来统一这两种理论。比如美国电视剧《生活大爆炸》里的主角Sheldon搞的超弦理论以及另一套M理论，都是目前非常流行的方案。但这些方案的问题是，它们只能在数学上进行统一，却无法用实验验证。因为实验所需要的技术远远超过了人类现有的能力。不能用实验检验，这对于物理学家来说是很难接受的，但也没有什么好办法。</p><p>　　罗素的名言是：“三种单纯又极其强烈的激情支配着我的一生：<strong>对爱情的渴望，对知识的追求，以及对于人类苦难不可遏制的同情</strong>。”这话广为流传。<br>　　科学虽然倒打了理性一耙，但这一耙还不是很厉害。我们可以承认理性在宏观和微观上都有局限，但这并不是说理性在现实生活中就没地位了。在日常生活里，我们不还在用牛顿定律嘛。<br>　　这个逻辑学家只能继续出版这本书，并在书的末尾加上一句：“一个科学家所碰到的最不爽（undesirable）的事，莫过于在他的工作即将完成时却发现所干的工作的<strong>基础崩溃</strong>了。”然后他把罗素的信附在了结尾。<br><strong>富与穷：</strong>咱们过去受阶级观念的影响，认为穷人先进，富人落后。所以历史人物的形象，多半是越穷越聪明勇敢，越富越残暴愚蠢。但事实上，不少艺术家、思想家都出身富贵家庭。这是因为有钱人也不会是傻子，他们不少人当然不希望自己的后代变成纨绔子弟，他们和普通家庭的家长一样，都会尽力教育好自己的子女。而有钱人有更多的资源可以利用，培养出人才的概率也就可能更大一些。<br>　　一个全世界最有才华的哲学家，同时还是世界级的大富翁，放弃了一切财产去穷乡僻壤教孩子，这事正常人肯定理解不了。维特根斯坦的侄子就说：“作为乡村小学教师的亿万富翁肯定是个变态狂。”关于这事还有个段子，当时有人问他你为什么把钱给自己的家人，怎么不给穷人啊。维特根斯坦回答说，金钱让人堕落，而他的亲人已经够堕落了，所以再堕落点也没关系。<br>　　所以维特根斯坦说：“凡是可说的事情，都可以说清楚，凡是不可说的事情，我们必须保持沉默。”对这句话我的理解是：凡是符合逻辑实证规则的语言，内容都很清晰准确；凡是不符合逻辑实证规则的语言，说了也是没意义的，就不用说了。<br><strong>实用主义</strong>和逻辑实证主义的思路不一样，逻辑实证主义看到的是科学的严谨性，希望哲学也能和科学一样严谨。实用主义则看重科学的实用性，看到科学家没哲学家那么多废话，在科学研究中什么理论好用就相信什么。实用主义者觉得，哲学也得像科学这样，不再说空话，不再讨论空泛的大问题，而是重视哲学的实用性。<br>　　<br>　　实际上，马克思当年为了维护工人阶级利益提出的很多要求，大部分都被资本主义国家接受并且实现了。如今这种改良式的资本主义在西方颇受欢迎，这可以让我们看到实用主义在西方的用处。也不要以为实用主义只有西方人才喜欢。实用主义离我们也不远，有一句话我们很熟悉：“黑猫白猫，能抓住老鼠就是好猫。”实际上，我写的这本书就奉行着实用主义的观点。我以为，我们普通人学习哲学是为了解决各种靠物质无法解决的人生苦恼。就像俗语说的“能用钱解决的问题都不是问题”，我们就是来解决用钱解决不了的问题的。所以我在筛选、介绍哲学观点的时候，最关心的一件事就是：这个哲学观点能不能帮助我们减少痛苦，能不能让我们内心平静，能不能让我们不再空虚、不再恐惧、不再陷入物欲的无限烦恼之中？<br><strong>胡适</strong>在《中国哲学史大纲》中说：“凡研究人生的切要问题，从根本上着想，要寻一个根本的解决，这种学问，叫作哲学。”他所持的，也就是实用主义的哲学观。但要注意了，实用主义并不代表着只要观点对我们有用，我们就能没有原则地拿来相信。对于咱们前面提出的各种人生问题，最容易接受、做法，胡适在《中国哲学史大纲》中说：“凡研究人生的切要问题，从根本上着想，要寻一个根本的解决，这种学问，叫作哲学。”他所持的，也就是实用主义的哲学观。但要注意了，实用主义并不代表着只要观点对我们有用，我们就能没有原则地拿来相信。对于咱们前面提出的各种人生问题，最容易接受、效果又最好的观点，莫过于相信这世界上有神灵，公平地赏罚一切，而且人的灵魂不灭。以上这些观点是最“实用”的了，但我们绝不会因此就认为它们是真理。我们依旧严格按照逻辑、按照理性思辨来寻找我们的答案。就算我们得到一个让人绝望的结论，我们也会坦然接受。<br>最典型的例子是在17世纪之前，欧洲人见到的所有天鹅都是白色的。无数次的观察结果让欧洲人相信，天鹅一定是白色的。但在1697年，人类发现了黑色的天鹅。这个例子正好证明了实证主义的错误，即便人们发现再多的白天鹅，也不可能得出“所有天鹅都是白色”的结论。<br>波普尔看出了其中的问题，提出了一个检验科学理论的重要标准<strong>：证伪。</strong>什么是科学理论，什么不是？其中关键的标准，是看这个理论有没有可以被证伪的可能。<br>我们有的人可能会简单地以为，民主就是“大家一起投票，多数说了算”，就是“少数服从多数”。其实这种原始的民主制度有极大的缺陷，<strong>这个缺陷在雅典人判苏格拉底死刑、法国大革命的屠杀、希特勒被民众选上台等事件中已经暴露无遗，早就被现代社会抛弃了。</strong><br><strong>民主制度的核心精神：</strong>证伪主义的政治观，最关心的不是谁制定的政策，而是无论谁制定的政策，都不能成为绝对真理。不管是美国总统下的命令还是全世界人民投票的结果，都要给别人留出修改、推翻它的机会。在这种制度下，无论谁被民选上台，也不会给世界造成太大伤害。因为他上台后的个人权力非常有限，哪怕加个税都需要国会批准。<strong>他还必须随时面对全国媒体的质疑、随时可能被弹劾、干四年就得重选、干八年就得下台。这制度不能保证总统想出“最正确”的决策，但可以保证一旦总统作出“错误”的决策，举国上下有无数可以阻止它的机会。可以随时“纠错”而不是“多数说了算”，这才是现代民主制度的核心精神。</strong></p><p><strong>生物学为什么另起炉灶：</strong>您能意识到科学理论互相取代，依据的是什么原则了吗？是实用主义！那个市侩的、庸俗的、让我们瞧不起的实用主义，竟然是整个科学的核心？在相对论出现以后，我们发现，牛顿力学从较真的立场上来说都是错的。我们生活的空间是弯曲的，我们随便摆放一块橡皮就可以改变空间的弯曲程度。我们坐了一趟汽车，表就和标准时间有了一点点偏差。然而，我们在生活中从来不使用相对论解决问题。人们在制造汽车轮船的时候，用的仍旧是牛顿力学的公式。为什么明明有更准确的理论我们不用，非要用不够准确的呢？原因不用我说大家都知道：牛顿力学在日常生活中已经足够准确而且足够简单。<strong>一句话，更实用。</strong>再比如，生物体内的分子原子都严格遵守物理定律。那么我们可以把生物看成一个由大量分子组成的物体，使用种种物理定律去研究它的规律。然而事实上，<strong>我们在研究生物的时候，用的是和物理学完全不同的生物学，是一套全新的定义和理论。我们为什么抛弃掉物理学已经取得的巨大成就，在生物体研究上另起炉灶呢？这就是因为，当我们把某个器官当作一个整体，按照生物学的方法去研究时，要比把它当作一个复杂的分子集合体用物理学去研究简单省事得多。</strong>虽然物理学研究的结果更精确，但是生物学的方法简单实用，所以我们选择使用生物学。还是因为实用。</p><p>实用主义哲学家詹姆斯有一个比喻，原本是来说宗教信仰问题的，我给改写了一下。说有一个小伙子想要向一个女孩求婚。这个小伙子只想和美若天仙的女孩结婚，但除非结婚，否则他没办法知道这个女孩的相貌，于是小伙子就很纠结。因为女孩的外貌不能被检验啊，按照科学的原则，就得当作这事儿不成立。那么小伙子一直犹豫，也就一直没跟那女孩求婚。小伙子对待女孩子外貌的原则和我们对待隐形龙一样：女孩的相貌我没法知道，那我就得存疑，我不能证明女孩是一个美若天仙的人，我就一直不能做出结婚的决定，婚事就得一直拖着。但詹姆斯说了，小伙子对结婚犹豫不决，拖着没求婚，这不也是一种选择吗？这不就等于选择了相信“女孩并非貌若天仙”了吗？换句话说，怀疑论者以为自己把所有可疑的东西都悬置起来了，不当它是真的，实际上，这就相当于你当它是假的了！所以怀疑论者以为自己是谨慎的、中立的，但是怀疑论者对可疑的事情采取了不相信的态度，本身还是一种独断的选择。按照詹姆斯的话说，怀疑论者觉得<strong>“与其冒险步入谬误，倒不如冒险丧失真理”</strong>。这和盲目相信有什么区别呢？<br>虽然我们不能把科学当作衡量一切理论的标准，但是我们仍旧有标准可以用。我认为有两个原则必须坚持：<strong>第一是经验主义原则。</strong>换句话说，理论好用不好用，必须眼见为实，拿出大家都承认的证据来。<strong>第二是实用主义。</strong>理论还得有实用价值，不实用的理论再诱人也没有意义。<br>这可以证明：科学并不一定就是解释、改造世界的唯一标准。比如在做菜这件事上，科学方法就被打败了。而且我们评价两者孰优孰劣的标准是非常清晰的：立足经验主义的实用主义。中国烹饪法做出来的菜好吃，技巧容易掌握。所以就赢了。<br><strong>什么叫“迷信”呢？不经思考的相信，不允许别人质疑，就叫“迷信”。</strong><br>教会当年用了成千上万个宗教裁判所、遍地而起的火刑架都没能统一观点。科学家们只靠着几本学术期刊就搞定了。这不是非常了不起的事吗？很多宗教人士都乐于使用汽车、飞机、手机、电话等高科技产品。不为什么，就因为这些东西是最实用的。在一百年前，传教必须靠步行万里路，站在街头扯着脖子喊。现在传教可以用鼠标一点，网页上一发，几十万人都能同时看到。这样方便的技术，为什么不用？<br><strong>关于“意义”：</strong>我们都不需要有文化的人出手，随便拽过一个大妈，就把前面那几个问题都搞定了。你问大妈：“我怕死，怎么办？”大妈说：“怕啥啊！愁也是一天，乐也是一天，为啥不乐啊。”——她回答了，“追求快乐”就是人生意义，关注眼下的快乐，就可以不怕死亡。你问她：“我觉得满足欲望也没什么意思，怎么办？”大妈说：“人活着得有个爱好啊。你瞧我，天气一好就到广场上跳舞，身体好，还交了不少好朋友，多快乐！”——她回答了，“拥有爱好，锻炼身体，和朋友相伴”就是人生意义，这样做能避免孤独、沮丧和纵欲的空虚，拥有持久的幸福。你问她：“我不想过庸常的一生，怎么办？”大妈说：“我听不懂啊，啥叫庸常？平安是福，知足常乐。健健康康、没病没灾的，这日子不挺好吗？”——她回答了，“平常生活来之不易，因此平凡的生活并不平凡”，认识到这一点，就是人生意义。你问她：“人生要受那么多苦难，有什么意义？”大妈回答：“啥意义？我不懂啥意义，苦这东西，轮到你吃的时候你就得吃。反正吃苦总有个头呗。”——她回答了，“等到苦尽甘来的一刻”就是吃苦的意义。</p><p><strong>心理学可以驱散人的负面情绪，让人更充实快乐，这当然是一门很棒的学问。</strong>但是心理学不能告诉我们，这世界有没有终极存在，不能告诉我们人生的意义是什么。当心理医生为你解答这些问题的时候，他考虑的不是这些问题的真假，而是该怎么回答才能让你的心里更舒服一点、更健康一点。这是标准的<strong>实用主义</strong>，对你效果最好的答案，医生就会当成真理告诉你。</p><p>在理性的领域里，面对“人生的意义是什么”等等形而上学问题，要么去求助心理医生，要么就没有答案了。这就是本书的结论。如果这本书您看到这里，对有一些地方仍然感到迷迷糊糊，有些地方仍然没看懂，甚至光顾着去看八卦了，那也没有关系。我现在告诉您一句结论，只要记住这句结论，这本书就算没白看（睡觉的同学醒醒，老师划<strong>重点</strong>了）。<strong>这句话是：形而上学走不通，形而上学的问题都没有答案。记住这一句话就够了。</strong>我们说过，形而上学的任务，是用理性思维去研究世界本质等“大问题”。形而上学走不通，也就是说，理性不可能回答“世界的本质是什么”“有没有终极真理”“终极真理是什么”“人生的意义是什么”等大问题。硬要回答，答案一定是独断论的，或者在推理上有错误。形而上学家们研究了好几百年，就得出这么一个结论。实际上，所有的形而上学都会陷入无法证明自身的困境。我们说过，经验主义者们的论断“只有来源于经验的知识才是可靠的”，并非来自于经验。康德用来批判理性的工具却没经过自己的批判。黑格尔讲辩证法，但是他的辩证法到最后却并不辩证。尼采说所谓的真理都是谬误，那他自己的理论不也是谬误了吗？逻辑实证主义用来分析语句的规则，经过自己的分析都变成无意义的了。波普尔的证伪主义理论，是不能被证伪的。后来到实用主义的时候，罗素批评说：实用主义以“是否实用”为标准评价真理，但是“是否实用”的标准是什么呢？如此追问下去，必然会形成无限回溯，得不出结论。<br>艺术：问题没有答案，最聪明的人们追求到最后，不约而同地发现这是一条绝路。但正是因为这些艺术家陷于永远无法挣脱的苦闷，而他们又非要倚仗自己过人的天才全力挣扎，所以他们的作品才能深深打动我们。所以世界上才有艺术这东西。</p><p><strong>萨特</strong>反对这种借口。他认为，人在各种极端的情况下，都有选择的自由。哪怕是不选择，也是一种选择。既然有选择的自由，也就要对任何一种选择的后果都负责任。萨特说这话有很强烈的现实意义。他经历过德占时期，面对侵略者的铁蹄，很多法国人都面临着良心和生存的选择。有些人就会为自己的选择错误辩护说：我出卖良心，不是我自己选的，是迫不得已啊。萨特的回答是，<strong>你任何时候都有选择的自由，你有牺牲自己生命的自由，你只是没选而已。</strong>这并不是说我们要强迫每个人都要大义凛然地去牺牲，不，选择牺牲也是一种自由，也可以选择。问题不是你选择了什么，问题是无论选择什么，你都要为此负责任。</p><p>　　我们常说一句话，“小的时候想要快点长大，等到长成了大人，反倒要羡慕小孩子。”为什么会出现这种反差呢？我们细想，小时候羡慕大人什么？羡慕大人想做什么就可以做什么，也就是说，羡慕大人的自由。那么大人羡慕小孩子的什么呢？羡慕小孩子可以无忧无虑地生活，也就是说，羡慕小孩子没有生活压力，不需要尽很多生活的责任，因为有大人护着、养着他。因此，自由（即权力）和责任是相伴的，自由越大，责任越大。人们趋利避害，都想只占有自由的好处（当不负责的大人），又想逃避随之而来的责任（当自由自在的小孩）。可是，这怎么可能呢？萨特提醒我们，既然有选择的自由，就要承担选择的责任啊。而且萨特还强调，不能逃避选择，哪怕是什么都不选，你也是在选择，也要承担责任。因此自由多了并不是好事，反倒因为责任的增多给人增加负担。这是现代社会精神危机的一个根源，也是很多人宁愿选择盲从偶像、盲从权威的心理动机之一。<br>　　<br>比如宗教信徒认为，在教义的指导下生活就是人生目的；叔本华认为，对抗生命意志是人生目的；尼采认为，努力当超人是人生目的；黑格尔更是认为整个历史都是有目的的，个人的人生目的是去努力实现历史的目的。<br>总之，我们对自己的人生的评价、规划，一定是个故事模式，一定有高潮和结尾。但现实并不是如此。当现实和我们的印象发生冲突的时候，荒谬感就产生了。<br>那该怎么办呢？<strong>我觉得唯一的办法是和大众的做法一样，自己给自己找到人生意义（虽然是虚假的）。用西西弗的比喻来说，我们只能在推石头的时候哄自己说这么做是有意义的，并且乐在其中。这个哄骗自己的借口，就是人生意义。</strong><br>一个“人生意义”只有唯一答案的世界，是不是很可怕呢？参差多态，乃幸福之本源啊。那么，该如何找到自己的人生意义？我认为最有效的办法，是逼迫自己直面死亡。我们问人生的意义是什么，其实就是在给自己的人生找一个目标，就是在问：“我为什么活着？”这也就等于在问：“我为什么不立刻自杀？”<strong>加缪说过：“真正严肃的哲学问题只有一个，那就是自杀。”</strong><br>人小的时候要问“人为什么活着”，长大了就不问了，不一定是因为知道答案了，而是因为某些原因让他觉得不再需要问这个问题了。当你不再问这个问题的时候，或许就意味着你已经找到了答案。</p>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
          <category> Philosophy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>哲学家都干了什么-上</title>
      <link href="/2018/03/30/%E5%93%B2%E5%AD%A6%E5%AE%B6%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BB%80%E4%B9%88-%E4%B8%8A/"/>
      <url>/2018/03/30/%E5%93%B2%E5%AD%A6%E5%AE%B6%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BB%80%E4%B9%88-%E4%B8%8A/</url>
      <content type="html"><![CDATA[<p>【读书笔记】：哲学家们都干了些什么？上<br><a id="more"></a></p><blockquote><p>前言</p><p>上篇 理性的崛起<br>第一章哲学好讨厌<br>第二章少年国王<br>第三章使徒行传<br>第四章上帝之城<br>第五章异教徒<br>第六章神们自己<br>第七章群魔<br>第八章异端的权利<br>第九章奇怪的论调<br>第十章童年的终结<br>第十一章理性主义<br>第十二章形而上学<br>第十三章二元论<br>第十四章唯我论<br>第十五章寒冬夜行人<br>第十六章双星<br>第十七章名利场<br>第十八章巨匠与杰作<br>第十九章机械论<br>第二十章决定论<br>第二十一章暴风雨<br>第二十二章哲学怪兽<br>第二十三章谎言的衰落<br>第二十四章远离尘嚣<br>第二十五章王者之风</p><p>下篇 理性的陨落</p><p>第一章傲慢与偏见<br>第二章悲观主义<br>第三章理性的危机<br>第四章瞧！这个人<br>第五章钢铁之躯<br>第六章被侮辱的与被损害的<br>第七章科学新发现：理性的反击<br>第八章人猿星球<br>第九章科学倒打一耙<br>第十章寻欢作乐<br>第十一章快乐王子<br>第十二章逻辑实证主义<br>第十三章实用主义<br>第十四章终结形而上学<br>第十五章实用主义的科学<br>第十六章科学是什么<br>第十七章永恒的终结<br>第十八章存在与虚无<br>第十九章西西弗的神话<br>第二十章人生的意义</p></blockquote><h1 id="上篇：理性的崛起"><a href="#上篇：理性的崛起" class="headerlink" title="上篇：理性的崛起"></a>上篇：理性的崛起</h1><p><strong>基督教与犹太教的区分：</strong>首先是在犹太人中产生了犹太教，而基督教是从犹太教中发展来的。犹太教和基督教都信奉上帝，也都相信会有救世主来拯救他们（“基督”和“弥赛亚”是一个词，都是“救世主”的意思）。区别是，基督教认为救世主就是耶稣，而犹太教不承认耶稣是救世主，他们认为救世主还没有到来。在对待经文上，两者都信奉《旧约》，但只有基督教相信《新约》。《旧约》和《新约》的区别大致在于，一个是记录耶稣降生之前的事，一个是记录之后的事。</p><p>历史上有一个规律，在斗争中，哲学总站在弱者的一方。这是因为哲学讲思辨，讲道理，而只有弱者才会去讲理。强者不需要讲理。</p><p>宗教要求信仰，哲学要求怀疑，两者相悖。</p><p>宗教天生拒斥思考。</p><p>有句俗话叫“能用钱解决的问题都不是问题”，其实还可以说一句话：“必须用暴力解决的问题都是解决不了的问题。</p><p>阿拉伯人并不像后来的基督教十字军那样，对异教徒进行血腥屠戮，而是对占领区的基督徒非常宽待，允许他们保留信仰。唯一的“歧视”是，伊斯兰教徒可以不缴税，而基督徒要缴税。</p><p>不过说来有趣，就在这么无聊的经院哲学时期，却出现了一个对后来的科学发展极为重要的理论。它是一个教士在研究神学的时候提出来的。这个教士因为出生于一个叫作奥卡姆的地方，因此被人称为“奥卡姆的威廉”，这个理论就被后人称为“奥卡姆剃刀”。</p><p>我以为，原因之一在于对宗教的虔诚程度不同。中国人对待宗教有更多实用主义的倾向，信宗教大多是为了要点好处。而且佛教说的是因果报应，就算你不信佛，多做好事也可以有好报。不像基督教讲人有原罪，光做好事没用，你不信仰基督不受洗就进不了天堂。中世纪的教会认为，刚出生的婴儿如果没来得及受洗就夭折了，那也是要下地狱的。<br>中国人拜孔子，是普通的崇拜，并不是当神佛信仰。而且就算是真正的神灵，中国人的崇拜也是马马虎虎的。中国人过年敢用麦芽糖去封灶王爷的嘴，感觉灶王爷就跟胡同口儿那个见谁都打招呼、一下雨就满街喊收衣服的居委会大妈似的，哪有半点上帝的神圣感啊。中国民间还有“泥菩萨过河，自身难保”之类的俗语，直接拿神灵调侃。<br><strong>中国的“两教合一”：</strong>《红楼梦》里，度化女娲的是一僧一道，两人一起行动，一起施法，也不怕俩法术打架；《水浒传》里，和尚老道拜了把子一起去砍人；《西游记》里，道教的玉皇大帝遇到危险叫佛教的如来佛祖过来帮忙。<br><strong>西方宗教火刑由来：</strong>教会还有一个说法：“教会远离血腥。”所以教会不能杀人，把犯人扔给当地政府处罚的时候还要警告官吏，要避免犯人“所有流血及危及生命的可能”。但教会又顺口说了一句，咦，好像火刑不会流血耶。所以你就知道了为什么宗教裁判所的极刑都是火刑了吧。<br><strong>毁灭文化紧跟着杀人：</strong>历史忠实地履行着诗人海涅的那句名言：在他们开始烧书的地方，他们最终会烧人。<br>想减轻罪行吗，可以，裁判所会给你一段时间，让你去检举出更多的异端分子。这一招彻底把这个行业变成了传销，只不过他们要的不是钱，而是良心与鲜血。<br><strong>新教思维的萌动:</strong>说白了，罗马教会认为，外在的行为很重要。而马丁・路德认为，内心的信仰比外在的行为更重要。只要内心真诚信仰上帝就能得救；而是否遵守罗马教会的规定，是否上缴税款，是否完成昂贵的宗教仪式，这些都不重要。<br>因为有了印刷术，欧洲人才有了众多崭新的思想，有了哲学的复兴，有了科学的崛起，有了现代文明的一切：思想自由、理性、怀疑精神、科学、光明的未来。<br><strong>西方宗教的裂变：</strong>路德的影响被越骂越大，再加上各国皇室早就想摆脱罗马教皇的统治和盘剥，宗教革命终于遍及整个欧洲，千百万神父和知识分子卷入其中。几十年后，欧洲支持路德和罗马的两派贵族还打了一场惨烈的宗教战争。双方打了个势均力敌。从此，欧洲基督教分成了两大派：罗马一方被称为天主教；路德一方被称为新教。另外，东边的罗马帝国在此之前还搞了一个东正教。 天主教、新教、东正教，这就是今天基督教最主要的三大教派。新教的诞生全仰仗路德的努力。<br>一言兴邦，这个千百万文人的终极梦想，路德做到了。<br>俗话说“牛打江山马坐殿”。革命的理论者和掌权者常常不是同一个人。就像卢梭成就了罗伯斯庇尔，路德成就的是加尔文。</p><p>而<strong>怀疑是思考的起点</strong>，也是思考成果的检验者。怀疑的最大作用在于能避免独断论，这样才能引导我们寻找正确的答案，免得我们轻信一切未经证实的东西。<br>所以我们才能明白，为什么当年的苏格拉底那么招人讨厌，却能被后人奉为圣贤。因为他的怀疑是理性文明的开端和标尺。所有的思想都要因他的怀疑而诞生，最后还要能经得住他的怀疑才算合格。正是照着这个标准思考，<strong>西方人才有了哲学，才有了科学，才创造了现代文明。</strong></p><p>苏格拉底$\to$柏拉图$\to$亚里士多德$\to$亚里士多德$\to$亚历山大</p><p><strong>犹太教和基督教的关系</strong></p><p>犹太人中诞生犹太教，基督教是从犹太教发展来的。都信奉上帝，都相信会有救世主来拯救他们（“基督”和“弥赛亚”是一个词，都是救世主的意思）。</p><div class="table-container"><table><thead><tr><th style="text-align:left">犹太教</th><th style="text-align:left">基督教</th></tr></thead><tbody><tr><td style="text-align:left">犹太人产生</td><td style="text-align:left">从犹太教产生</td></tr><tr><td style="text-align:left">不承认耶稣是救世主，认为救世主还没来</td><td style="text-align:left">认为救世主就是耶稣</td></tr><tr><td style="text-align:left">信奉《旧约》</td><td style="text-align:left">信奉《旧约》《新约》：区别是记录耶稣降生之前和之后的事情</td></tr></tbody></table></div><p>特殊人物：保罗</p><p>　　拥有罗马公民身份。早年保罗是犹太教徒，积极迫害基督徒。《使徒行传》记载，保罗在追捕耶稣门徒的路上突见耶稣显灵，从这天起，保罗从迫害者转变成虔诚的基督徒。他的皈依对基督教即为重要。<br>　　他做的最重要的事是：向犹太人以外的民族传播基督教。他的武器就是希腊哲学。他撰写大量神学文章称作《保罗新书》，后来成为《新约》的重要组成部分。<br>——此时哲学称为<strong>教父哲学</strong>。</p><p><strong>经院哲学</strong>——集大成者托马斯·阿奎纳</p><p><strong>马丁·路德</strong></p><p>罗马一方称为天主教，路德一方称为新教，东边罗马帝国还有个东正教。</p><p><strong>笛卡尔——我思故我在</strong></p><p>　　只要有了怀疑的念头，就说明“我”肯定是存在的-“我”要是不存在就不会有这些年念头了。</p><p>　　“我思”和“我在”不是因果关系，而是推理样衣关系，即：从前者为真可以推导出后者为真。也就是从“我思”为真，可以推导出“我在”为真。而不是说“我不思”的时候就“我不在”了，在不在我们不知道。</p><p><strong>形而上学</strong></p><p>追问这个世界的本质：世界的本质是物质的还是精神的？解释的本质是物理定律还是我们对物理定律的信念？</p><p>回答这些问题都是形而上学的问题：</p><p>“世界本质是什么”的问题：在哲学里称作“本体论”。</p><p>“哪些知识是真实可信”的问题：在哲学里又称作“认识论”。</p><p><strong>二元论</strong>：心灵一个元，外界一个元。这两个元是互相独立的、平等的，虽然可以互相影响，但谁也不能完全决定另外一个。</p><p>从二元论的角度说，他人对我们的评价和我们的精神世界无关，我们可以完全忽视。但是对于我们在乎的人，这点事极难做到的。一旦做到了，我们也就成了完全不关心任何人的冷血动物。<br>实际上，当我们在乎外人的感受时，就相当于我们把自己的喜怒哀乐寄托于外物，我们即不可能控制一切外物，也不可能让他人的感受总符合我们的意愿。因此不仅仅是二元论，其他自我安慰的手段对于我们说关心的人都有些束手无策。</p><p>反对二元论：唯物主义：说世界的本质是物质的，我们精神世界不过是大脑生理活动的结果。换句话说，精神是从物质中产生的。这种观点叫做物质一元论。</p><p>同时有唯心主义一元论，认为世界的本质是精神的，外面世界不过是自己心灵的产物。</p><p>从二元论进一步得到唯我论。：假设我们只停留在“我在”的阶段，我们只能确认我自己的存在，外界一切存在不存在我不知道，这叫做“唯我论”。</p><p>“唯我论”还可以和目的论结合在一起。：就是认为世间万物是因为某种目的而存在的。</p><p>但是唯我论和目的论能赋予人生一种特殊的美。能给与我们一个理解人生的全新视角。</p><p><strong>斯宾诺莎</strong>：他是笛卡尔的继承者。：按照欧式几何学的模式来建立哲学体系：具体来说，就是先找出一些不言自明的公社，再以这些公社为基础，按照演绎推理的方法建立整个哲学体系。</p><p>他的观点：<strong>实体</strong>的特征是自己就是机制存在的原因，不依赖外物存在。意味着，外物也不可能摧毁实体。推出实体肯定是永远存在的。实体是无限的、是唯一的、不可分的、是善的。</p><p>科学靠归纳法搞研究。事实上，我们今天取得的所有科学成就，都是综合使用归纳法和演绎推理的结果。</p><p>科学派哲学家：洛克：“儿童心灵是白纸”也承认人的本能是天生的。</p><p>笛卡尔、斯宾诺莎代表的数学派，被称为“理性主义”。</p><p>洛克代表的科学家被称为“经验主义”。</p><div class="table-container"><table><thead><tr><th style="text-align:left">理论名称</th><th>理性主义</th><th style="text-align:left">经验主义</th></tr></thead><tbody><tr><td style="text-align:left">代表人物</td><td>数学派哲学家</td><td style="text-align:left">科学派哲学家</td></tr><tr><td style="text-align:left">研究方法</td><td>演绎法</td><td style="text-align:left">归纳法</td></tr><tr><td style="text-align:left">优点</td><td>严谨</td><td style="text-align:left">产生新知识</td></tr><tr><td style="text-align:left">缺点</td><td>不产生新知识，公社未必可靠</td><td style="text-align:left">结论不能保证绝对正确，永远有出错的可能</td></tr></tbody></table></div><p>用一个比喻来描述两个学派的特点：</p><ul><li><p>假如哲学是一座通向终极定理的巴别塔的话，那么理性主义者的塔高耸入云，每搭建一次，都似乎马上可以触摸到天堂。但是这座塔的根基却是几根木头，经验主义者们经常来溜达，随便踹上几脚，这座塔就塌了。</p></li><li><p>经验主义不同，他们的塔盖得极为结实。但是由于能力有限，他们只能零零散散地在各地建造一些矮塔，这些塔既连不到一块，又没法盖得很高。因此经验主义者们的塔虽然结实，却根本没法满足人类的要求，盖得再多也没有用。</p></li></ul><p><strong>新教思维的萌动:</strong>说白了，罗马教会认为，外在的行为很重要。而马丁・路德认为，内心的信仰比外在的行为更重要。只要内心真诚信仰上帝就能得救；而是否遵守罗马教会的规定，是否上缴税款，是否完成昂贵的宗教仪式，这些都不重要。<br>因为有了印刷术，欧洲人才有了众多崭新的思想，有了哲学的复兴，有了科学的崛起，有了现代文明的一切：思想自由、理性、怀疑精神、科学、光明的未来。<br><strong>西方宗教的裂变：</strong>路德的影响被越骂越大，再加上各国皇室早就想摆脱罗马教皇的统治和盘剥，宗教革命终于遍及整个欧洲，千百万神父和知识分子卷入其中。几十年后，欧洲支持路德和罗马的两派贵族还打了一场惨烈的宗教战争。双方打了个势均力敌。从此，欧洲基督教分成了两大派：罗马一方被称为天主教；路德一方被称为新教。另外，东边的罗马帝国在此之前还搞了一个东正教。 天主教、新教、东正教，这就是今天基督教最主要的三大教派。新教的诞生全仰仗路德的努力。<br>一言兴邦，这个千百万文人的终极梦想，路德做到了。<br>俗话说“牛打江山马坐殿”。革命的理论者和掌权者常常不是同一个人。就像卢梭成就了罗伯斯庇尔，路德成就的是加尔文。<br>而<strong>怀疑是思考的起点</strong>，也是思考成果的检验者。怀疑的最大作用在于能避免独断论，这样才能引导我们寻找正确的答案，免得我们轻信一切未经证实的东西。<br>所以我们才能明白，为什么当年的苏格拉底那么招人讨厌，却能被后人奉为圣贤。因为他的怀疑是理性文明的开端和标尺。所有的思想都要因他的怀疑而诞生，最后还要能经得住他的怀疑才算合格。正是照着这个标准思考，<strong>西方人才有了哲学，才有了科学，才创造了现代文明。</strong></p><p><strong>笛卡尔的观点</strong>“我思”和“我在”不是因果关系，而是推理演绎的关系。即：从前者为真可以推导出后者为真。也就是从“我思”为真，可以推导出“我在”为真。而不是说“我不思”的时候就“我不在”了，在不在我们不知道。从“我思故我在”开始，西方哲学的精妙之处可见一斑。</p><p><strong>欧几里得第五公设：</strong>咱们来看看剩下的第五公设。内容是：若两条直线都与第三条直线相交，并且在同一边的内角之和小于两个直角，则这两条直线在这一边必定相交。事实上，由于欧几里得的成就实在是太令人着迷了，公设加推理演绎的研究思想影响了当时整个欧洲的思想界。近代西方法学家们喜欢讲的“天赋人权”、《独立宣言》中讲的“我们认为以下真理不言而喻”，这些都是典型的公设，不需要解释，应无条件承认。然后其他的结论再从这些公设中推导出来。</p><p><strong>“形而上学”：</strong>可以简单地理解成是用理性思维去研究那些能统一世间一切问题的“大道理”。就像笛卡尔希望的那样，要寻找到一个能高于客观世界、统领一切事物的真理。比如世界的本质是什么样子的啊、人生的意义是什么啊之类的问题。</p><p><em>加缪说过：“真正严肃的哲学问题只有一个，那就是自杀。”研究“人为什么不自杀”，其实就是在研究“人为什么活着”。</em></p><p>“世界的本质是什么”的问题，在哲学里又称作<strong>“本体论”</strong>。“哪些知识是真实可信”的问题，在哲学里又称作<strong>“认识论”</strong>。</p><p>　　二元论能帮助我们的关键是：我们在自己的精神世界里是无敌的，而一切体验归根到底都是精神体验。<br>　　而在二元论的观念下，世界被一分为二：外界和内心。痛苦虽然来自于外界，但真正承受痛苦的是我的内心。因此我们虽然仍旧需要尽力去改变外物，但在客观世界这一元里的得失其实不重要，关键是固守自己的内心这一元，固守住我们获得体验的最后一关。而在内心世界里，我们自己能完全做主，这就让人产生了很大的安全感。</p><p>　　我们想，对人伤害最大的其实不是一时的痛苦，而是对未来痛苦的恐惧。这就像打针对于孩子来说，可怕的地方在于排队，在于来苏水味、叮叮当当的针管以及胳膊上的凉意。真正的肉体疼痛与此相比微不足道。我们怕穷，并不是因为我们不能忍受粗糙的吃穿，而是因为不愿意整日生活在对贫穷的恐惧和屈辱中。我们不愿意忍受的是那种担惊受怕的状态。所以，在面对痛苦的时候，我们应该把自己的感受局限在此时一瞬，而不要顾及那些未到的痛苦。<br>　　<br>　　从二元论的角度说，他人对我们的评价和我们的精神世界无关，我们可以完全忽视。但是对于我们在乎的人，这点是极难做到的。一旦做到了，我们也就成了完全不关心任何人的冷血动物。实际上，当我们在乎外人感受的时候，就相当于我们把自己的喜怒哀乐寄托于外物。我们既不可能控制一切外物，也不可能让他人的感受总符合我们的意愿。因此不仅是二元论，其他自我安慰的手段，对于我们所关心的人都有些束手无策。<br>　　<br>　　张爱玲的短篇小说《倾城之恋》里，已经是明日黄花的女主人公本想靠情场手腕俘虏男主人公，怎奈技不如人，眼看就要错失良婿，这时日军突然向香港开战。在战火中，男女主人公同生共死，得以终成眷属。此时张爱玲写道：“香港的陷落成全了她⋯⋯谁知道呢，也许就因为要成全她，一个大都市倾覆了。”这段话是典型的<strong>唯我论和目的论。</strong><br>　　<br>　　不光勇敢，笛卡尔还很有风度，据说有个人因为争抢女人要找他决斗。笛卡尔只说了一句话，就消除了那情敌的敌意。笛卡尔对他说的是：“你的生命不应该献给我，应该献给那位夫人。”</p><p>　　笛卡尔说过：“不管多么荒谬、多么不可置信的事，无一不是这个或那个哲学家主张过的。”</p><p><strong>斯宾诺莎</strong>最有影响的著作叫《伦理学》，在他去世后才发表。这本书的全称是《按几何顺序证明的伦理学》。<br>　　当斯宾诺莎意识到自己的幸福应该通过理性思考来追求的时候，他发现，在得出最终答案之前还需要很长时间。那么在这段时间里，自己该怎么生活呢？他总结了几个可以暂时执行的原则，大意是： <strong>第一，说话要尽量让别人明白，只要别人对我们的要求不会影响我们实现自己的目标（比如求知），那就尽量满足。 第二，只享受为保持健康所必需的生活乐趣。第三，只求取为生活和健康所必需的金钱。</strong> 这些生活准则并非出于斯宾诺莎的哲学思考，而是他以一个普通人的身份、一个立志求知者的身份思考出来的。这些结论平实朴素，完全就是心灵鸡汤的标准素材。<br>　　<br><strong>培根</strong>强调要重视事实。而在事实的基础上进一步形成科学知识，就要靠归纳法了。归纳法的意思是，人们通过观察多个个别的现象，总结出普遍的规律。比如人观察到，每一次把石头扔出去，最后石头总要落地。那么他就能总结出“空中的石头总会落地”这么条规律来。<strong>事实上，我们今天取得的所有科学成就，都是综合使用归纳法和演绎推理的结果。</strong></p><p>　　由于这场争论是哲学界的一件大事，所以哲学家们给这两派学说分别起了名字。笛卡尔、斯宾诺莎代表的数学家派，被称为“理性主义”。 在归纳法里，最重要的是实验数据，是观测结果，它们是科学理论的基础和证据。这些东西可以用一个词来统称：经验。所以洛克代表的科学家派被称为“经验主义”。<br>　　就说理性主义和经验主义之间的分歧吧，其实可以上溯到柏拉图和亚里士多德的分歧。他们俩对世界的看法就不一样。<strong>一个重视心灵理性，一个重视现实经验。</strong><br>　　洛克说，理性主义者们所谓的一些先于经验的公设啊，理念啊，和动物的本能没有区别。莱布尼茨针锋相对地反驳：你知道<strong>人跟禽兽有什么区别吗？区别就是禽兽做事只凭经验，人却能根据经验总结出必然规律。禽兽不知道思考，总以为过去发生的事情，在以后相似的场合下还会发生。所以人可以利用禽兽的习性，去设计陷阱捕捉禽兽。而你们这帮经验主义者，你们只强调经验，不承认必然规律，那你们的联想能力不就跟禽兽一样了吗？</strong> 话说得可真狠啊！ 但应该强调的是，在论战中，莱布尼茨是非常有风度的。他把自己和洛克辩论的书信集结成了一本《人类理智新论》。但是当这本书写成的时候，洛克已经去世了。莱布尼茨认为对手不能答辩了，自己发表和他的辩论是不公平的，于是在自己生前一直藏着这本书没有发表。当然，除了要灭洛克之外，莱布尼茨还有自己的哲学成果，我们也简单说一下。</p><p><strong>莱布尼茨</strong>的公设是这样的：物质是占据空间的对吧？那么只要是能占据空间的东西，就可以被分成更小、更简单的东西。 物质无限地分下去，最后剩下的，一定是不占据空间的“东西”——要是占据空间就能再分下去了。这“东西”不占据空间，所以它不是物质。所以它是精神。 所以一切物质都是由精神组成的。</p><p><strong>牛顿</strong>，旷世天才，伟大的物理学家、数学家、天文学家、哲学家、神学家、炼金术士、小心眼儿、世界末日预测者。——对，你没听错，牛顿晚年通过复杂的公式，计算出了世界末日的具体时间，就在2060年。</p><p><strong>正确并且要简单：</strong>简单地说，我们衡量某个学说、理论、定理是不是好用，<strong>有两个标准：第一看它能否准确地预测未来，第二看它是否足够简要。</strong></p><p>第二个标准，就是一套理论在保持准确性的前提下，越简练越好。<strong>我们今天都接受“日心说”，知道地球绕着太阳公转，同时地球自己还自转。但是不要忘了，运动都是相对的啊。假如我们以地球为静止不动的宇宙中心，我们同样可以描绘出太阳等星球相对于地球的运动轨道来，同样可以符合天文现象。这不就成了“地心说”了吗？之所以我们没选择“地心说”而选择了“日心说”，并不是因为前者不准确，而是因为在两者同样准确的前提下，“日心说”更加简洁。在哥白尼之前的时代，坚持“地心说”的天文学家们为了让理论能和观测结果符合，不得不给太阳等星球画出非常复杂的轨道来。比如让太阳在一个大圆周运动上再做小圆周运动，就像螺旋一样。如果他们按照观测结果不断地修正理论，那么这套“地心说”学说有一天也可以和“日心说”理论一样准确。但是模型和计算过程就无比复杂了。</strong></p><p>这里存在疑问，为什么不是说因为是无线接近真相而相信而是说因为简单，我觉得这里作者理解有误。</p><p>用物理学来解释包括人类意识在内的整个世界，这种观点就叫作<strong>“机械论”</strong>。</p><p><strong>机械论</strong>很好理解，我们在学校的时候都受过辩证唯物主义的训练。<strong>机械论就是除掉了辩证法之后的唯物主义，也可以叫作“机械唯物主义”。</strong></p><p>机械论虽然可以条理清晰地解释这个世界，但是按照机械论的说法，人类不过是这个世界中可有可无的一件事物而已，和桌子板凳、花鸟鱼虫没有本质的区别。我们的意识不过是一系列物质作用的结果，随时可以消失，毫无永存的希望，更谈不上还有什么人生意义。就像世间的其他事物一样，存在就存在了，消失就消失了。这很容易推导出虚无主义和享乐主义。但这还不是最可怕的，最可怕的是这个： <strong>决定论。</strong> 决定论的意思很简单，既然世间万物都可以用物理规律来解释，那么每一个事件之间必然要遵循严格的因果关系。如果人的意识是完全由物质决定的，那肯定也得服从严格的物理定律。那么，整个世界该如何发展，该走向何处，都是由自然定律决定好了的。就像人们根据力学可以预测星辰位置一样，人们也可以根据自然规律来预测未来所有的事件。</p><p>　　中国人很狡猾，遇到好事的时候就不说是“命”了，男女相聚，说的是“缘”。缘是什么？佛教概念里讲的是因果报应。<strong>遇到好事讲“缘”</strong>，意思就是说这是因为我之前做过什么好事，这是我应得的。但自己遇到坏事就像前面说的，不讲因果改讲宿命论。但等到讨厌的人遇到坏事呢，就又是因果了，骂人家这是“报应”，这是“活该”。那么，要是自己讨厌的人遇到好事了，中国人怎么办呢？多半心中暗骂：某某某你等着，三十年河东三十年河西，谁笑到最后谁笑得最好——他又开始讲辩证法</p><p><strong>休谟</strong>想，有什么知识是切实可信的呢？他找到两种。 第一种是不依赖于经验的知识。比如几何学，它自身是不矛盾的，完全符合逻辑规则，而且不依赖经验存在。我们前面说过，在现实世界中观察不到任何严格的三角形，但是我们仍旧有三角形这个概念。三角形不依赖外物存在。自然，像斯宾诺莎、莱布尼茨这些人的哲学体系，因为根基是可疑的，所以不在休谟的承认之列。 第二种可靠的知识是我们自己感受到的经验，摸到什么、看到什么，这些都是可信的（当然，还是那句话，这经验是不是来自于幻觉我们先不管）。休谟想来想去，觉得可信的知识就这两种，于是他很彪悍地说了一段话：<strong>我们去图书馆随便拿起一本书，问这些书中包含着数和量的抽象推论吗？包含着关于实在事实和存在的任何经验的推论吗？如果都没有，就可以烧掉，因为里面只有诡辩和幻想。</strong></p><p><strong>相关性与因果性：</strong>举个简单的例子，假如有一个没有科学知识的原始人，他通过观察发现，公鸡打鸣之后，总伴随着太阳升起，没有一天例外。那么他会认为，公鸡打鸣是太阳升起的原因。这显然是错的。</p><p><strong>归纳法的弊端：</strong>罗素有一个比喻，说假设农场里有一只鸡，每次一看到农场主来，就被喂食物，那么这只鸡就会以为农场主和给它喂食之间有因果联系。但结果有一天，农场主带来的不是鸡食而是一把猎枪，农夫把鸡杀了。换句话说，鸡通过观察发现，农夫和喂食这两件事总在一起发生，便以为其中有因果关系。但实际上，耗费它毕生时间得到的观察结果，仍旧不能证明这两件事之间有必然联系或者因果关系。<br><strong>统计学上有一句经典的话，“相关性不代表因果性”。</strong><br>实际上，我们生活中常常遇到类似不靠谱的结论。比如因为“少年犯中80%的人都玩网络游戏”，就得出结论，说“玩网络游戏会导致青少年犯罪”。那“100%的少年犯每天都吃饭”该怎么解释？<br>在因果律问题的两端，一边是没有因果律，那科学就完蛋了；一边是有因果律，但就会没有了自由和道德。你说你相信哪个？两个都不好受。<br><strong>关于伽利略的“两个铁球同时落地”：</strong>这个实验结果也是错的。因为虽然两个球受到的空气阻力是一样的，但是两个球受到的重力不同，用重力减去相同空气阻力得到的合力，不再和各自的质量成正比，所以实际的加速度也是不同的。只要学过初中物理，列个式子就明白了。<br><strong>人人都信基督教：</strong>我们读过的课本里讲科学史的时候，常常把科学家和教会当成不共戴天的敌人一个无比光辉、一个反动透顶。实际上在那个年代，别说科学家了，连那些被烧死的异端算一块儿，几乎每个人都信基督教，而且不少人还无比虔诚。什么哥白尼、伽利略、牛顿，全是基督徒。拉美特里那样的才是真正的特例。<br><strong>德国哲学“梦之队”：</strong>包括康德，以及后面的谢林、黑格尔、费尔巴哈、叔本华、尼采、马克思、胡塞尔、海德格尔，还有对哲学影响颇大的爱因斯坦、海森堡①。这个超豪华阵容全部都是德意志人。他们中有不少是犹太人。后来希特勒迫害犹太人，致使一堆超级智囊人士脱离德国国籍到英美去作贡献了。德国古典哲学的一个共同特点是晦涩难懂，不只是康德，德国哲学家全这德行。<br><strong>康德：</strong>在这个世界里，人类是一种非常可怜的生物。人类永远无法认识到这个世界的真面目。人类所感受到的这个世界，都是通过人类心灵中某个特殊的机制加工处理过的。这个负责加工的机制，我们起个名字叫作“先天认识形式”。 <strong>世界的真面目，起个名字叫“物自体”（也被译作“自在之物”）。 人类感觉到的世界，也就是“物自体”经过“先天认识形式”加工后得到的东西，我们把他（们）叫作“表象”。</strong></p><p>在康德的哲学世界里，所有的知识（也就是来自于物自体的知识）都要先经过人类心灵的加工，才能被人类认识。所以他自比哲学界的哥白尼，在他的哲学里，不是心灵去感受经验，而是心灵加工经验，心灵生产了经验。当然，这加工过程并不是任意的。</p><p>这个比喻说，假设每个人终身都必须戴着一副蓝色的有色眼镜。这个世界上所有的事物，必须都通过有色眼镜的过滤才能被人看到。那么所有人看到的就是一个蓝色的世界，而世界真实的面貌是人永远看不到的。在这个比喻里，<strong>有色眼镜是先天认识形式，事物原本的颜色是物自体，人类看到的蓝色的世界，是表象。</strong></p><p>换句话说，康德让人的意志受到了先天认识形式的严密保护，因果律不能穿透先天认识形式去控制人的内心意志，所以人仍旧是自由的。</p><p><strong>所谓“二律背反”，</strong>就是一些关于“空间是不是有限”之类的形而上学问题。康德一一讨论这些问题，发现这些问题无论是证明为真还是为假，都是成立的。换句话说，要靠理性去研究这些命题，得出的都会是自我矛盾的答案。康德认为，这背后的原因就是，这些命题讨论的内容不在表象世界中，而是属于物自体的世界，是我们的理性无法认识的。如果我们非要用理性去讨论，就会出现这种自我矛盾的情况。这也就是为什么不同的理性主义者研究这些问题会得出相反结论的原因。</p><p>康德的解决方法是，他把世界分成了两个部分。一个部分完全不可知，另一个部分则可以用理性把握。不可知的那部分因为永远不可知，所以对我们的生活没有什么影响。只要我们在可把握的世界里生活，理性就又恢复了威力。这样，既没有破坏休谟的理论（想破坏也没那能力），又让人类重新信任理性，重新踏实了。 康德的学说并不是和我们完全无关的玄学，而是有很重要的现实意义。 假如我们接受康德的世界观，我们就同意，这世上总有一些东西是我们无法认识的。我们只要安于在能认识的世界里生活就对了。<br>所以，我们说，如果一个人做好事是为了得到表扬，在康德这里就不算道德。这让我们想起了孟子说过的“恻隐之心，人皆有之”。孟子说你遇到一个小孩掉井里了，你救他，不是因为你认识他的父母，不是因为你想受表扬，让你救他的，是你心中的<strong>道德情感。</strong>孟子和康德一样，都认为道德是先天的。</p><p>用<strong>叔本华</strong>的话说，任何人在读懂康德之前都只是一个孩子。</p><p><strong>黑格尔</strong>：可以打一个比方，哲学家们的工作情形就好比有一块大石头，叫作“理性”，哲学家们打算去研究这玩意儿了。但哲学家们唯一能用的工具也只能是“理性”。黑格尔之前的哲学家们，用手中的理性工具去钻研面前的理性石头，一番努力之后，面前的理性石头变了模样。最终，哲学家们看着石头，抹抹头上的汗说：“我的工作完成了，我终于发现终极真理了！”但是这帮哲学家们都忘了，眼前的理性石头变样后，他手里的理性工具也随之变样了！</p><p>我们今天对辩证法有一种庸俗的理解，说辩证法就是“看待事物要分两个方面”。别人批评一个现象，你非要说“要辩证地看这件事，这件事也有好的一面嘛”。这是对辩证法的极大误读。这不叫辩证法，这叫诡辩法，它的唯一作用是把所有的事实都捣成一片糨糊，逃避一切有意义的结论。</p><p>黑格尔说，一个判断并不是世界的永恒真相。他认为，世界不是容不得矛盾的，而恰恰相反，到处都是矛盾，矛盾就是世界的本质。<br>因此黑格尔认为，事物是不断变化的，这种变化是自发的、抑制不住的。同时，这种变化不是无序的，而是有方向的，这个方向就是较低级的正题和反题不断变成更高级的合题，也就是事物不断在向高级形态变化。 变化到最后是什么呢？ 就是黑格尔的终极真理，黑格尔给它起个名字，叫做“<strong>绝对精神”。</strong></p><p>因为重视历史过程，黑格尔是第一个重视研究哲学史的人。今天人们学西方哲学的时候，公认最好的办法是先读一本《西方哲学史》才有资格再谈别的，这个风气就是从黑格尔开始的。</p><p>我们知道，黑格尔的历史观后来被马克思批判性地继承，变成了辩证唯物主义历史观——马克思也认为，历史的进程是有方向的，不可逆转、不可阻止的，但是可以预测的。<br><strong>马克思</strong>预测历史通向的是共产主义，那么黑格尔的历史通向哪里呢？黑格尔的历史通向绝对精神。他认为宗教比自然科学更高级，哲学又比宗教高级。最后，绝对精神会通过哲学完成自己的发展，达到最完美的境界。</p><p>形而上学到了黑格尔这里，变得史无前例的庞大。黑格尔用理性建造了一座宏伟的形而上学大厦，囊括了世间万物，实现了形而上学家们多年以来的终极梦想。所以我们说，<strong>黑格尔是形而上学的巅峰。</strong></p><p><strong>辩证法：</strong>再庸俗地解读一下辩证法。黑格尔认为世界一切事物的发展都要符合辩证法，这个看法太教条了。但有时辩证性的确有道理。 比如，一个人是怎么成长的呢？一个人先有一个原有的思想（正题），然后在生活中遇到了这思想不能解决的问题（反题），思想和现实问题发生了冲突，才会引起他反思人生。这个反思的结果不可能说最后完全不顾以前的旧想法（正题），最后的新思想（合题）肯定是结合了正题和反题。这就代表着人变得更成熟了。</p><p>小资情调的文章会说，<strong>男女之间最悲凉的关系不是憎恨，而是淡漠。</strong>因为如果你恨，就意味着你对伴侣还有很深的感情。——在这里，憎恨是正题。光看正题，我们会觉得这两个人的关系已经差到极点了，差到负数了。但如果我们通过这个正题去找它的反题，会发现，憎恨的原因是受过深深的伤害。受过深深的伤害是因为曾经非常在乎。还憎恨，就说明心结还没有打开，还在乎对方。这就是《苦月亮》等爱情故事的逻辑基础。</p><p>再比如，您应该会同意，我们追求个人幸福的最高境界并不是有钱有权有一大堆情人围着，并不是肉体享乐。哲学史上也没有哪个哲学家认为纵欲是快乐之道。连古希腊的享乐主义者追求的也不是肉欲的极限，而是适度的享乐、劳逸结合的生活。<strong>这是因为大家都发现一个问题，肉欲快乐固然很好，但是纵欲总是和它的反题——痛苦、空虚紧紧连在一起的。不存在某种只给人快乐、不带来痛苦的享乐。这正符合了辩证法的观点。所以最后的结论就是，我们追求个人幸福的最高境界，不是纵欲，而是内心的平静。</strong></p><p>此时的叔本华不仅在生活上独立了，在哲学的道路上也有了自己的看法。他崇拜康德鄙视黑格尔。就像我们前面说的，他觉得黑格尔是欺世盗名的骗子。</p><p><strong>“生命意志”</strong>这个词呢，在叔本华的理论中有专门的意思，跟我们平时聊天时的含义不同。我们在谈叔本华哲学的时候，大多就直接写“生命意志”或者“意志”、但请记住这是一个专有名词，不要按照字面意思去理解。</p><p><strong>那么生命意志是个什么东西呢？</strong>简单地说，是一股永不停歇的力量。这股力量驱使着万物去运动，去发展。 比如人和动物的食欲性欲，比如植物破土而出的欲望。 动物没有理性，可是动物生下来就知道觅食、交配、躲避危险，在很多情况下比人的求生能力还要强。动物这么强大的生存能力哪儿来的呢？叔本华认为，这是背后的生命意志驱使的。生命意志的概念比一般的生物欲望还要宽泛，还包括没有生命的事物在内。叔本华认为，宇宙中万事万物背后都有生命意志在驱动。小到磁石相吸，大到星球运行，背后的本质原因都是生命意志。在叔本华看来，生命意志是世界上最本质的东西，是不可抗拒的，是永不停歇的。 因为物自体是非理性的，所以生命意志也是非理性的，也就是盲目的。 对于人来说，生命意志主要表现在人的生存欲望。</p><p>我们可以打个比方：意志是个充满欲望的君王，但是它头脑糊涂，只知道发布命令，不知道该怎么去更好地达到目的。理性是个头脑清醒的老臣，它虽然对君王的命令有意见，但是限于身份，只能偶尔劝谏君王，大部分时候都是在用它的聪明才智去满足君王的欲望。理性不是没有用，只是实现意志的工具而已。</p><p><strong>我们平时和别人发生了争执，我们说服别人用的是理性吗？绝大多数时候，靠的不是理性而是利益。</strong>比如邻居乱堆杂物占了我们家地方，有几个平常人能用逻辑、用“不侵占公共空间的善是一种普世道德”去说服邻居呢？真正能说服别人的，靠的是利益的威胁（再占我们家地方我就告居委会去）和诱惑（您说咱邻里公平和睦地生活多好啊）来说服对方。</p><p>所以在康德看来，理性就是我们这个世界的统治者。没错，理性确实管不了物自体，但是物自体也不影响我们的世界呀。叔本华说，不，物自体能影响我们的世界。不仅能影响，而且影响力超大，我们用理智控制不了。 在康德那里，这个世界的基础是井井有条的理性。 在叔本华这里，这个世界的基础是无法控制的生命意志。<strong>因此康德对世界的看法是乐观的。 叔本华对世界的看法是悲观的。</strong></p><p>叔本华也不看好爱情。在他看来，爱情是生命意志为了引诱人们生殖下一代所行使的骗术。为了爱情而结婚是非常傻的行为。既然是骗术，那么爱情也不会持久，早晚会幻灭，追求永恒的爱情是徒劳的。因此，如果非要结婚的话，还不如出于功利目的结婚的好。<br>那么什么办法管用呢？叔本华认为，应当增强自己克制欲望的意志力。 所有需要克制的欲望中，首要的是性欲。 我们的很多行为受到性欲的驱使，性欲满足的目的是为了诞生新的生命。而新的生命又意味着新的痛苦。所以叔本华认为，生殖行为就好像人和生命意志签订的卖身契。因此在人类社会里性行为总和可耻相连。（我想社会学家不会同意这句话。）那么意志该怎么控制欲望呢？ 叔本华认为，我们可以提高自己对这世界的认识（当然是去认识叔本华所理解的那个世界），增强我们的理性，用理性抑制和控制感性冲动。然后，把自己的感情和欲望上升为全人类的感情和欲望，这样就可以尽量消除个人的欲望。接下来，我们要强迫自己不去做想做的事，反而去做不想做的事，抛弃一切现实的理想。像苦行僧一样地修行，通过苦行来抑制生命意志。不反对别人损害自己，欣然接受任何损失，把这当作考验自己战胜生命意志的机会。最终欣然接受死亡。<br><strong>古典乐的力量：在所有的艺术中，叔本华最推崇音乐。当然，他喜欢的都是他那个年代的流行音乐——也就是我们今天的古典音乐。我们今天投入地听古典音乐，的确会感到心旷神怡，能把各种欲望、名利、贪心都暂时抛到一边。</strong></p><p>佛教不同，佛教认为欲望是痛苦的来源，主张彻底摒弃一切欲望。这和叔本华的观点很像。这不是巧合，叔本华的哲学观点深受印度佛教的影响。<strong>据说他的书桌上经常摆放的是一尊康德像和一尊佛像。</strong></p><p>叔本华强调非理性的欲望比理性对人的影响更大，这和后来的弗洛伊德心理学强调潜意识的观点很像。但现在的心理学一般认为，潜意识虽然会影响我们，但没有叔本华的生命意志那么无孔不入，人类理性的控制力还是很强的。<br>悲观主义对于我们来说仍旧有现实意义。首先，叔本华的悲观主义从某些角度上看确实是成立的。虽然说理性未必就会败给欲望，但对于大部分人来说，欲望的确是生活的主题。我们是为了获得尽可能多的安全感，为了有更好的物质享乐，为了和别人攀比，才会去忍受无穷无尽的艰辛劳动和在各种挫折中的垂头丧气。大部分人这一辈子活着，为的都是满足各种各样的欲望。我们也同意，欲望是永远不会被满足的。满足了就会产生新的欲望，不满足就会产生饥渴感。所以叔本华的世界观对于大部分人来说，是对的、没问题的。 叔本华提出的解决方案也没问题：<strong>既然满足欲望是一条不归路，那我们就应该早点看清这一点，不再去满足欲望。但欲望不满足我们会饥渴痛苦啊，那就可以像叔本华建议的那样，用无关欲望的对艺术品的欣赏来获得暂时解脱。这也是被社会普遍接受的生活观。</strong></p><p>叔本华不信任朋友。他说：“凡是对敌人保密的事也要对朋友保密。”他甚至刻薄地说：“患难朋友并不是真朋友，他不过是个借钱人。”<br><strong>自由意志：</strong>前面说过，我们很害怕没有<strong>自由意志</strong>。没有自由意志，我们就如同牵线木偶一样，自己不能控制自己。那什么叫“自己控制自己”呢？ 比如有一天，你喝多了，脑子浑浑噩噩，失去了理性。结果你碰见喜欢的姑娘，把人家姑娘给亲了。第二天，你特别后悔，赶紧给姑娘打电话说：“昨天真对不起，昨天我没控制住自己。”昨天你为什么没控制住自己呢？ 因为你在酒精和性欲的作用下，失去了理性。 所以“控制”自己，靠的也是理性。 没有了理性，我不能控制自己，也就等于没有自由意志了。</p><p>这是两个常被误解的人：叔本华，一般人以为他是一个悲天悯人的慈祥老头。不！生活中他暴躁刻薄。 尼采，一般人以为他是一个放荡不羁的狂人——不，生活中他是一个温和的智者。<br><strong>尼采</strong>继承了叔本华的形而上学。叔本华说物自体是<strong>“生命意志”</strong>，尼采给改造成了<strong>“权力意志”</strong>。“权力意志”一词中的“权力”容易引起误解。这并不是政治权力的意思，而是指要让自己变得更强大、更强壮、更富创造力的欲望。</p><p>尼采把人分成了强者和弱者。强者体现了权力意志，他们的特征是积极向上、勇于进取、勇于牺牲、善于创造。弱者相反，特点是胆小、保守、善妒、虚伪。传统欧洲人相信基督教的普世精神和卢梭的人文主义，两者强调的都是对弱者的关怀，强调人人平等。 尼采不同意。</p><p>尼采认为，奴隶道德和贵族道德最明显的区别在于：奴隶道德总是在禁止，不许人们做这做那；贵族道德则是在鼓励人们自由创造。</p><p>有句话说：“中国人本来就穷，可身上的虱子还分三六九等。”在生活中给我们最大痛苦的，往往不是那些有大权力的强者，而是掌握了小权力的弱者。所谓“阎王好见，小鬼难缠”，“小鬼”处境越卑微，能力越弱，在掌握了权力以后就越会肆无忌惮，越心狠手辣，越愿意欺凌比他还弱小的人。在现实生活中，让我们活得好好的突然一瞬间起杀人念头的，大都是这种“小鬼”。</p><p>尼采和叔本华一样，认为这世界是悲观的。但他的解决方法和叔本华不同。尼采的世界观带有强烈的激情，他认为叔本华的禁欲是胆小者的逃避行为。他觉得人不应该像叔本华宣扬的那样避免痛苦，而是应该承认痛苦，迎战痛苦。简而言之，尼采推崇的是一种<strong>精英主义</strong>。</p><p>尼采说：真理就是一种如果离开它、某种生物便不能活的错误。</p><p>当然，现代社会的价值观是多元的，除了崇拜成功外，也有推崇人性、亲情的，也有对拜金主义的批判和反思，这样的文艺作品也不少。但是，<strong>推崇成功是现代社会和过去的基督教社会最大的区别之一，是西方社会价值观最大的变革之一。</strong>在这个背景下看，尼采自然会赢得西方社会的广泛接受，甚至可以算是一个预言家。<br><strong>尼采歧视的女性：</strong>最后再说一下，尼采和叔本华有一个共同之处，他们都鄙视女性。<br>尼采歧视女人，或许是因为他觉得女人不会成为强者吧。不过这么说也够亏心的，他一生全赖女人的照料。尤其是他的母亲。尼采的父母都是虔诚的教徒，尼采攻击基督教的言论深深地伤害了母亲，但她一直都全心全意地呵护照料着自己的爱儿。</p><p>我们之前在说斯宾诺莎的时候提到，越是生活痛苦的人越关心个人幸福。克尔凯郭尔的遭遇如此痛苦，可以想象，他对个人幸福、个人命运会有多么关心！他又受到他父亲的影响，非常关注信仰。</p>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
          <category> Philosophy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《七堂极简物理课》</title>
      <link href="/2018/02/16/%E4%B8%83%E5%A0%82%E6%9E%81%E7%AE%80%E7%89%A9%E7%90%86%E8%AF%BE/"/>
      <url>/2018/02/16/%E4%B8%83%E5%A0%82%E6%9E%81%E7%AE%80%E7%89%A9%E7%90%86%E8%AF%BE/</url>
      <content type="html"><![CDATA[<p>《七堂极简物理课》</p><p>第一课：最美的理论</p><p>爱因斯坦 广义相对论 </p><p>“牛顿试图解释物体下落和行星运转的原因。他假设在万物之间存在一种相互吸引的“力量”，他称之为“引力”。那么这个力是如何牵引两个相距甚远，中间又空无一物的物体的呢？这位伟大的现代科学之父对此显得谨慎小心，未敢大胆提出假设。牛顿想象物体是在空间中运动的，他认为空间是一个巨大的空容器，一个能装下宇宙的大盒子，也是一个硕大无朋的框架，所有物体都在其中做直线运动，直到有一个力使它们的轨道发生弯曲。至于“空间”，或者说牛顿想象的这个可以容纳世界的容器是由什么做成的，牛顿也没有给出答案。就在爱因斯坦出生前的几年，英国的两位大物理学家——法拉第（ Michael Faraday）和麦克斯韦（ James Maxwell）——为牛顿冰冷的世界添加了新鲜的内容：电磁场。所谓“电磁场”，是一种无处不在的真实存在，它可以传递无线电波，可以布满整个空间；它可以振动，也可以波动，就像起伏的湖面一样；它还可以将电力“四处传播”。爱因斯坦……很快他想到，就像电力一样，引力一定也是由一种场来传播的，一定存在一种类似于“电场”的“引力场”。他想弄明白这个“引力场”是如何运作的，以及怎样用方程对其进行描述。就在这时，他灵光一闪，想到了一个非同凡响的点子，一个百分百天才的想法：引力场不“弥漫”于空间，因为它本身就是空间。这就是广义相对论的思想。其实，牛顿的那个承载物体运动的“空间”与“引力场”是同一个东西。这是一个惊世骇俗的理论，对宇宙做了惊人的简化：空间不再是一种有别于物质的东西，而是构成世界的“物质”成分之一，一种可以波动、弯曲、变形的实体。我们不再身处一个看不见的坚硬框架里，而更像是深陷在一个巨大的容易形变的软体动物中。太阳会使其周围的空间发生弯曲，所以地球并不是在某种神秘力量的牵引下绕着太阳旋转，而是在一个倾斜的空间中行进，就好像弹珠在漏斗中滚动一样：漏斗中心并不会产生什么神秘的“力量”，是弯曲的漏斗壁使弹珠滚动的。所以无论是行星绕着太阳转，还是物体下落，都是因为空间发生了弯曲。那么我们该如何描述这种空间的弯曲呢？ 19世纪最伟大的数学家、“数学王子”卡尔·弗里德里希·高斯（ Carl Friedrich Gauss）已经写出了描述二维曲面（比如小山丘的表面）的公式。他还让自己的得意门生将这一理论推广到三维乃至更高维的曲面。这位学生就是波恩哈德·黎曼（ Bernhard Riemann），他就此问题写了一篇重量级的博士论文，但当时看起来全然无用。黎曼论文的结论是，任何一个弯曲空间的特征都可以用一个数学量来描述，如今我们称之为“黎曼曲率”，用大写的“ R”来表示。后来爱因斯坦也写了一个方程，将这个 R与物质的能量等价起来，也就是说：空间在有物质的地方会发生弯曲。就这么简单。这个方程只有半行的长度，仅此而已。空间弯曲这个观点，现在变成了一个方程。然而，这个方程中却蕴含着一个光彩夺目的宇宙。首先，这个方程描述了空间如何在恒星周围发生弯曲。由于这个弯曲，不仅行星要在轨道上绕着恒星转，就连光也发生了偏折，不再走直线。爱因斯坦预测，太阳会使光线偏折。在 1919年，这个偏折被测量出来，从而证实了他的这一预测。其实不仅是空间，时间也同样会发生弯曲。爱因斯坦曾预言，在高空中，在离太阳更近的地方，时间会过得比较快，而在低的地方，离地球近的地方时间则过得比较慢。这一预测后来也经测量得到了证实。</p><p>当一个大恒星燃烧完自己所有的燃料（氢）时，它就会熄灭。残留的部分因为没有燃烧产生的热量的支撑，会因为自身的重量而坍塌，导致空间强烈弯曲，最终塌陷成一个真真正正的洞。这就是著名的“黑洞”。……整个宇宙空间可以膨胀和收缩。爱因斯坦的方程还指出，空间不可能一直保持静止，它一定是在不断膨胀的。……这个方程还预测，这个膨胀是由一个极小、极热的年轻宇宙的爆炸引发的：这就是我们所说的“宇宙大爆炸”。……但大量证据纷纷出现在我们眼前，直至在太空中观测到了“宇宙背景辐射”，也就是原始爆炸的余热里弥漫的光。事实证明，爱因斯坦方程的预言是正确的。此外，这个理论还说，空间会像海平面一样起伏，目前人们已经在宇宙中的双星上观测到了“引力波”的这种效应，与爱因斯坦理论的预言惊人一致，精确到了千亿分之一。……所有这一切都源自一个朴素的直觉，那就是，空间和引力场本是一回事。这一切也可以归结为一个简洁的方程，……”</p><p>第二课 量子</p><p>量子力学诞生于1900年，……德国物理学家马克思·普朗克（Max Planck）计算了一个“热匣子”内处于平衡态的电磁场。为此他用了一个巧妙的办法：假设电磁场的能量都分布在一个个“量子”上，也就是说能量是一包一包或一块一块的。用这个方法算出的结果与测量得到的数据完全吻合（所以应该算是正确的），但却与当时人们的认知背道而驰，因为人们认为能量是连续变动的，硬把它说是由一堆“碎砖块”构成的，简直是无稽之谈。</p><p>对于普朗克来说，把能量视为一个个能量包块的集合只是计算上使用的一个特殊策略，就连他自己也不明白为什么这种方法会奏效。然而五年后，有事爱因斯坦，终于认识到这些“能量包”是真是存在的。</p><p>爱因斯坦指出光是由成包的光粒子构成的，今天我们称之为“光子”。他在那篇文章的引言中写道：“在我看来，如果我们假设光的能量在空间中的分布是不连续的，我们就能更好地理解有关黑体辐射、荧光、紫外线产生的阴极射线，以及有关其他有关光的发射和转化的现象。依据这个假设，点光源发射出的一束光线的能量，并不会在越来越广的空间中连续分布，而是由有限数目的‘能量量子’组成，它们在空间中点状分布，作为能量发射和吸收的最小单元，能量量子不可再分。”</p><p>20世纪10-20年代，丹麦人尼尔斯·波尔（Dane Niels Bohr）引领了这一理论的发展，他了解到原子核内电子的能量跟光能一样，只是特定值，而更重要的是，电子只有在特定的能量之下才能从一个原子轨道“跳跃”到另一个原子轨道上，并同时释放或吸收一个光子，这就是著名的“量子跃迁”。 </p><p>1925年，量子理论的方程终于出现了，取代整个牛顿力学。</p><p>率先为这个新理论列出方程的是一个非常年轻的德国天才——维尔纳·海森堡（Werner Heisenberg），他所依据的理念简直让人晕头转向。</p><p>海森堡想象电子并非一只存在，只有人看到它们的时，或者更确切的说，只有和其他东西相互作用时它们才会存在。当它们与其他东西相撞时，就会以一个可计算的概率在某个地方出现。从一个轨道到另一个轨道的“量子跃迁”是它们现身的唯一方式：一个电子就是相互作用下的一连串跳跃。如果么有受到打扰，电子就没有固定的栖身之所，它甚至不会存在于一个所谓的“地方”。</p><p>在量子力学中，没有一样东西拥有确定的位置，除非它撞上了别的东西。为了描述电子从一种相互作用到另一个相互作用的飞跃，就要借助一个抽象的公式，它只存在于抽象的数学空间，而不存在于真实空间。</p><p>更糟的是，这些从一处到另一处的飞跃大多是随机的，不可预测的。我们无法预知一个电子再次出现会是在哪儿，只能计算它出现在这里或那里的“概率”。这个概率问题直捣物理的核心，可原本物理学的一切问题都是被那些普遍且不可改变的铁律所控制的。</p><p>一个世纪过去了，我们还停在原点。量子力学的方程以及用它们得出的结果每天都被应用于物理、工程、化学、生物乃至更广阔的领域中。量子力学对于当代科技的整体发展有着至关重要的意义。没有量子力学就不会出现晶体管。然而这些方程仍然十分神秘，因为它们并不描述在一个物理系统内发生了什么，而只说明一个物理系统是如何影响另外一个物理系统的。</p><p>第三课 宇宙的构造</p><p>20世纪上半叶，爱因斯坦用相对论描述了空间和时间的运作方式，而波尔和他年轻的门徒们则用一系列方程捕捉到了物质奇怪的量子特性。20世纪下半叶，物理学家们在此基础上，把这两个新理论广泛应用在了自然界的各个领域：从宏观世界的宇宙构造，到微观世界的基本粒子。</p><p>哥白尼的日心说</p><p>太阳系只是不计其数的星系中的一个，而我们的太阳也只是众多恒星中普普通通的一颗，是浩瀚银河系星云中的沧海一粟。</p><p>但是在20世纪30年代，天文学家对星云（恒星之间近乎白色的云团）进行精确的测量后发现，银河系本身也只是众多星系间浩瀚星云中的一粒尘埃。这些星系一只蔓延到我们最强大的天文望远镜也看不到的地方。</p><p>这片均匀无边的宇宙并不像看上去那么简单。就像我在第一节课中解释过的那样，空间不是一马平川，而是弯曲的。宇宙布满了星系，所以我们想象它的纹理会像海浪一样起伏，激烈处还会产生黑洞空穴。</p><p>我们今天终于知道，这个布满星系、富有弹性的浩瀚宇宙是大约150亿年前由一个极热极密的小星云演化来的。</p><p>宇宙的诞生的时候就像一个小球，大爆炸后一直膨胀到它现在的规模。这就是我们现在对宇宙最大程度的了解了。</p><p>第四课 粒子</p><p>我们身边所有物体都是由电子、夸克、光子和胶子组成的。它们就是粒子物理学中所讲的“基本粒子”。除此之外还有几种粒子，例如中微子（neutrino）——它布满整个宇宙，但并不跟我们发生交互作用，还有希格斯玻色子（Higgs boson）——不久前日内瓦欧洲核子研究中心的大型强子对撞机发现的粒子。但这些粒子并不多，只有不到十种。这些少量的基本原料，如同大型乐高玩具中的小积木，靠它们建造出了我们身边的整个物质世界。</p><p>量子力学描述了这些粒子的性质和运动方式。这些粒子当然并不像小石子那般真实可感，而是相应的场的“量子”，比方说光子是电磁场的“量子”。就跟在法拉第和麦克斯韦的电磁场中一样，它们是这些变化的基底场的元激发，是极小的移动的波包。它们的消失和重现遵循量子力学的奇特定律：存在的每样东西都是不稳定的，永远都在从一种相互作用跃迁到另一种相互作用。</p><p>即使我们观察的是空间中一块没有原子的区域，还是可以探测到粒子的微小涌动。彻底的虚空是不存在，就像最平静的海面，我们凑近看还是会发现细微的波动和振荡。构成世界的各种场也会轻微的波动起伏，我们可以想象，组成世界的基本粒子在这样的波动中不断产生、消失。</p><p>这就是量子力学和粒子理论描述的世界。这同牛顿和拉普拉斯（Laplace）的世界相去甚远：在那里，冰冷的小石子在不变的集合空间里沿着精确而漫长的轨迹永恒不变地运动着。量子力学和粒子试验告诉我们，世界是物体连续、永不停歇的涌动，是稍纵即逝的实体不断出现和消失，是一系列的振荡，就像20世界60年代时髦的嬉皮时代，一个由事件而非物体构成的世界。</p><p>第五课 空间的颗粒</p><p>“圈量子引力”试图将广义相对论和量子力学统一起来。</p><p>它的中心思想很简单。广义相对论告诉我们空间不是一个静止的盒子，而是在不断运动，像一个移动中的巨大软体动物，可以压缩和扭曲，而我们被包在里面。另一方面，量子力学告诉我们，所有这样的场都“由量子构成”，具有精细的颗粒状结构。于是物理空间当然也是“由量子构成的”。</p><p>这正是圈量子引力的核心结论：空间是不连续的，不可被无限分割，而是由细小的颗粒，或者说“空间原子”构成。这些颗粒极其微小，比最小的原子核还要小几亿亿倍。圈量子引力用数学形式描述了这些“空间原子”，也给出了它们演化的方程。它们被称为“圈”或环，因为它们环环相扣，形成了一个相互关联的网络，从而编制出空间的纹理，就像细密织成的巨大锁子甲上的小铁圈一样。</p><p>第六课 概率、时间和黑洞的热</p><p>英国物理学家麦克斯韦和奥地利物理学家玻尔兹曼（Ludwig Boltzmann）发现了热的本质。</p><p>玻尔兹曼发现其中的原因惊人的简单：这完全是随机的。玻尔兹曼的解释非常精妙，用到了概率的概念。热量从热的物体跑到冷的物体并非遵循什么绝对的定律，只是这种情况发生的概率比较大而已。原因在于：从统计学的角度看，一个快速运动的热物体的原子更有可能撞上一个冷物体的原子，传递给它一部分能量；而相反过程发生的概率则很小。在碰撞的过程中能量是是守恒的，但当发生大量偶然碰撞时，能量倾向于平均分布。就这样，相互接触的物体温度趋向于相同。热的物体和冷的物体接触后温度不降反升的情况并非不可能，只是概率小的可怜罢了。</p><p>尾声 我们</p>]]></content>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>宽客：华尔街顶级数量金融大师的另类人生</title>
      <link href="/2018/02/12/%E5%AE%BD%E5%AE%A2%EF%BC%9A%E5%8D%8E%E5%B0%94%E8%A1%97%E9%A1%B6%E7%BA%A7%E6%95%B0%E9%87%8F%E9%87%91%E8%9E%8D%E5%A4%A7%E5%B8%88%E7%9A%84%E5%8F%A6%E7%B1%BB%E4%BA%BA%E7%94%9F/"/>
      <url>/2018/02/12/%E5%AE%BD%E5%AE%A2%EF%BC%9A%E5%8D%8E%E5%B0%94%E8%A1%97%E9%A1%B6%E7%BA%A7%E6%95%B0%E9%87%8F%E9%87%91%E8%9E%8D%E5%A4%A7%E5%B8%88%E7%9A%84%E5%8F%A6%E7%B1%BB%E4%BA%BA%E7%94%9F/</url>
      <content type="html"><![CDATA[<p>本书讲述了一场以可怕的失败告终的资本市场实验，在20世纪90年代和21世纪的第一个十年里，买卖、包装复杂证券的技术风靡美国和欧洲。人们建新数学和计算机可以保护投资者免受不利事件的冲击，但事实证明，人们被引入了歧途。这一次信念蒙蔽了专家的眼睛，让他们对所承担的真实风险视而不见。</p><p>本书也描述了金融创新以及新崛起的势力：数学和计算机专家是如何扩展市场、增加市场稳定性，从而为更多的人带去财富和荣华富贵的。量化投资如果应用得当，通常可以令市场更有效、更健康。</p><p>总而言之，这是一场平衡游戏。量化投资也是世界上最给力、最成功的投资方法，但也可以导致可怕的错误，滋生灾难性的自满。宽客必须意识到，世界并不总是风平浪静的，人类情绪可以造成他们始料未及的市场波动。<br><a id="more"></a></p>]]></content>
      
      <categories>
          
          <category> 随想 </category>
          
          <category> Baseball </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>一只特立独行的猪</title>
      <link href="/2018/02/11/%E4%B8%80%E5%8F%AA%E7%89%B9%E7%AB%8B%E7%8B%AC%E8%A1%8C%E7%9A%84%E7%8C%AA/"/>
      <url>/2018/02/11/%E4%B8%80%E5%8F%AA%E7%89%B9%E7%AB%8B%E7%8B%AC%E8%A1%8C%E7%9A%84%E7%8C%AA/</url>
      <content type="html"><![CDATA[<p>王小波的书<br><a id="more"></a></p><p>《一只特立独行的猪》</p><p>将近一个月的时间里断断续续的阅读完这本书，不得不说有些囫囵吞枣，但我本人又是一个兴趣导向的人，能断断续续的把这本书读完，排除自吹自擂的成分，确实说明这本书比较符合我的胃口。<br>此书由一个个小故事编排，时间不长，阅读随意。此时已经距离王小波先生写此书过去十多年，但我任感慨于王小波先生，敏锐的观察力和洞察力，文科部分不是我的专长，不好菲薄，但其中理科特别是零星科技的见解却符合这十多年的发展，作者阅读适应的文章，做出适应的见解，这似乎印证了，随机观点，小概率的阅读，引发大概率的观点。<br>能想来，许多内容在当时应该出格叛逆，当时在如今时代看来却有网络作者的风格，这也是一种时代的进步吧<br>在这本书中，体现了王小波先生丰富的人生阅历和当时看来深刻的思索，但愿也能有机遇体会到王先生的感悟。<br>书中有一章《海明威&lt;老人与海&gt;》<br>“那么，什么也没有得到的老人竟是胜利者？我确是这样看的。我认为，胜利就是战斗到最后的时候。老人总怀着无比的勇气走向莫测的大海，他的信心是不可战胜的。<br>他和其他许多人一样，是强悍的人类的一员。我喜欢这样的人，也喜欢这样的人性，我发现，人们常常那这样的事情当做人性最可贵的表露：七尺男子汉坐在厨房里和三姑六婆磨嘴皮子，或者衣着笔挺的男女们坐在海滨，谈论着高尚的。别人不能理解的感情。我不喜欢人们像这样沉溺在人性的软弱的部分之中，更不喜欢人们总这样描写人性。<br>正像老人每天走向大海一样，很多人每天也走向与他们的限度斗争的战场，仿佛他们要与命运一比高低似的。他们是人中的强者。<br>人类本身有自己的限度，但是当人们一再把手伸到限度以外，这个限度就一天一天地扩大了。人类在预先的斗争中成长。他们把飞船送上太空，他们也用简陋的渔具在加勒比海捕捉巨大的马林鱼。这些事情是同样伟大的。做这样不可思议的事情的人都是英雄。而那些永远不肯或不能超越自己限度的人是平庸的人。<br>在人类前进的道路上，强者与弱者的命运是不同的。弱者不羡慕强者的命运，强者也讨厌弱者的命运。强者带有人性中强悍的一面，弱者带有人性中软弱的一面。强者为弱者开辟道路，但是强者往往为弱者所奴役，就像是老人为大腹便便的游客打鱼一样。<br>《老人与海》讲了一个老渔夫的故事，但是在这个故事里却揭示了人类的共同命运。我佩服老人的勇气，佩服他不屈不挠的斗争精神，也佩服海明威。”</p><p>《关于“媚雅”》<br>“前不久在报纸上看到一篇文章，谈到“媚俗”和“媚雅”的问题。作者认为，米兰•昆德拉用出来一个词儿，叫做“媚俗”，是指艺术家为了取悦大众，放弃了艺术的格调。他还说，我们国内有些小玩闹造出个新词“媚雅”，简直不知是什么意思。这个词的意思我是知道，是指大众受到某些人的蛊惑或者误导，一味追求艺术的格调，也不问问自己是不是消受得了。在这方面，我有些经验，都与欣赏音乐有关。高雅音乐高调很高，大概没有疑问。我自己在音乐方面品味很低，乡村音乐还有听得住，再高就受不了。”</p><p>《诚实与浮嚣》<br>“人忠于已知事实叫做诚实，不忠于事实叫做虚伪。还有些人只忠于经过选择的事实，这既不叫诚实，也不叫虚伪，我把它叫做浮嚣。”</p><p>《工作与人生》<br>这是书中最后一章，前面还有96、97年《写给新的一年》。</p><p>“人活在世上，不但有身体，还有头脑和心胸——对此请勿从解剖学上理解。人脑是怎样的一种东西，科学还不能清楚。心胸是怎么一回事就更难说清。对我自己来说，心胸是我在生活中想要达到的最低目标，某件事有悖于我的心胸，我就认为它不值得一做；某个人有悖于我的心胸，我就觉得他不值得一交；某种生活有悖于我的心胸，我就会以为它不值得一过。罗素先生曾言，对人来说，不加检点的生活，确实不值得一过。我同意他的意见：不加检点的生活，属于不能接受的生活之一种。人必须过他可以接受的生活，这恰恰是他改变一切的动力。人有了心胸，就可以用它来改变自己的生活。”<br>深以为然。</p><p>“中国人喜欢接受这样的想法：只要能活着就是好的，活成什么样子无所谓。从一些电影的名字就可以看出来，《活着》、《找乐》……我对于这种想法是断然地不赞成，因为抱有这种想法的人就可能活成任何一种糟糕的样子，从而使生活本身失去意义。高尚、清洁、充满乐趣的生活是好的。人们很容易得到共识、但只有这两条远远不够。我以写作为生，我知道某种文章好，也知道某种文章坏。仅知道这两条尚不足以开始写作。还有更重要的一条，那就是：某种样子的文章对我来说是不可取的，决不能让它从我笔下写出来，冠以我的名字登在报刊上。以小喻大，这也是我的生活态度。”</p><p>只写确定无误的事情，不写旁门左道疑问重重的事情<br>《我的精神家园》<br>小时候，我曾对许多事情有很强烈的好奇心，就将少儿百科全书拿来阅读，有些内容今日看来十分错误，但在当时却深信不疑。比如UFO等，小时候不像王先生那么博闻多智，曾看鲁西西皮皮鲁等，小时候也成看什么魔方大厦现在感觉十分恐怖，但在当时却感到十分曲折，弯弯绕绕太多，这些在我心里倒也没有留下许多阴影。</p><blockquote><p>我为什么要写作》<br>小时候，看见我的姥姥写的一手好钢笔字，这里说一下，我的姥姥是个小学老师，就是那个台湾国民党连战的小学（后来还专门回来小学看过，学校也是花了十几万整理下门前道路，算是迎接连主席的面子工程吧），后来给学校写校史，小时候感觉姥姥最骄傲的事情，就是她说，她写的校史连一个字都没改过，而且这校史还是她老人家一个字一个字钢笔写出来的，最后锁在学校里。而我的姥爷是个会计，但也是个很体面的会计，跟外国文学里绅士有点类似，无论何时何地见何人，每天早上醒来，必然梳洗一番，换假牙，用发胶将头发梳的一丝不苟，连鬓角也不放过，永远喜欢涤纶的衣服因为笔挺，冬天喜欢卓别林的帽子围巾，大衣，从小就觉得很酷，跟个特务一般，姥爷退休后喜欢书法，自己钻研隶书，算是有所小成，周围有人求字也是欣然挥墨，不过，在书法上也被人骗过，有人冒充书法协会要姥爷入会员，要求姥爷的字和会费，明显后者才是目的，但是姥爷也是确信不疑，家人劝阻全然不顾，大有古人虽万人劝阻，吾亦往亦的架势，上当几次后也是明白过来，再接到同样书信自嘲都是骗子，但是却有杂志将姥爷的字刊登出来，还寄了基本样本回来，都被姥爷宝贝般的收藏起来。<br>小学作文常常被当做范文也是受到姥姥的影响吧，姥爷无论何时何地熨帖的发胶总是给我留下深刻的印象。</p><p>我的师承<br>数学，小时候盛行奥数，小学四五年级就开始上奥数班，一开始几个没多大印象，印象最深的是一个叫刘立新的老师，他是教育学院的老师，就是给老师上课的老师，估计是个大学讲师吧（小学奥数就是个大学讲师上课，确实有点高规格），他在博物馆对面租个研究所厂区，周六日四个半天不停歇，每个班上半天课，他上课颇有些心意，他自己编题，不过都是有根源的，几百道题印在几十页纸上，只有题，没有其他任何东西，怎么上课也有新意，讲个题目意思解决语文理解的问题，有时什么都不讲，学生自己做，做出来的举手，等有一半做出来的就开始讲题，反正学生是很竞争了，过段时间还分班，举手越快的做的越靠前。所以第一排就是一些看见题什么都不用讲自己就能做出来的学生，我当时也是坐在中间吧，对前排学生不是羡慕，而是一种膜拜，一个小学生就已经独立完成当时高中难度的题目，还不用讲解，估计都是清华北大的苗子吧，感慨神人再世，后来小学毕业，也就结束了，后来这批人中的某些继续跟着刘老师升到初中难度，我是没有这种待遇了，虽然初中在数理化竞赛中都拿到名次，但我明白天外有天，人外有人。还有段时间是一个姓黄还是庄的老教授，神情狡黠，目光深邃，我们的材料也是他老人家用一种很规整却不是标准的字体书写，再用紫印纸，印在下面五六张上，而最上面几张效果最好，能拿到都是一种荣幸。<br>初中到高中，数学辅导一直是一个大学退休的教授辅导，所有教案都是老人亲笔书写，再复印几本，老人讲课十分细腻，所以有时我听得索然无味，可惜了高考数学太简单，这若干年的培养没有在试卷里发挥多大作用。不过老人严谨的态度后来对我的影响颇深，一笔一划，上下求索。</p></blockquote><p>《个人尊严》<br>“在国外时看到，人们对时事做出价值评判时，总是从两个独立的方面来进行：一个方面是国家或者社会的尊严，这像是时事的经线；另一个方面是人的尊严，这像是时事的纬线。回到国内，一条纬线就像是没有了，连尊严这个词也感到陌生。”<br>“提到尊严这个概念，我首先想到英文单词dignity，然后才想到相应的的中文词。在英文中，这个词不仅有尊严之义，还有体面、身份的意思。尊严不但指人受到尊重，它还是人价值之所在。从上古到现代，数以万计的中国人里，没有几个人有过属于个人的尊严。举个大点的例子，中国历史上有过皇上对大臣施廷仗的事，无论多大的官，一言不合，就有可能受到如此当众羞辱，……”<br>“举个小点的例子，没到春运高潮，大家就会在传媒上看到一辆硬座车厢里挤了三四百人，厕所里也挤了十几人。谈到这件事，大家会说国家的铁路需要建设，说道铁路工人的工作难做，提到安全问题，提到所有方面，就是不提这些民工这样挤在一起，完全没有了个人尊严——仿佛这件事很不重要似的。……”</p>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>三体-星际穿越</title>
      <link href="/2018/02/02/%E4%B8%89%E4%BD%93-%E7%BB%B4%E5%BE%B7%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E7%94%B7%E4%B8%BB/"/>
      <url>/2018/02/02/%E4%B8%89%E4%BD%93-%E7%BB%B4%E5%BE%B7%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E7%94%B7%E4%B8%BB/</url>
      <content type="html"><![CDATA[<p>我是在敦煌的戈壁滩上，看完整个《三体》，</p><p>1月份敦煌的戈壁滩，气候极其干燥，没有一丝下雪的痕迹，室外阳光高照，但总是狂风大作，仿佛要狠狠的从戈壁滩上刮走什么，在这种时候仍需要每日进入戈壁腹地进行测试工作，</p><p>清晨赶着晨曦上车，左右颠簸中来到地图上也没标记的地点，从车上下车前，每人都会领到帽子护膝围脖等用品，那几日，往往赶上狂风大作，下车后每一步都不像是走路，而是与大风作对，风沙从你的衣领裤腿缝隙钻入，的工作听着风从车的缝隙中钻入，人在外面没有防护措施的呆不过十分钟，就会被大风带走身上大部分的热量，人在车外工作十分钟就会迫不及待的跳上车子，在这里，前前后后十几公里都不会有人存在，整个车子都沉浸在风沙中，你只会觉得仿佛在外星球，而你坐的车就是你的飞船，你只会觉得整个戈壁滩上，只有这个车子才是你唯一的依靠，出了车子，你绝对活不过数十分钟的。<br>就是在这种环境下，我看完了整个三体，<br>将车想做是飞船，将外面的戈壁滩想做的荒凉的沙漠，<br>而我们的保证仅仅是几厘米厚的钢板，</p><p>其中的故事情节，有太多的解读，这里写下我的感受最深的。<br>维德-为了目的不择手段，是为了人类的延续，真正的男人。不达目的誓不罢休，可惜他与书中的当权者，或者是外星人的选择无关，如果设想最后的执剑人是维德，恐怕最后的结局也会改写吧。<br>反观女主，优柔寡断，在关键时刻，没能把握机会，从一开始，三体人就布下所有的计划，等罗辑交出执剑人身份的时候，就发动进攻。</p><p>星际穿越，也是我看的感触比较深的电影，<br>一、是在男主骗女主要去新的星球，最后将机会给了女主，那句“物理学第二定律，总要留下些什么”<br>二、结尾，男主在恢复后，义无反顾的“偷来”宇航船，去奔向女主，不论是什么原因，爱情还是同甘共苦的情谊，感受到了人类的顽强生生不息，这种感觉不是简单的迸发，而是整个电影的铺垫，</p><p>《星际穿越》诺兰大神，拍出了不一样的意味，整个电影不像一部电影，而像一部纪录片，其中无数的人性冲突，理性的煎熬，诺兰德早就知道计算结果的局限，直到生命的最后一刻，才说出那句对不起，男主的女儿，苦苦思索，到电影最后，终于思索出，书架上的暗示，整个电影不喜不悲，或者说已经很艺术的处理物理环境的恶劣，人类无论是哲学上的处于“不自知”的状态，还是在整个宇宙环境下，都是脆弱的，人类距离外太空，只有几公里的大气层和地球的磁场的保护，没有的磁场保护，人类只能存活半小时，没有大气层，人类连十分钟都活不到，</p><p>在这部电影中，我看出了不一样的情节。“无论乐观悲观，只想完成任务。”电影简化了人性的选择，除了马特戴蒙的角色，无一不是按照使命完成任务，他们在物理条件面前，想到过自己么，理性会高于人性，这19/12位宇航员，都是这样的么。</p><p>最后的男主被未来人类拯救来到高维空间，这里有一个疑问，此时被拯救的男主改变了时间线，必将产生新的未来人类，旧的未来人类消失，根据“祖父悖论”，那也就不会去高维空间，那也就不会解决公式，那新的未来人类也不会产生，但时间线继续发展，还会产生旧的未来人类，旧的未来人类继续拯救男主，产生新的未来人类，旧的未来人类消失……以此类推，人类历史会不会形成闭环？<br>或者新的未来人类仍然拯救男主，来到高维空间……此时旧的未来人类和新的未来人类在时间线中做出了同样一件事情。那根据电影中的设定，他们必然通过重力进行沟通，（设定每次的选择都会产生新的时间线，新的宇宙，这也是平行宇宙的理解）</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>03机器学习实战-第3章 决策树</title>
      <link href="/2018/01/28/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC3%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2018/01/28/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC3%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><h1 id="第3章-决策树"><a href="#第3章-决策树" class="headerlink" title="第3章 决策树"></a>第3章 决策树</h1><h2 id="本章内容"><a href="#本章内容" class="headerlink" title="本章内容"></a>本章内容</h2><blockquote><ul><li>决策树简介  </li><li>在数据集中度量一致性  </li><li>使用递归构造决策树     </li><li>使用Matplotlib绘制树形图   </li></ul></blockquote><h2 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h2><blockquote><p>优点：计算复杂度不高，输出易于理解，对中间值得确实不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题<br>使用数据类型：数值型和标称型  </p></blockquote><p><strong>创建分支伪代码函数createBranch()如下：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">检测数据集中的每个指向是否属于同一个分类：</span><br><span class="line">    IF so return 类标签</span><br><span class="line">    Else</span><br><span class="line">        寻找划分数据集的最好特征</span><br><span class="line">        划分数据集</span><br><span class="line">        创建分支节点</span><br><span class="line">            for 每个划分的子集</span><br><span class="line">                调用函数createBranch并增加返回结果到分支节点中</span><br><span class="line">        return 分支节点</span><br></pre></td></tr></table></figure></p><p>上述是一个递归函数</p><h2 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h2><blockquote><p>(1) 收集数据：可以使用任何方法。<br>(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>(4) 训练算法：构造树的数据结构。<br>(5) 测试算法：使用经验树计算错误率。<br>(6) 使用算法：词步骤可以使用于任何监督学习算法，而使用决策树可能更好地理解数据的内在含义。  </p></blockquote><p><strong> 摘要</strong></p><ol><li>信息论相关知识</li><li>决策树算法原理</li><li>代码实现与解释   </li></ol><p>今天总结决策树算法，目前建立决策树有三种主要算法：ID3、C4.5以及CART。由于算法知识点比较琐碎，我分成两节来总结。</p><p>第一节主要是梳理决策树算法中ID3和C4.5的知识点；第二节主要梳理剪枝技术、CART算法和随机森林算法的知识。</p><h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><h3 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1.信息熵"></a>1.信息熵</h3><p>在决策树算法中，熵是一个非常非常重要的概念。</p><p>一件事发生的概率越小，我们说它所蕴含的信息量越大。</p><p>比如：我们听女人能怀孕不奇怪，如果某天听到哪个男人怀孕了，我们就会觉得emmm…信息量很大了。</p><p>所以我们这样衡量信息量：</p><script type="math/tex; mode=display">i(y)=-log{P(y)}</script><p>其中，$P(y)$是事件发生的概率。</p><p>信息熵就是所有可能发生事件的信息量的期望：  </p><script type="math/tex; mode=display">H(Y)=-\sum_{i=1}^{n}P(y_i)log{P(y_i)}</script><p>表达了$Y$事件发生的不确定度。  </p><h3 id="2-条件熵"><a href="#2-条件熵" class="headerlink" title="2.条件熵"></a>2.条件熵</h3><p>条件熵：表示在X给定条件下，$Y$的条件概率分布的熵对$X$的数学期望。其数学推导如下：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y|X) & =\sum_{x\in{X}}P{(x)}H(Y|X=x) \\   & =-\sum_{x\in{X}}P(x)\sum_{y\in{Y}}P(y|x)log{P(y|x)}\\   & =-\sum_{x\in{X}}\sum_{y\in{Y}}P(x,y)log{P(y|x)}\end{aligned}</script><p>条件熵$H（Y|X）$表示在已知随机变量$X$的条件下随机变量Y的不确定性。注意一下，条件熵中X也是一个变量，意思是在一个变量$X$的条件下（变量$X$的每个值都会取到），另一个变量$Y$的熵对$X$的期望。</p><p>举个例子</p><p>例：女生决定主不主动追一个男生的标准有两个：颜值和身高，如下表所示：</p><div class="table-container"><table><thead><tr><th></th><th>颜值</th><th>身高</th><th>追不追</th></tr></thead><tbody><tr><td>1</td><td>帅</td><td>高</td><td>追</td></tr><tr><td>2</td><td>帅</td><td>不高</td><td>追</td></tr><tr><td>3</td><td>不帅</td><td>高</td><td>不追</td></tr></tbody></table></div><p>上表中随机变量$Y=\{追，不追\}$，$P(Y=追)=2/3$，$P(Y=不追)=1/3$，得到$Y$的熵：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y) & =-\frac{2}{3}log\frac{2}{3}-\frac{1}{3}log\frac{1}{3} \\   & =0.918\end{aligned}</script><p>这里还有一个特征变量$X$，$X=｛高，不高｝$。当$X=高$时，追的个数为1，占1/2，不追的个数为1，占1/2，此时：</p><script type="math/tex; mode=display">H(Y|X=高)=-\frac{1}{2}log\frac{1}{2}-\frac{1}{2}log\frac{1}{2}</script><p>同理：</p><script type="math/tex; mode=display">H(Y|X=不高)=-{1}log{1}-{1}log{1}</script><p>（注意：我们一般约定，当$p=0$时，$plogp=0$）</p><p>所以我们得到条件熵的计算公式：  </p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y|X=身高) & =P(X=不高)*H(Y|X=不高)+P(X=高)*H(Y|X=高)\\            & =0.67\end{aligned}</script><h3 id="3-信息增益"><a href="#3-信息增益" class="headerlink" title="3.信息增益"></a>3.信息增益</h3><p>当我们用另一个变量$X$对原变量$Y$分类后，原变量$Y$的不确定性就会减小了（即熵值减小）。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：</p><script type="math/tex; mode=display">Gain(Y,X)=H(Y)-H(Y|X)</script><p>此外，信息论中还有互信息、交叉熵等概念，它们与本算法关系不大，这里不展开。 </p><h2 id="代码实现与解读"><a href="#代码实现与解读" class="headerlink" title="代码实现与解读"></a>代码实现与解读</h2><p><strong>1.计算给定数据的香浓熵 </strong>    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算给定数据集的香农熵</span></span><br><span class="line"><span class="comment">#从math中导入log函数</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)   <span class="comment">#计算实例中的个数</span></span><br><span class="line">    </span><br><span class="line">    labelCounts = &#123;&#125;    <span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:    <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">        </span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]    <span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():     <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">            </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>       <span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">          </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>     <span class="comment">#标签为currentLabel的个数加1       </span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries    <span class="comment">#计算每一个标签的概率p</span></span><br><span class="line">        </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)    <span class="comment">#log base 2利用公式计算香农熵</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="comment">#change to discrete values</span></span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line">myDat,labels=createDataSet()</span><br><span class="line">myDat,labels</span><br></pre></td></tr></table></figure><pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]], [&#39;no surfacing&#39;, &#39;flippers&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calcShannonEnt(myDat)</span><br></pre></td></tr></table></figure><pre><code>0.9709505944546686</code></pre><p><strong>2.创建选取的数据特征属性划分数据集</strong></p><p>程序清单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照给定特征划分数据集</span></span><br><span class="line"><span class="comment">#参数解释：dataSet待划分数据集</span></span><br><span class="line"><span class="comment">#axis：划分数据集的特征，这个函数里指函数第几列</span></span><br><span class="line"><span class="comment">#value：特征返回值，指的是特征划分的标准</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []     <span class="comment">#创建一个新列表</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:   <span class="comment">#如果这组数据特征值等于特征返回值的话</span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec = featVec[:axis]       <span class="comment">#这两行是把原来的数据除掉划分数据的特征那一列 </span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)   <span class="comment">#把列表reduceFeatVect放入retDataSet中</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">1</span>) </span><br><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line"><span class="comment"># 将myDat的第1列按照取出所有等于1的方式划分</span></span><br></pre></td></tr></table></figure><pre><code>[[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>[[1, &#39;no&#39;], [1, &#39;no&#39;]]</code></pre><p><strong>3.根据信息增益准则，选取最好的划分特征</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#找到最好的数据集划分方式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>    <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)   <span class="comment">#计算数据原始香农熵</span></span><br><span class="line">   </span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>   <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):       </span><br><span class="line">        </span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">        </span><br><span class="line">        uniqueVals = set(featList)    <span class="comment">#set()，生成一个集合数据类型，其和列表类型一样，不同之处在于</span></span><br><span class="line">                                      <span class="comment">#集合数据类型里面的值不重复，是唯一的</span></span><br><span class="line">        </span><br><span class="line">        newEntropy = <span class="number">0.0</span>    <span class="comment">#初始化新熵为0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:    <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">           </span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)    <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">            </span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))   <span class="comment">#计算概率</span></span><br><span class="line">            </span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)    <span class="comment">#计算新熵 </span></span><br><span class="line">       </span><br><span class="line">        infoGain = baseEntropy - newEntropy     <span class="comment">#计算信息增益calculate the info gain; ie reduction in entropy</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):      <span class="comment">#得到最大的信息增益和选取特征 #compare this to the best gain so far</span></span><br><span class="line">            bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature                      <span class="comment">#returns an integer</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">    <span class="comment">#计算数据原始香农熵</span></span><br><span class="line"><span class="comment"># numFeatures</span></span><br><span class="line">baseEntropy = calcShannonEnt(myDat)</span><br><span class="line">print(<span class="string">"numFeatures=%d"</span> %numFeatures) </span><br><span class="line">print(<span class="string">"原始熵是："</span>,baseEntropy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):        <span class="comment">#iterate over all the features</span></span><br><span class="line">        <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(<span class="string">"第%d个特征的所有取值"</span> %i,featList)</span><br><span class="line">    </span><br><span class="line">    uniqueVals = set(featList) </span><br><span class="line">    <span class="comment">#初始化新熵为0#get a set of unique values</span></span><br><span class="line">    newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">    print(<span class="string">"-简化取值:"</span>,uniqueVals)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">        subDataSet = splitDataSet(myDat, i, value)</span><br><span class="line">        print(<span class="string">"--按照%d划分取值"</span>% value,subDataSet)</span><br><span class="line">            <span class="comment">#计算概率</span></span><br><span class="line">        prob = len(subDataSet)/float(len(myDat))</span><br><span class="line">            <span class="comment">#计算新熵</span></span><br><span class="line">        print(<span class="string">"---去此值的概率是："</span>,prob)</span><br><span class="line">        newEntropy += prob * calcShannonEnt(subDataSet)   </span><br><span class="line">            <span class="comment">#计算信息增益</span></span><br><span class="line">        print(<span class="string">"---新熵是"</span>,newEntropy)</span><br><span class="line">    infoGain = baseEntropy - newEntropy     <span class="comment">#calculate the info gain; ie reduction in entropy</span></span><br><span class="line">    print(<span class="string">"-----信息增益"</span>,infoGain)</span><br><span class="line">        <span class="comment">#得到最大的信息增益和选取特征</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (infoGain &gt; bestInfoGain):       <span class="comment">#compare this to the best gain so far</span></span><br><span class="line">        bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">        bestFeature = i</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"此时最好的熵是"</span>,bestInfoGain,<span class="string">"此时最佳特征值是"</span>,bestFeature)</span><br></pre></td></tr></table></figure><pre><code>numFeatures=2原始熵是： 0.9709505944546686第0个特征的所有取值 [1, 1, 1, 0, 0]-简化取值: {0, 1}--按照0划分取值 [[1, &#39;no&#39;], [1, &#39;no&#39;]]---去此值的概率是： 0.4---新熵是 0.0--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]---去此值的概率是： 0.6---新熵是 0.5509775004326937-----信息增益 0.4199730940219749此时最好的熵是 0.4199730940219749 此时最佳特征值是 0第1个特征的所有取值 [1, 1, 0, 1, 1]-简化取值: {0, 1}--按照0划分取值 [[1, &#39;no&#39;]]---去此值的概率是： 0.2---新熵是 0.0--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;], [0, &#39;no&#39;]]---去此值的概率是： 0.8---新熵是 0.8-----信息增益 0.17095059445466854此时最好的熵是 0.4199730940219749 此时最佳特征值是 0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chooseBestFeatureToSplit(myDat)</span><br></pre></td></tr></table></figure><pre><code>0</code></pre><p><strong>从数据集构造决策树算法：其工作原理如下：</strong></p><ol><li>得到原始数据集  </li><li>基于最好的属性值划分数据集（可能存在大于两个分支的数据集划分）    </li><li>第一次划分后，数据被向下传递到树分支的下一个节点（可以用递归的思想）</li></ol><p><strong>递归的条件： </strong><br>程序遍历完所有划分数据集属性，或者每个分支下的所有实例都具有相同的分支。</p><p><strong>4.多数表决器</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多数表决器</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    </span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="comment">#for程序用来计数</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): </span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#排序函数</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>5.创建决策树</strong>  </p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建树</span></span><br><span class="line"><span class="comment">#     myDat  = [[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no']]</span></span><br><span class="line"><span class="comment">#     labels = ['no surfacing','flippers']</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#classLsit里面是dataSet里面的标签</span></span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList): <span class="comment">#第一个终止条件：所有类标签都相同，country（）函数用来计数0</span></span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]<span class="comment">#stop splitting when all of the classes are equal</span></span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: <span class="comment">#第二个终止条件：用完了所有的特征#stop splitting when there are no more features in dataSet</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#得到标签里的所有属性值</span></span><br><span class="line">    uniqueVals = set(featValues)<span class="comment">#得到属性值集合</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]       <span class="comment">#copy all of labels, so trees don't mess up existing labels</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br></pre></td></tr></table></figure><pre><code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}</code></pre><p><strong>6.使用决策树进行分类</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用决策树分类函数</span></span><br><span class="line"><span class="comment">#三个参数意义：input：决策树；featLabels：特征标签；testVec：测试向量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure><p><strong>7.决策树在磁盘中的存储与导入</strong>  </p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#将磁盘中的对象加载出来，这里filename就是上面函数中的txt文件</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br><span class="line"><span class="comment"># storeTree(myTree,'classifierStorage.txt')</span></span><br><span class="line"><span class="comment"># grabTree('classifierStorage.txt')</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------IndexError                                Traceback (most recent call last)&lt;ipython-input-16-33c9af9c39fa&gt; in &lt;module&gt;()----&gt; 1 createTree(myDat,labels)      2 # storeTree(myTree,&#39;classifierStorage.txt&#39;)      3 # grabTree(&#39;classifierStorage.txt&#39;)&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)     21     for value in uniqueVals:     22         subLabels = labels[:]       #copy all of labels, so trees don&#39;t mess up existing labels---&gt; 23         myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)     24     return myTree&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)     14         return majorityCnt(classList)     15     bestFeat = chooseBestFeatureToSplit(dataSet)---&gt; 16     bestFeatLabel = labels[bestFeat]     17     myTree = {bestFeatLabel:{}}     18     del(labels[bestFeat])IndexError: list index out of range</code></pre><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li>CART决策树</li><li>决策树的剪枝技术</li><li>Bagging与随机森林</li><li>决策树中缺失值的处理</li><li>决策树代码</li></ol><p>注：本节代码对应第九章“树回归”</p><h2 id="CART决策树"><a href="#CART决策树" class="headerlink" title="CART决策树"></a>CART决策树</h2><p>为什么同样作为建立决策树的三种算法之一，我们要将CART算法单独拿出来讲。</p><p>因为ID3算法和C4.5算法采用了较为复杂的熵来度量，所以它们只能处理分类问题。而CART算法既能处理分类问题，又能处理回归问题。</p><p>对于分类树，CART采用基尼指数最小化准则；对于回归树，CART采用平方误差最小化准则</p><h3 id="1-CART分类树"><a href="#1-CART分类树" class="headerlink" title="1.CART分类树"></a>1.CART分类树</h3><p>CART分类树与上一节讲述的ID3算法和C4.5算法在原理部分差别不大，唯一的区别在于划分属性的原则。CART选择“基尼指数”作为划分属性的选择。</p><p>Gini指数作为一种做特征选择的方式，其表征了特征的不纯度。</p><p>在具体的分类问题中，对于数据集D，我们假设有K个类别，每个类别出现的概率为$P_k$，则数据集$D$的基尼指数的表达式为：</p><script type="math/tex; mode=display">Gini=1-\sum_{k=1}^{K}{P_k}^2</script><p>我们取一个极端情况，如果数据集合中的类别只有一类，那么：</p><script type="math/tex; mode=display">Gini(D)=0</script><p>我们发现，当只有一类时，数据的不纯度是最低的，所以Gini指数等于零。Gini(D)越小，则数据集D的纯度越高。</p><p>特别地，对于样本D，如果我们选择特征A的某个值a，把D分成$D_1$和$D_2$两部分，则此时，Gini指数为：  </p><script type="math/tex; mode=display">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>与信息增益类似，我们可以计算如下表达式：  </p><script type="math/tex; mode=display">\Delta{Gini(A)}=Gini(D)-Gini(D,A)</script><p>即以特征A划分后，数据不纯度减少的程度。显然，我们在做特征选取时，应该选择最大的一个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat,labels</span><br></pre></td></tr></table></figure><pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]], [&#39;no surfacing&#39;, &#39;flippers&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">currentLabel</span><br></pre></td></tr></table></figure><pre><code>&#39;no&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">labelCounts = &#123;&#125;<span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]<span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">    <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">        labelCounts[currentLabel] = <span class="number">0</span><span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">    labelCounts[currentLabel] += <span class="number">1</span><span class="comment">#标签为currentLabel的个数加1</span></span><br><span class="line">labelCounts</span><br></pre></td></tr></table></figure><pre><code>{&#39;yes&#39;: 2, &#39;no&#39;: 3}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(featList)</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 0, 0][1, 1, 0, 1, 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>[1, 1, &#39;yes&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">numFeatures</span><br></pre></td></tr></table></figure><pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><pre><code>01</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">classList</span><br></pre></td></tr></table></figure><pre><code>[&#39;yes&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;no&#39;, &#39;no&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bestFeat = chooseBestFeatureToSplit(myDat)</span><br><span class="line">bestFeatLabel = labels[bestFeat]</span><br><span class="line">myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">myTree</span><br></pre></td></tr></table></figure><pre><code>{&#39;no surfacing&#39;: {}}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">featValues</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 0, 0]</code></pre><p>至此，我们完成了决策树算法原理和主要代码的学习。</p><p>下一节我们将学习CART算法、随机森林算法以及剪枝技术。</p><p>以上原理部分主要来自于《机器学习》—周志华，《统计学习方法》—李航，《机器学习实战》—Peter Harrington。代码部分主要来自于《机器学习实战》，我对代码进行了版本的改进，文中代码用Python3实现，这是机器学习主流语言，本人也会尽力对代码做出较为详尽的注释。</p><h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p><strong>熵</strong>： 熵（entropy）指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p><p><strong>信息熵（香农熵）</strong>： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p><p>信息增益： 在划分数据集前后信息发生的变化称为信息增益。</p><h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br>我们使用 createBranch() 方法，如下所示：</p><blockquote><p>检测数据集中的所有数据的分类标签是否相同:<br>         If so return 类标签<br>            Else:<br>                寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）<br>                划分数据集<br>                创建分支节点<br>                        for 每个划分的子集<br>                                调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中<br>                return 分支节点    </p></blockquote><p>​        </p><h4 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h4><blockquote><p>收集数据：可以使用任何方法。<br>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>训练算法：构造树的数据结构。<br>测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充）<br>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。     </p></blockquote><p><strong>决策树 算法特点</strong></p><blockquote><p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题。<br>适用数据类型：数值型和标称型。</p></blockquote><h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>1.算法简介</strong></p><p>决策树算法是一类常见的分类和回归算法，顾名思义，决策树是基于树的结构来进行决策的。</p><p>以二分类为例，我们希望从给定训练集中学得一个模型来对新的样例进行分类。</p><p><strong>举个例子</strong></p><p>有一个划分是不是鸟类的数据集合，如下：</p><div class="table-container"><table><thead><tr><th></th><th>是否会飞</th><th>是否会跑</th><th>属于鸟类</th></tr></thead><tbody><tr><td>1</td><td>是</td><td>是</td><td>否</td></tr><tr><td>2</td><td>是</td><td>是</td><td>是</td></tr><tr><td>3</td><td>是</td><td>否</td><td>否</td></tr><tr><td>4</td><td>否</td><td>是</td><td>否</td></tr><tr><td>5</td><td>否</td><td>是</td><td>否</td></tr></tbody></table></div><p>这时候我们建立这样一个决策树：  </p><p><img src="https://pic4.zhimg.com/80/v2-4a601bdc74abb553c0873fbd61597035_hd.jpg" ,width="400,height=400"></p><p>当我们有了一组新的数据时，我们就可以根据这个决策树判断出是不是鸟类。创建决策树的伪代码如下：  </p><p><img src="https://pic4.zhimg.com/80/v2-c226901dc50538bd40410e7aae938f47_hd.jpg" ,width="400,eight=400"></p><p>生成决策树是一个递归的过程，在决策树算法中，当出现下列三种情况时，导致递归返回： </p><p>(1)当前节点包含的样本属于同一种类，无需划分；</p><p>(2)当前属性集合为空，或者所有样本在所有属性上取值相同，无法划分；</p><p>(3)当前节点包含的样本集合为空，无法划分。</p><p><strong>2.属性选择</strong></p><p>在决策树算法中，最重要的就是划分属性的选择，即我们选择哪一个属性来进行划分。三种划分属性的主要算法是：ID3、C4.5以及CART。</p><p><strong>2.1 ID3算法</strong></p><p>ID3算法所采用的度量标准就是我们前面所提到的“信息增益”。当属性a的信息增益最大时，则意味着用a属性划分，其所获得的“纯度”提升最大。我们所要做的，就是找到信息增益最大的属性。由于前面已经强调了信息增益的概念，这里不再赘述。</p><p><strong>2.2 C4.5算法</strong></p><p>实际上，信息增益准则对于可取值数目较多的属性会有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：  </p><script type="math/tex; mode=display">Gain\_ratio(Y,X)=\frac{Gain(Y,X)}{H(X)}</script><p>其中，分子为信息增益，分母为属性$X$的熵。</p><p>需要注意的是，增益率准则对可取值数目较少的属性有所偏好。</p><p>所以一般这样选取划分属性：<strong>先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</strong>。</p><p><strong>2.3 CART算法</strong></p><p>ID3算法和C4.5算法主要存在三个问题：</p><p>(1)每次选取最佳特征来分割数据，并按照该特征的所有取值来进行划分。也就是说，如果一个特征有4种取值，那么数据就将被切成4份，一旦特征被切分后，该特征就不会再起作用，有观点认为这种切分方式过于迅速。</p><p>(2)它们不能处理连续型特征。只有事先将连续型特征转换为离散型，才能在上述算法中使用。</p><p>(3)会产生过拟合问题。</p><p>为了解决上述(1)、(2)问题，产生了CART算法，它主要的衡量指标是基尼系数。为了解决问题(3)，主要采用剪枝技术和随机森林算法，这部分内容，下一次再详细讲述。</p><p>上述就是决策树算法的原理部分，下面展示完整代码和注释。代码中主要采用的是ID3算法。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>01机器学习实战-机器学习基础</title>
      <link href="/2018/01/27/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC1%E7%AB%A0%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/27/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC1%E7%AB%A0%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><ul><li>主要来源自《机器学习实战》《机器学习》《利用Python进行数据分析》，还有一些网站资料</li></ul><h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><h2 id="第1章-机器学习基础"><a href="#第1章-机器学习基础" class="headerlink" title="第1章 机器学习基础"></a>第1章 机器学习基础</h2><p><strong>机器学习概述</strong></p><p>机器学习就是无序的数据转换成有用信息。</p><blockquote><ol><li><p>获取海量的数据   </p></li><li><p>从海量数据中获取有用信息</p></li></ol></blockquote><h3 id="机器学习场景"><a href="#机器学习场景" class="headerlink" title="机器学习场景"></a>机器学习场景</h3><blockquote><p>例如：识别动物猫<br>模式识别（官方标准）：人们通过大量的经验，得到结论，从而判断它就是猫。<br>机器学习（数据学习）：人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。<br>深度学习（深入数据）：人们通过深入了解它，发现它会’喵喵’的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域：语音识别、图像识别）   </p></blockquote><p>模式识别（pattern recognition）：模式识别是最古老的（作为一个术语而言，可以说是很过时的）。<br>    我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。<br>    通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。    </p><p>机器学习（machine learning）：机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。<br>    在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。<br>    “机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。<br>    机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。    </p><p>深度学习（deep learning）：深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。<br>    深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</p><p>参考地址： </p><p><a href="http://www.csdn.net/article/2015-03-24/2824301" target="_blank" rel="noopener">http://www.csdn.net/article/2015-03-24/2824301</a></p><p><a href="http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q" target="_blank" rel="noopener">http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q</a></p><blockquote><p>机器学习已应用于多个领域，远远超出大多数人的想象，横跨：计算机科学、工程技术和统计学等多个学科。</p></blockquote><ul><li>搜索引擎：根据你的搜索点击，优化你下次的搜索结果。   </li><li>垃圾邮件：会自动的过滤垃圾广告邮件到垃圾箱内。   </li><li>超市优惠券：你会发现，你在购买小孩子尿布时候，售货员会赠送给你一张优惠券可以兑换免费啤酒。  </li><li>邮件邮寄：手写软件自动识别寄送贺卡的地址。  </li><li>申请贷款：通过你最近的金融活动信息进行综合评定，决定你是否合格。  </li></ul><h3 id="机器学习组成"><a href="#机器学习组成" class="headerlink" title="机器学习组成"></a>机器学习组成</h3><p><strong>主要任务</strong></p><ul><li>分类：将实例数据划分到合适的类别中。  </li><li>回归：主要用于预测数值型数据（示例：数据通过给定数据点耐力和最优曲线）   </li></ul><p><strong>监督学习</strong></p><ul><li>必须确定目标变量的值，以便机器学习可以发现特征值和目标变量之间的关系。（包括分类和回归）  </li><li>样本集：训练数据+测试数据<ul><li>训练样本=特征（feature）+目标变量（label：分类-离散值/回归-连续值）  </li><li>特征通常是训练样本集的列，它们是独立测量得到的。 </li><li>目标变量：目标变量是机器学习预测算法的测试结果。<ul><li>在分类算法中目标变量的类型通常是标称型（如：真与假），而在回归算法中通常是连续型（如：1~100）</li></ul></li></ul></li></ul><ul><li>知识表示：<br>1.可以采用规则集的形式【例如：数学成绩大于90分为优秀】<br>2.可以采用概率分布的形式【例如：通过统计分布，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】<br>3.可以使用训练样本集中的一个实例【例如：通过样本集合，我们训练处一个模型实例，得出年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】</li></ul><p><strong>非监督学习</strong>  </p><ul><li>数据没有类别，也不会给定目标值。  </li><li>聚类：在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。  </li><li>此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。</li></ul><p><strong>训练过程</strong>  </p><p><img src="https://pic4.zhimg.com/80/v2-711466f238bbb969618e6fe669a16a4f_hd.jpg" ,width="400,height=400"></p><p><strong>算法汇总</strong>   </p><center>**用于执行分类、回归、聚类和密度估计的机器学习算法**</center><div class="table-container"><table><thead><tr><th><p align="center">监督学习的用途</p></th></tr></thead><tbody><tr><td><p align="left">k-近邻算法</p></td><td><p align="left">线性回归</p></td></tr><tr><td><p align="left">朴素贝叶斯算法</p></td><td><p align="left">局部加权线性回归</p></td></tr><tr><td><p align="left">支持向量机</p></td><td><p align="left">Ridge回归</p></td></tr><tr><td><p align="left">决策树</p></td><td><p align="left">Lasso最小回归系数估计</p></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th><p align="center">无监督学习的用途</p></th></tr></thead><tbody><tr><td><p align="left">K-均值</p></td><td><p align="left">最大期望算法</p></td></tr><tr><td><p align="left">DBSCAN</p></td><td><p align="left">Parzen窗设计</p></td></tr></tbody></table></div><p>机器学习使用</p><blockquote><p>选择算法需要考虑的两个问题</p></blockquote><p>1.算法场景  </p><ul><li>预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法   </li><li>给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。    </li></ul><p>2.需要收集或分析的数据是什么  </p><blockquote><p>举例</p></blockquote><p><img src="https://pic3.zhimg.com/80/v2-88ee740c5e4a2a2bdab7e08043321e08_hd.jpg" ,width="400,height=400"></p><blockquote><p>机器学习开发流程</p></blockquote><ul><li>收集数据: 收集样本数据</li><li>准备数据: 注意数据的格式</li><li>分析数据: 为了确保数据集中没有垃圾数据；<br>  如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；<br>  另外该步骤需要人工干预，会降低自动化系统的价值。</li><li>训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤</li><li>测试算法: [机器学习算法核心]评估算法效果</li><li>使用算法: 将机器学习算法转为应用程序</li></ul><p><strong>Python语言优势</strong></p><ol><li>可执行伪代码</li><li>Python比较流行：使用广泛、代码范例多、丰富模块库，开发周期短</li><li>Python语言的特色：清晰简练、易于理解</li><li>Python语言的缺点：唯一不足的是性能问题</li><li>Python相关的库    </li></ol><ul><li>科学函数库：SciPy、NumPy（底层语言：C和Fortran）</li><li>绘图工具库：Matplotlib</li></ul><h3 id="科普"><a href="#科普" class="headerlink" title="科普"></a>科普</h3><p>　　机器学习（Machine Learning）专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合。  ——来自 百度百科</p><p>简单来讲，机器学习就是一门让机器能够进行自我学习并不断优化功能的学科。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>00机器学习实战-机器学习的数学基础</title>
      <link href="/2018/01/26/00%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/26/00%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><h1 id="机器学习：机器学习的数学基础"><a href="#机器学习：机器学习的数学基础" class="headerlink" title="机器学习：机器学习的数学基础"></a>机器学习：机器学习的数学基础</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>机器学习的特点就是：以计算机为工具和平台，以数据为研究对象，以学习方法为中心；<br>是概率论、线性代数、数值计算、信息论、最优化理论和计算机科学等多个领域的交叉学科。</p><h2 id="二、线性代数"><a href="#二、线性代数" class="headerlink" title="二、线性代数"></a>二、线性代数</h2><h3 id="2-1-标量"><a href="#2-1-标量" class="headerlink" title="2.1 标量"></a>2.1 标量</h3><p>一个标量就是一个单独的数，一般用小写的的变量名称表示。</p><h3 id="2-2-向量"><a href="#2-2-向量" class="headerlink" title="2.2 向量"></a>2.2 向量</h3><p>一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱：</p><script type="math/tex; mode=display">X=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\quad</script><p>我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。 </p><h3 id="2-3-矩阵"><a href="#2-3-矩阵" class="headerlink" title="2.3 矩阵"></a>2.3 矩阵</h3><p>矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如$A$。如果一个实数矩阵高度为$m$，宽度为$n$，那么我们说$A\epsilon R^{m\times n}$。</p><script type="math/tex; mode=display">A=\begin{bmatrix}a_{11} & a_{12} & \dots & a_{1n}\\a_{21} & a_{22} & \dots & a_{2n}\\\vdots & \vdots &  & \dots\\a_{m1} & a_{m2} & \dots & a_{mn}\end{bmatrix}\quad</script><p>矩阵这东西在机器学习中就不要太重要了！实际上，如果我们现在有$N$个用户的数据，每条数据含有$M$个特征，那其实它对应的就是一个$N<em>M$的矩阵呀；再比如，一张图由$16</em>16$的像素点组成，那这就是一个$16*16$的矩阵了。</p><h3 id="2-4-张量"><a href="#2-4-张量" class="headerlink" title="2.4 张量"></a>2.4 张量</h3><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。</p><p>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：</p><p><img src="https://pic4.zhimg.com/v2-c0c16793d4662bfcdd7e112030096f94_r.jpg" alt=""></p><p>其中表的横轴表示图片的宽度值，这里只截取$0-319$；表的纵轴表示图片的高度值，这里只截取$0-4$；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为$[1.0,1.0,1.0]$，代表的就是$RGB$三原色在图片的这个位置的取值情况（即$R=1.0$，$G=1.0$，$B=1.0$）。</p><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p><h3 id="2-5-范数"><a href="#2-5-范数" class="headerlink" title="2-5 范数"></a>2-5 范数</h3><p>有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数$(norm)$的函数衡量矩阵大小。$L_p$范数如下：</p><script type="math/tex; mode=display">\left| \left| x \right| \right| _{p}^{} =\left( \sum_{i}^{}{\left| x_{i} \right| ^{p} } \right) _{}^{\frac{1}{p} }</script><p>所以：</p><p>$L_1$范数$ \left| \left| x \right| \right|$：为$x$向量各个元素绝对值之和；</p><p>$L<em>2$范数$ \left| \left| x \right| \right| </em>{2}$：为$x$向量各个元素平方和的开方。</p><p>这里先说明一下，在机器学习中，$ L_1$范数和$ L_2$范数很常见，主要用在损失函数中起到一个限制模型参数复杂度的作用，至于为什么要限制模型的复杂度，这又涉及到机器学习中常见的过拟合问题。具体的概念在后续文章中会有详细的说明和推导，大家先记住：这个东西很重要，实际中经常会涉及到，面试中也常会被问到！！！</p><h3 id="2-6-特征分解"><a href="#2-6-特征分解" class="headerlink" title="2.6 特征分解"></a>2.6 特征分解</h3><p>许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。</p><p>方阵$ A$的特征向量是指与$ A$相乘后相当于对该向量进行缩放的非零向量$ \nu$ ：</p><script type="math/tex; mode=display">A\nu =\lambda \nu</script><p>标量$\lambda$被称为这个特征向量对应的特征值。 </p><p>使用特征分解去分析矩阵$ A$时，得到特征向量构成的矩阵$ V$和特征值构成的向量$\lambda$，我们可以重新将A写作：</p><script type="math/tex; mode=display">A=Vdiag\left( \lambda \right) V^{-1}</script><h3 id="2-7-奇异值分解（Singular-Value-Decomposition，SVD）"><a href="#2-7-奇异值分解（Singular-Value-Decomposition，SVD）" class="headerlink" title="2-7 奇异值分解（Singular Value Decomposition，SVD）"></a>2-7 奇异值分解（Singular Value Decomposition，SVD）</h3><p>矩阵的特征分解是有前提条件的，那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称$ SVD$。通过奇异分解，我们会得到一些类似于特征分解的信息。</p><p>它的具体做法是将一个普通矩阵分解为奇异向量和奇异值。比如将矩阵$ A$分解成三个矩阵的乘积：</p><script type="math/tex; mode=display">A=UDV^{T}</script><p>假设<strong>$ A$是一个$m\times n$矩阵，那么$ U$是一个$m\times m$矩阵，$D$是一个$m\times n$矩阵，$V$是一个$n\times n$矩阵</strong>。</p><p>这些矩阵每一个都拥有特殊的结构，其中$U$和$V$都是正交矩阵，$D$是对角矩阵（注意，$D$不一定是方阵）。对角矩阵$D$对角线上的元素被称为矩阵$A$的奇异值。矩阵$U$的列向量被称为<strong>左奇异向量</strong>，矩阵$V$的列向量被称<strong>右奇异向量</strong>。</p><p>$SVD$最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，$SVD$可用于推荐系统中。</p><h3 id="2-8-Moore-Penrose伪逆"><a href="#2-8-Moore-Penrose伪逆" class="headerlink" title="2-8 Moore-Penrose伪逆"></a>2-8 Moore-Penrose伪逆</h3><p>对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵$A$的左逆$B$来求解线性方程：</p><script type="math/tex; mode=display">Ax=y</script><p>等式两边同时左乘左逆$B$后，得到：</p><script type="math/tex; mode=display">x=By</script><p>是否存在唯一的映射将$A$映射到$B$取决于问题的形式。</p><p>如果矩阵$A$的行数大于列数，那么上述方程可能没有解；如果矩阵$A$的行数小于列数，那么上述方程可能有多个解。</p><p>$Moore-Penrose$伪逆使我们能够解决这种情况，矩阵$A$的伪逆定义为：</p><script type="math/tex; mode=display">A^+=\lim_{a\rightarrow{0}}(A^T{A}+\alpha{I}^{-1})A^T</script><p>但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：</p><script type="math/tex; mode=display">A^+=VD^+{U}^T</script><p>其中，矩阵$U$，$D$和$V$是矩阵$A$奇异值分解后得到的矩阵。对角矩阵$D$的伪逆$D^+$是其非零元素取倒之后再转置得到的。</p><h3 id="2-9-几种常用的距离"><a href="#2-9-几种常用的距离" class="headerlink" title="2-9 几种常用的距离"></a>2-9 几种常用的距离</h3><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有$100$个特征，那么他对应的就是一个$100$维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的$KNN$算法和$K-means$算法中很明显。</p><p>设有两个$n$维变量$A=\left[ x<em>{11}, x</em>{12},…,x<em>{1n} \right]$和$B=\left[ x</em>{21} ,x<em>{22} ,…,x</em>{2n} \right]$，则一些常用的距离公式定义如下：</p><h4 id="1、曼哈顿距离"><a href="#1、曼哈顿距离" class="headerlink" title="1、曼哈顿距离"></a>1、曼哈顿距离</h4><p>曼哈顿距离也称为城市街区距离，数学定义如下：</p><script type="math/tex; mode=display">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| }</script><p>曼哈顿距离的$Python$实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *  </span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  </span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])  </span><br><span class="line"><span class="keyword">print</span> sum(abs(vector1-vector2))</span><br></pre></td></tr></table></figure><h4 id="2、欧氏距离"><a href="#2、欧氏距离" class="headerlink" title="2、欧氏距离"></a>2、欧氏距离</h4><p>欧氏距离其实就是$L_2$范数，数学定义如下： </p><script type="math/tex; mode=display">d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } }</script><p>欧氏距离的$Python$实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt((vector1-vector2)*(vector1-vector2).T)</span><br></pre></td></tr></table></figure></p><h4 id="3、闵可夫斯基距离"><a href="#3、闵可夫斯基距离" class="headerlink" title="3、闵可夫斯基距离"></a>3、闵可夫斯基距离</h4><p>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：</p><script type="math/tex; mode=display">d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } }</script><p>实际上，当$p=1$时，就是曼哈顿距离；当$p=2$时，就是欧式距离。</p><h4 id="4、切比雪夫距离"><a href="#4、切比雪夫距离" class="headerlink" title="4、切比雪夫距离"></a>4、切比雪夫距离</h4><p>切比雪夫距离就是$L_{\varpi}$，即无穷范数，数学表达式如下：</p><script type="math/tex; mode=display">d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)</script><p>切比雪夫距离额Python实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt(abs(vector1-vector2).max)</span><br></pre></td></tr></table></figure></p><h4 id="5、夹角余弦"><a href="#5、夹角余弦" class="headerlink" title="5、夹角余弦"></a>5、夹角余弦</h4><p>夹角余弦的取值范围为$[-1,1]$，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值$1$；当两个向量的方向完全相反时，夹角余弦取最小值$-1$。</p><p>机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：</p><script type="math/tex; mode=display">cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } }</script><p>夹角余弦的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))</span><br></pre></td></tr></table></figure></p><h4 id="6、汉明距离"><a href="#6、汉明距离" class="headerlink" title="6、汉明距离"></a>6、汉明距离</h4><p>汉明距离定义的是两个字符串中不相同位数的数目。</p><p>例如：字符串$’1111’$与$’1001’$之间的汉明距离为$2$。</p><p>信息编码中一般应使得编码间的汉明距离尽可能的小。</p><p>汉明距离的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">smstr = nonzero(matV[<span class="number">0</span>]-matV[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> smstr</span><br></pre></td></tr></table></figure></p><h4 id="7、杰卡德相似系数"><a href="#7、杰卡德相似系数" class="headerlink" title="7、杰卡德相似系数"></a>7、杰卡德相似系数</h4><p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号$J(A,B)$表示，数学表达式为：</p><script type="math/tex; mode=display">J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| }</script><p>杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p><h4 id="8、杰卡德距离"><a href="#8、杰卡德距离" class="headerlink" title="8、杰卡德距离"></a>8、杰卡德距离</h4><p>与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：</p><script type="math/tex; mode=display">J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| }</script><p>杰卡德距离的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> scipy.spatial.distance <span class="keyword">as</span> dist</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> dist.pdist(matV,<span class="string">'jaccard'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="三、概率"><a href="#三、概率" class="headerlink" title="三、概率"></a>三、概率</h2><h3 id="3-1-为什么使用概率？"><a href="#3-1-为什么使用概率？" class="headerlink" title="3.1 为什么使用概率？"></a>3.1 为什么使用概率？</h3><p>概率论是用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。</p><p>在人工智能领域，我们主要以两种方式来使用概率论。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。</p><p>计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常。因为机器学习大部分时候处理的都是不确定量或随机量。</p><h4 id="3-2-随机变量"><a href="#3-2-随机变量" class="headerlink" title="3.2 随机变量"></a>3.2 随机变量</h4><p>随机变量可以随机地取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如，x<em>{1}  和x</em>{2}  都是随机变量X可能的取值。</p><p>对于向量值变量，我们会将随机变量写成X，它的一个值为x。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。</p><p>随机变量可以是离散的或者连续的。</p><h4 id="3-3-概率分布"><a href="#3-3-概率分布" class="headerlink" title="3.3 概率分布"></a>3.3 概率分布</h4><p>给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。</p><p>从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。</p><h4 id="3-4-条件概率"><a href="#3-4-条件概率" class="headerlink" title="3.4 条件概率"></a>3.4 条件概率</h4><p>很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。</p><p>我们将给定$X=x$时$Y=y$发生的概率记为$P\left( Y=y|X=x \right)$，这个概率可以通过下面的公式来计算：</p><script type="math/tex; mode=display">P\left( Y=y|X=x \right) =\frac{P\left( Y=y,X=x \right) }{P\left( X=x \right) }</script><h4 id="3-5-贝叶斯公式"><a href="#3-5-贝叶斯公式" class="headerlink" title="3.5 贝叶斯公式"></a>3.5 贝叶斯公式</h4><p>先看看什么是“先验概率”和“后验概率”，以一个例子来说明：</p><p>假设某种病在人群中的发病率是$0.001$，即$1000$人中大概会有$1$个人得病，则有：$P(患病) = 0.1\%$；即：在没有做检验之前，我们预计的患病率为$P(患病)=0.1\%$，这个就叫作”先验概率”。 </p><p>再假设现在有一种该病的检测方法，其检测的准确率为$95\%$；即：如果真的得了这种病，该检测法有$95\%$的概率会检测出阳性，但也有5%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有$95\%$的概率检测出阴性，但也有$5\%$的概率检测为阳性。用概率条件概率表示即为：$P(显示阳性|患病)=95\%$。</p><p>现在我们想知道的是：在做完检测显示为阳性后，某人的患病率$P(患病|显示阳性)$，这个其实就称为”后验概率”。</p><p>而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”。</p><p>这里先了解<strong>条件概率公式</strong>：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}</script><p>由条件概率可以得到<strong>乘法公式</strong>：  </p><script type="math/tex; mode=display">P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)</script><p>将条件概率公式和乘法公式结合可以得到：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}</script><p>再由<strong>全概率公式</strong>：</p><script type="math/tex; mode=display">P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)}</script><p>代入可以得到<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }</script><p>在这个例子里就是：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberp(患病|显示阳性) & =\frac{P(显示阳性|患病)P(患病)}{P(显示阳性)}\\                & =\frac{P(显示阳性|患病)P(患病)}{P(显示阳性|患病)P(患病)+{P(显示阳性|无病)P(无病)}}\\                & =\frac{95\%*0.1\%}{95\%*0.1\%+5\%*99.9\%}=1.86\%\end{aligned}</script><p>贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。</p><p>期望、方差、协方差等主要反映数据的统计特征，机器学习的一个很大应用就是数据挖掘等，因此这些基本的统计概念也是很有必要掌握。另外，像后面的EM算法中，就需要用到期望的相关概念和性质。</p><h4 id="3-6-期望"><a href="#3-6-期望" class="headerlink" title="3.6 期望"></a>3.6 期望</h4><p>在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。</p><p>假设X是一个离散随机变量，其可能的取值有：$\left\{ x<em>{1} ,x</em>{2} ,……,x<em>{n} \right\}$，各个取值对应的概率取值为：$P\left( x</em>{k} \right) , k=1,2,……,n$，则其数学期望被定义为：</p><script type="math/tex; mode=display">E\left(X \right) =\sum_{k=1}^{n}{x_{k} P\left( x_{k} \right) }</script><p>假设$X$是一个连续型随机变量，其概率密度函数为$P\left( x \right)$则其数学期望被定义为：</p><script type="math/tex; mode=display">E\left( x \right) =\int_{-\varpi }^{+\varpi } xf\left( x \right) dx</script><h4 id="3-7-方差"><a href="#3-7-方差" class="headerlink" title="3.7 方差"></a>3.7 方差</h4><p>概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下： </p><script type="math/tex; mode=display">Var\left( x \right) =E\left\{ \left[ x-E\left( x \right) \right] ^{2} \right\} =E\left( x^{2} \right) -\left[ E\left( x \right) \right] ^{2}</script><h4 id="3-8-协方差"><a href="#3-8-协方差" class="headerlink" title="3.8 协方差"></a>3.8 协方差</h4><p>在概率论和统计学中，协方差被用于衡量两个随机变量$X$和$Y$之间的总体误差。数学定义式为：</p><script type="math/tex; mode=display">Cov\left( X,Y \right) =E\left[ \left( X-E\left[ X \right] \right) \left( Y-E\left[ Y \right] \right) \right] =E\left[ XY \right] -E\left[ X \right] E\left[ Y \right]</script><h4 id="3-9-常见分布函数"><a href="#3-9-常见分布函数" class="headerlink" title="3.9 常见分布函数"></a>3.9 常见分布函数</h4><h4 id="1）0-1分布"><a href="#1）0-1分布" class="headerlink" title="1）0-1分布"></a>1）0-1分布</h4><p>$0-1$分布是单个二值型离散随机变量的分布，其概率分布函数为：  </p><script type="math/tex; mode=display">P\left( X=1 \right) =pP\left( X=0 \right) =1-p</script><h4 id="2）几何分布"><a href="#2）几何分布" class="headerlink" title="2）几何分布"></a>2）几何分布</h4><p>几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前$k-1$次皆失败，第k次成功的概率。其概率分布函数为：</p><script type="math/tex; mode=display">P\left( X=k \right) =\left( 1-p \right) ^{k-1} p</script><p>性质：</p><script type="math/tex; mode=display">E\left( X \right) =\frac{1}{p} Var\left( X \right) =\frac{1-p}{p^{2} }</script><h4 id="3）二项分布"><a href="#3）二项分布" class="headerlink" title="3）二项分布"></a>3）二项分布</h4><p>二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：   </p><script type="math/tex; mode=display">P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}</script><p>性质：   </p><script type="math/tex; mode=display">E\left( X \right) =npVar\left( X \right) =np\left( 1-p \right)</script><h4 id="4）高斯分布"><a href="#4）高斯分布" class="headerlink" title="4）高斯分布"></a>4）高斯分布</h4><p>高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：(A这里少高斯分布图像)</p><p><img src="https://pic4.zhimg.com/80/v2-a0811acc8ab121a3ad8f2e37ff6c37cc_hd.jpg" alt=""></p><p>若随机变量$X$服从一个数学期望为$\mu$，方差为$\sigma ^{2}$的正态分布，则我们将其记为：$N\left( \mu ,\sigma^{2} \right)$。其期望值$\mu$决定了正态分布的位置，其标准差$\sigma$（方差的开方）决定了正态分布的幅度。</p><h4 id="5）指数分布"><a href="#5）指数分布" class="headerlink" title="5）指数分布"></a>5）指数分布</h4><p>指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了$t$小时，它总共使用至少$t+s$小时的条件概率，与从开始使用时算起它使用至少$s$小时的概率相等。下面这些都属于指数分布：</p><ul><li>婴儿出生的时间间隔</li><li>网站访问的时间间隔</li><li>奶粉销售的时间间隔    </li></ul><p>指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间$t$，就等同于$t$之内没有任何婴儿出生，即：   </p><script type="math/tex; mode=display">P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t} }{0!}=e^{-\lambda t}</script><p>则：   </p><script type="math/tex; mode=display">P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}</script><p>如：接下来15分钟，会有婴儿出生的概率为：</p><script type="math/tex; mode=display">P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53</script><p>指数分布的图像如下：</p><p><img src="https://pic4.zhimg.com/80/v2-a58c37c481e032bbb53ff17113754ef6_hd.jpg" alt="tu"></p><h4 id="6）泊松分布"><a href="#6）泊松分布" class="headerlink" title="6）泊松分布"></a>6）泊松分布</h4><p>日常生活中，大量事件是有固定频率的，比如：</p><p>某医院平均每小时出生3个婴儿<br>某网站平均每分钟有2次访问<br>某超市平均每小时销售4包奶粉<br>它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。</p><p>泊松分布就是描述某段时间内，事件具体的发生概率。其概率函数为：   </p><script type="math/tex; mode=display">P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t} }{n!}</script><p>其中：</p><p>$P$表示概率，$N$表示某种函数关系，$t$表示时间，$n$表示数量，1小时内出生3个婴儿的概率，就表示为$P(N(1) = 3)$；$λ$表示事件的频率。</p><p>还是以上面医院平均每小时出生3个婴儿为例，则$\lambda =3$；</p><p>那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：   </p><script type="math/tex; mode=display">P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025</script><p>同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：    </p><script type="math/tex; mode=display">P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0 \right) - P\left( N\left( 1 \right)=1 \right)\approx 0.8</script><p>【注】上面的指数分布和泊松分布参考了阮一峰大牛的博客：“泊松分布和指数分布：10分钟教程”，在此说明，也对其表示感谢！</p><h4 id="3-10-Lagrange乘子法"><a href="#3-10-Lagrange乘子法" class="headerlink" title="3.10 Lagrange乘子法"></a>3.10 Lagrange乘子法</h4><p>对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。</p><p>对于一个要求极值的函数$f\left( x,y \right)$，图上的蓝圈就是这个函数的等高图，就是说$f\left( x,y \right) =c<em>{1} ,c</em>{2} ,…,c<em>{n}$分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组$\left( x,y \right)$，使它的$c</em>{i}$值越大越好，但是这点必须满足约束条件$g\left( x,y \right)$（在黄线上）。</p><p><img src="https://pic1.zhimg.com/80/v2-e59fd8c296c7e8c3b804726998610b31_hd.jpg" alt="tu1" title="tu1"></p><p>也就是说$f(x,y)$和$g(x,y)$相切，或者说它们的梯度$\nabla{f}$和$\nabla{g}$平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为$\lambda $倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。</p><p>在支持向量机模型（SVM）的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。</p><h4 id="3-11、最大似然估计"><a href="#3-11、最大似然估计" class="headerlink" title="3-11、最大似然估计"></a>3-11、最大似然估计</h4><p>最大似然也称为最大概似估计，即：在“模型已定，参数$θ$未知”的情况下，通过观测数据估计未知参数θ 的一种思想或方法。</p><p><strong>其基本思想是</strong>：  给定样本取值后，该样本最有可能来自参数$\theta$为何值的总体。即：寻找$\tilde{\theta } _{ML}$使得观测到样本数据的可能性最大。</p><p>举个例子，假设我们要统计全国人口的身高，首先假设这个身高服从服从正态分布，但是该分布的均值与方差未知。由于没有足够的人力和物力去统计全国每个人的身高，但是可以通过采样（所有的采样要求都是独立同分布的），获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布的均值与方差。</p><p>求极大似然函数估计值的一般步骤：</p><ol><li>写出似然函数； <script type="math/tex; mode=display">L(\theta_1,\theta_2,\dots,\theta_n)=\left \{\begin{array}{c}\prod_{i=1}^{n}p(x_i;\theta_1,\theta_2,\dots,\theta_n)\\\prod_{i=1}^{n}f(x_i;\theta_1,\theta_2,\dots,\theta_n)\end{array}\right.</script></li><li>对似然函数取对数；  </li><li>两边同时求导数；  </li><li>令导数为0解出似然方程。  </li></ol><p>在机器学习中也会经常见到极大似然的影子。比如后面的<strong>逻辑斯特回归模型（LR）</strong>，其核心就是构造对数损失函数后运用极大似然估计。</p><h3 id="四、信息论"><a href="#四、信息论" class="headerlink" title="四、信息论"></a>四、信息论</h3><p>信息论本来是通信中的概念，但是其核心思想“熵”在机器学习中也得到了广泛的应用。比如决策树模型$ID3$，$C4.5$中是利用<strong>信息增益</strong>来划分特征而生成一颗决策树的，而信息增益就是基于这里所说的<strong>熵</strong>。所以它的重要性也是可想而知。</p><h4 id="4-1-熵"><a href="#4-1-熵" class="headerlink" title="4.1 熵"></a>4.1 熵</h4><p>如果一个随机变量X的可能取值为$X=\left\{ x<em>{1},x</em>{2} ,…..,x<em>{n} \right\}$，其概率分布为$P\left( X=x</em>{i} \right) =p_{i} ,i=1,2,…..,n$，则随机变量X的熵定义为$H(X)$：</p><script type="math/tex; mode=display">H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i} \right) logP\left( x_{i} \right) } =\sum_{i=1}^{n}{P\left( x_{i} \right) \frac{1}{logP\left( x_{i} \right) } }</script><h4 id="4-2-联合熵"><a href="#4-2-联合熵" class="headerlink" title="4.2 联合熵"></a>4.2 联合熵</h4><p>两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量XY的不确定性的度量，用$H(X,Y)$表示：</p><script type="math/tex; mode=display">H\left( X,Y \right) =-\sum_{i=1}^{n}{\sum_{j=1}^{n}{P\left( x_{i} ,y_{j} \right)} logP\left( x_{i},y_{j} \right) }</script><h4 id="4-3-条件熵"><a href="#4-3-条件熵" class="headerlink" title="4.3 条件熵"></a>4.3 条件熵</h4><p>在随机变量$X$发生的前提下，随机变量$Y$发生新带来的熵，定义为$Y$的条件熵，用$H(Y|X)$表示：</p><script type="math/tex; mode=display">H\left(Y|X \right) =-\sum_{x,y}^{}{P\left( x,y \right) logP\left( y|x \right) }</script><p>条件熵用来衡量在已知随机变量$X$的条件下，随机变量$Y$的不确定性。</p><p>实际上，熵、联合熵和条件熵之间存在以下关系：</p><script type="math/tex; mode=display">H\left( Y|X \right) =H\left( X,Y\right) -H\left( X \right)</script><p>推导过程如下：  </p><script type="math/tex; mode=display">\begin{array}{ll}H\left( X,Y\right) -H\left( X \right)\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(x,y)}+\sum_{x}p(x)log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(x,y)}+\sum_{x}{(\sum_{y}p(x,y))}log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(y|x)}+\sum_{x,y}p(x,y)log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)}log{\frac{p(x,y)}{p(x)}}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)}logp(y|x)\end{array}</script><p>其中：</p><ul><li>第二行推到第三行的依据是边缘分布$P(x)$等于联合分布$P(x,y)$的和；</li><li>第三行推到第四行的依据是把公因子$logP(x)$乘进去，然后把$x$,$y$写在一起；</li><li>第四行推到第五行的依据是：因为两个sigma都有$P(x,y)$，故提取公因子$P(x,y)$放到外边，然后把里边的$-（log P(x,y) - log P(x)）$写成$- log (P(x,y) / P(x) ) $；</li><li>第五行推到第六行的依据是：$P(x,y) = P(x) * P(y|x)$，故$P(x,y) / P(x) =  P(y|x)$。   </li></ul><h4 id="4-4-相对熵"><a href="#4-4-相对熵" class="headerlink" title="4.4 相对熵"></a>4.4 相对熵</h4><p>相对熵又称互熵、交叉熵、$KL$散度、信息增益，是描述两个概率分布$P$和$Q$差异的一种方法，记为<strong>$D(P||Q)$</strong>。在信息论中，$D(P||Q)$表示当用概率分布$Q$来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，$Q$表示$P$的拟合分布。</p><p>对于一个离散随机变量的两个概率分布$P$和$Q$来说，它们的相对熵定义为：</p><script type="math/tex; mode=display">D\left( P||Q \right) =\sum_{i=1}^{n}{P\left( x_{i} \right) log\frac{P\left( x_{i} \right) }{Q\left( x_{i} \right) } }</script><p>注意：$D(P||Q) ≠ D(Q||P)$</p><p>相对熵又称<strong>$KL$散度($Kullback–Leibler divergence$)</strong>，$KL$散度也是一个机器学习中常考的概念。</p><h4 id="4-5-互信息"><a href="#4-5-互信息" class="headerlink" title="4.5 互信息"></a>4.5 互信息</h4><p>两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵称为互信息，用I(X,Y)表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。  </p><script type="math/tex; mode=display">I\left( X,Y \right) =\sum_{x\in X}^{}{\sum_{y\in Y}^{}{P\left( x,y \right) } log\frac{P\left( x,y \right) }{P\left( x \right) P\left( y \right) } }</script><p>互信息、熵和条件熵之间存在以下关系：$ H\left( Y|X \right) =H\left( Y \right) -I\left( X,Y \right) $  </p><p>推导过程如下：</p><p><img src="https://pic3.zhimg.com/v2-6f41bffde009999cbc370f7f38cab092_r.jpg" alt="img"></p><p>通过上面的计算过程发现有：$H(Y|X) = H(Y) - I(X,Y)$，又由前面条件熵的定义有：$H(Y|X) = H(X,Y) - H(X)$，于是有$I(X,Y)= H(X) + H(Y) - H(X,Y)$，此结论被多数文献作为互信息的定义。</p><h3 id="4-6、最大熵模型"><a href="#4-6、最大熵模型" class="headerlink" title="4-6、最大熵模型"></a>4-6、最大熵模型</h3><p>最大熵原理是概率模型学习的一个准则，它认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。通常用约束条件来确定模型的集合，所以，最大熵模型原理也可以表述为：在满足约束条件的模型集合中选取熵最大的模型。 </p><p>前面我们知道，若随机变量X的概率分布是$P\left( x_{i} \right)$ ，则其熵定义如下：    </p><script type="math/tex; mode=display">H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i} \right) logP\left( x_{i} \right) } =\sum_{i=1}^{n}{P\left( x_{i} \right) \frac{1}{logP\left( x_{i} \right) } }</script><p>熵满足下列不等式：</p><script type="math/tex; mode=display">0\leq H\left( X \right) \leq log\left| X \right|</script><p>式中，$|X|$是$X$的取值个数，当且仅当$X$的分布是均匀分布时右边的等号成立。也就是说，当$X$服从均匀分布时，熵最大。</p><p>直观地看，最大熵原理认为：要选择概率模型，首先必须满足已有的事实，即约束条件；在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性；“等可能”不易操作，而熵则是一个可优化的指标。</p><h2 id="五、-数值计算"><a href="#五、-数值计算" class="headerlink" title="五、 数值计算"></a>五、 数值计算</h2><h3 id="5-1-上溢和下溢"><a href="#5-1-上溢和下溢" class="headerlink" title="5.1 上溢和下溢"></a>5.1 上溢和下溢</h3><p>在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。</p><p>一种特别毁灭性的舍入误差是下溢。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除。</p><p>另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为\varpi 或-\varpi 时发生上溢。进一步的运算通常将这些无限值变为非数字。</p><p>必须对上溢和下溢进行数值稳定的一个例子是softmax 函数。softmax 函数经常用于预测与$multinoulli$分布相关联的概率，定义为：</p><p><img src="https://pic2.zhimg.com/80/v2-7283f680255ba0da3a69f2df58b58ae0_hd.jpg" alt=""></p><p>softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 $x<em>{i} $的类别标签$ y</em>{i}$ 属于$K$个类别的概率，最后判别$ y_{i}$ 所属类别时就是将其归为对应概率最大的那一个。</p><p>当式中的$w<em>{k} x</em>{i} +b$都是很小的负数时，$e^{w<em>{k} x</em>{i} +b }$ 就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的$x<em>{w</em>{k} x<em>{i} +b} $是很大的正数时，$e^{w</em>{k} x_{i} +b }$ 就会发生上溢导致结果是未定的。</p><h3 id="5-2-计算复杂性与NP问题"><a href="#5-2-计算复杂性与NP问题" class="headerlink" title="5.2 计算复杂性与NP问题"></a>5.2 计算复杂性与NP问题</h3><h4 id="1-算法复杂性"><a href="#1-算法复杂性" class="headerlink" title="1.算法复杂性"></a>1.算法复杂性</h4><p>现实中大多数问题都是离散的数据集，为了反映统计规律，有时数据量很大，而且多数目标函数都不能简单地求得解析解。这就带来一个问题：算法的复杂性。</p><p>算法理论被认为是解决各类现实问题的方法论。衡量算法有两个重要的指标：时间复杂度和空间复杂度，这是对算法执行所需要的两类资源——时间和空间的估算。</p><p>一般，衡量问题是否可解的重要指标是：该问题能否在多项式时间内求解，还是只能在指数时间内求解？在各类算法理论中，通常使用多项式时间算法即可解决的问题看作是易解问题，需要指数时间算法解决的问题看作是难解问题。</p><p>指数时间算法的计算时间随着问题规模的增长而呈指数化上升，这类问题虽然有解，但并不适用于大规模问题。所以当前算法研究的一个重要任务就是将指数时间算法变换为多项式时间算法。</p><h4 id="2-确定性和非确定性"><a href="#2-确定性和非确定性" class="headerlink" title="2.确定性和非确定性"></a>2.确定性和非确定性</h4><p>除了问题规模与运算时间的比较，衡量一个算法还需要考虑确定性和非确定性的概念。</p><p>这里先介绍一下“自动机”的概念。自动机实际上是指一种基于状态变化进行迭代的算法。在算法领域常把这类算法看作一个机器，比较知名的有图灵机、玻尔兹曼机、支持向量机等。</p><p>所谓确定性，是指针对各种自动机模型，根据当时的状态和输入，若自动机的状态转移是唯一确定的，则称确定性；若在某一时刻自动机有多个状态可供选择，并尝试执行每个可选择的状态，则称为非确定性。</p><p>换个说法就是：确定性是程序每次运行时产生下一步的结果是唯一的，因此返回的结果也是唯一的；非确定性是程序在每个运行时执行的路径是并行且随机的，所有路径都可能返回结果，也可能只有部分返回结果，也可能不返回结果，但是只要有一个路径返回结果，那么算法就结束。</p><p>在求解优化问题时，非确定性算法可能会陷入局部最优。</p><h4 id="3-NP问题"><a href="#3-NP问题" class="headerlink" title="3.NP问题"></a>3.NP问题</h4><p>有了时间上的衡量标准和状态转移的确定性与非确定性的概念，我们来定义一下问题的计算复杂度。</p><p>P类问题就是能够以多项式时间的确定性算法来对问题进行判定或求解，实现它的算法在每个运行状态都是唯一的，最终一定能够确定一个唯一的结果——最优的结果。</p><p>NP问题是指可以用多项式时间的非确定性算法来判定或求解，即这类问题求解的算法大多是非确定性的，但时间复杂度有可能是多项式级别的。</p><p>但是，NP问题还要一个子类称为NP完全问题，它是NP问题中最难的问题，其中任何一个问题至今都没有找到多项式时间的算法。</p><p>机器学习中多数算法都是针对NP问题（包括NP完全问题）的。</p><h3 id="5-3-数值计算"><a href="#5-3-数值计算" class="headerlink" title="5.3 数值计算"></a>5.3 数值计算</h3><p>上面已经分析了，大部分实际情况中，计算机其实都只能做一些近似的数值计算，而不可能找到一个完全精确的值，这其实有一门专门的学科来研究这个问题，这门学科就是——数值分析（有时也叫作“计算方法”）；运用数值分析解决问题的过程为：实际问题→数学模型→数值计算方法→程序设计→上机计算求出结果。</p><p>计算机在做这些数值计算的过程中，经常会涉及到的一个东西就是“迭代运算”，即通过不停的迭代计算，逐渐逼近真实值（当然是要在误差收敛的情况下）。</p><h2 id="六、最优化"><a href="#六、最优化" class="headerlink" title="六、最优化"></a>六、最优化</h2><p>本节介绍机器学习中的一种重要理论——最优化方法。</p><h3 id="6-1-最优化理论"><a href="#6-1-最优化理论" class="headerlink" title="6.1 最优化理论"></a>6.1 最优化理论</h3><p>无论做什么事，人们总希望以最小的代价取得最大的收益。在解决一些工程问题时，人们常会遇到多种因素交织在一起与决策目标相互影响的情况；这就促使人们创造一种新的数学理论来应对这一挑战，也因此，最早的优化方法——线性规划诞生了。</p><p>在李航博士的《统计学习方法》中，其将机器学习总结为如下表达式：</p><p>机器学习 = 模型 + 策略 + 算法</p><p>可以看得出，算法在机器学习中的 重要性。实际上，这里的算法指的就是优化算法。在面试机器学习的岗位时，优化算法也是一个特别高频的问题，大家如果真的想学好机器学习，那还是需要重视起来的。</p><h3 id="6-2-最优化问题的数学描述"><a href="#6-2-最优化问题的数学描述" class="headerlink" title="6.2 最优化问题的数学描述"></a>6.2 最优化问题的数学描述</h3><p>最优化的基本数学模型如下：</p><p><img src="https://pic3.zhimg.com/80/v2-f35226b3e0fa018db6a4b233c51eccbe_hd.jpg" alt=""></p><p>它有三个基本要素，即：</p><p>设计变量：$x$是一个实数域范围内的$n$维向量，被称为决策变量或问题的解；<br>目标函数：$f(x)$为目标函数；<br>约束条件：$h<em>{i} \left( x \right) =0$称为等式约束，$g</em>{i} \left( x \right) \leq 0$为不等式约束，$i=0,1,2,……$</p><h3 id="6-3-凸集与凸集分离定理"><a href="#6-3-凸集与凸集分离定理" class="headerlink" title="6.3 凸集与凸集分离定理"></a>6.3 凸集与凸集分离定理</h3><h4 id="1-凸集"><a href="#1-凸集" class="headerlink" title="1.凸集"></a>1.凸集</h4><p>实数域R上（或复数C上）的向量空间中，如果集合S中任两点的连线上的点都在S内，则称集合S为凸集，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-608f89f47688c41e4c3f83cfad095c84_hd.jpg" alt=""></p><p>数学定义为：</p><p>设集合$D\subset R^{n} $，若对于任意两点$x,y\in D$，及实数$\lambda \left( 0\leq \lambda \leq 1 \right)$ 都有：   </p><script type="math/tex; mode=display">\lambda x+\left( 1-\lambda \right) y\in D</script><p>则称集合$D$为凸集。</p><h4 id="2-超平面和半空间"><a href="#2-超平面和半空间" class="headerlink" title="2.超平面和半空间"></a>2.超平面和半空间</h4><p>实际上，二维空间的超平面就是一条线（可以使曲线），三维空间的超平面就是一个面（可以是曲面）。其数学表达式如下：</p><p>超平面：$H=\left\{ x\in R^{n} |a<em>{1} +a</em>{2}+…+a_{n} =b \right\} $</p><p>半空间：$H^{+} =\left\{ x\in R^{n} |a<em>{1} +a</em>{2}+…+a_{n} \geq b \right\} $ </p><h4 id="3-凸集分离定理"><a href="#3-凸集分离定理" class="headerlink" title="3.凸集分离定理"></a>3.凸集分离定理</h4><p>所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用一张超平面将两者隔在两边，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-4116a3bda12faa5e2421ce27efb7fb71_hd.jpg" alt=""></p><h4 id="4-凸函数"><a href="#4-凸函数" class="headerlink" title="4.凸函数"></a>4.凸函数</h4><p>凸函数就是一个定义域在某个向量空间的凸子集C上的实值函数。</p><p><img src="https://pic2.zhimg.com/80/v2-f1b39d0aad4388433158679221f813d2_hd.jpg" alt=""></p><p>数学定义为： </p><p>对于函数$f(x)$，如果其定义域$C$是凸的，且对于$∀x,y∈C，0\leq \alpha \leq 1$，<br> 有：  </p><script type="math/tex; mode=display">f\left( \theta x+\left( 1-\theta \right) y \right) \leq \theta f\left( x \right) +\left( 1-\theta \right) f\left( y \right)</script><p>则$f(x)$是凸函数。</p><p>注：如果一个函数是凸函数，则其局部最优点就是它的全局最优点。这个性质在机器学习算法优化中有很重要的应用，因为机器学习模型最后就是在求某个函数的全局最优点，一旦证明该函数（机器学习里面叫“损失函数”）是凸函数，那相当于我们只用求它的局部最优点了。</p><h3 id="6-4-梯度下降算法"><a href="#6-4-梯度下降算法" class="headerlink" title="6.4 梯度下降算法"></a>6.4 梯度下降算法</h3><h4 id="1-引入"><a href="#1-引入" class="headerlink" title="1.引入"></a>1.引入</h4><p>前面讲数值计算的时候提到过，计算机在运用迭代法做数值计算（比如求解某个方程组的解）时，只要误差能够收敛，计算机最后经过一定次数的迭代后是可以给出一个跟真实解很接近的结果的。</p><p>这里进一步提出一个问题，如果我们得到的目标函数是非线性的情况下，按照哪个方向迭代求解误差的收敛速度会最快呢？</p><p>答案就是沿梯度方向。这就引入了我们的梯度下降法。</p><h4 id="2-梯度下降法"><a href="#2-梯度下降法" class="headerlink" title="2.梯度下降法"></a>2.梯度下降法</h4><p>在多元微分学中，梯度就是函数的导数方向。</p><p>梯度法是求解无约束多元函数极值最早的数值方法，很多机器学习的常用算法都是以它作为算法框架，进行改进而导出更为复杂的优化方法。</p><p>在求解目标函数f\left( x \right) 的最小值时，为求得目标函数的一个凸函数，在最优化方法中被表示为：</p><script type="math/tex; mode=display">minf\left( x \right)</script><p>根据导数的定义，函数$f\left( x \right) $的导函数就是目标函数在$x$上的变化率。在多元的情况下，目标函数$f\left( x,y,z \right) $在某点的梯度$grad f\left( x,y,z \right) =\left( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z} \right)$ 是一个由各个分量的偏导数构成的向量，负梯度方向是$f\left( x,y,z \right) $减小最快的方向。</p><p><img src="https://pic4.zhimg.com/80/v2-e61c38f10e34badf5b2c1f3b9c9bcfa0_hd.jpg" alt=""></p><p>如上图所示，当需要求$f\left( x \right) $的最小值时（机器学习中的$f\left( x \right) $一般就是损失函数，而我们的目标就是希望损失函数最小化），我们就可以先任意选取一个函数的初始点$x<em>{0} $（三维情况就是$\left( x</em>{0} ,y<em>{0} ,z</em>{0} \right)$ ），让其沿着途中红色箭头（负梯度方向）走，依次到$x<em>{1} ，x</em>{2} ，…，x_{n} $（迭代n次）这样可最快达到极小值点。</p><p>梯度下降法过程如下：</p><p>输入：目标函数$f\left( x \right) $，梯度函数$g\left( x \right) =grad f\left( x \right) $，计算精度$\varepsilon $</p><p>输出：$f\left( x \right) 的极小值点x^{*} $</p><ol><li>任取取初始值$x_{0}$ ，置$k=0$；</li><li>计算$f\left( x_{k} \right) $；</li><li>计算梯度$g<em>{k} =grad f\left( x</em>{k} \right) $，当$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $时停止迭代，令$x^{*} =x</em>{k}$ ；</li><li>否则令$P<em>{k} =-g</em>{k}$ ，求$\lambda <em>{k} 使f\left( x</em>{k+1} \right) =minf\left( x<em>{k} +\lambda </em>{k} P_{k} \right) $；</li><li>置$x<em>{k+1} =x</em>{k} +\lambda <em>{k} P</em>{k} $，计算$f\left( x<em>{k+1}\right) $，当$\left| \left| f\left( x</em>{k+1}\right) -f\left( x<em>{k}\right) \right| \right| &lt;\varepsilon 或\left| \left| x</em>{k+1} -x<em>{k} \right| \right| &lt;\varepsilon $时，停止迭代，令$x^{*} =x</em>{k+1}  $；</li><li>否则，置$k=k+1$，转$3$。</li></ol><h3 id="6-5-随机梯度下降算法"><a href="#6-5-随机梯度下降算法" class="headerlink" title="6.5 随机梯度下降算法"></a>6.5 随机梯度下降算法</h3><p>上面可以看到，在梯度下降法的迭代中，除了梯度值本身的影响外，还有每一次取的步长\lambda _{k} 也很关键：步长值取得越大，收敛速度就会越快，但是带来的可能后果就是容易越过函数的最优点，导致发散；步长取太小，算法的收敛速度又会明显降低。因此我们希望找到一种比较好的方法能够平衡步长。</p><p>随机梯度下降法并没有新的算法理论，仅仅是引进了随机样本抽取方式，并提供了一种动态步长取值策略。目的就是又要优化精度，又要满足收敛速度。</p><p>也就是说，上面的批量梯度下降法每次迭代时都会计算训练集中所有的数据，而随机梯度下降法每次迭代只是随机取了训练集中的一部分样本数据进行梯度计算，这样做最大的好处是可以避免有时候陷入局部极小值的情况（因为批量梯度下降法每次都使用全部数据，一旦到了某个局部极小值点可能就停止更新了；而随机梯度法由于每次都是随机取部分数据，所以就算局部极小值点，在下一步也还是可以跳出）</p><p>两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p><h3 id="6-6-牛顿法"><a href="#6-6-牛顿法" class="headerlink" title="6.6 牛顿法"></a>6.6 牛顿法</h3><h4 id="1-牛顿法介绍"><a href="#1-牛顿法介绍" class="headerlink" title="1.牛顿法介绍"></a>1.牛顿法介绍</h4><p>牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。</p><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。</p><p><img src="https://pic3.zhimg.com/80/v2-e22ea8c565434e945a17a80bec5630b6_hd.jpg" alt=""><br>或者从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p><h4 id="2-牛顿法的推导"><a href="#2-牛顿法的推导" class="headerlink" title="2.牛顿法的推导"></a>2.牛顿法的推导</h4><p>将目标函数$f\left( x \right)  在x_{k}$ 处进行二阶泰勒展开，可得：   </p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k} \right) +f^{'} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{''}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}</script><p>因为目标函数$f\left( x \right)$ 有极值的必要条件是在极值点处一阶导数为0，即：$f^{‘} \left( x \right) =0 $</p><p>所以对上面的展开式两边同时求导（注意$x$才是变量，$x<em>{k}$ 是常量$\Rightarrow f^{‘} \left( x</em>{k} \right) ,f^{‘’} \left( x_{k} \right)$ 都是常量），并令$f^{‘} \left( x \right) =0$可得：  </p><script type="math/tex; mode=display">f^{'} \left( x_{k} \right) +f^{''} \left( x_{k} \right) \left( x-x_{k} \right) =0</script><p>即：   </p><script type="math/tex; mode=display">x=x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>于是可以构造如下的迭代公式：</p><script type="math/tex; mode=display">x_{k+1} =x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>这样，我们就可以利用该迭代式依次产生的序列$\left\{x<em>{1},x</em>{2},…., x_{k} \right\} $才逐渐逼近$f\left( x \right)$ 的极小值点了。</p><p>牛顿法的迭代示意图如下：</p><p><img src="https://pic1.zhimg.com/80/v2-e908f9721cc82415fa7e70c763351f3a_hd.jpg" alt=""></p><p>上面讨论的是2维情况，高维情况的牛顿迭代公式是：</p><p>式中，$ ▽f是f\left( x \right)$ 的梯度，即：</p><p><img src="https://pic4.zhimg.com/80/v2-71df54a8e32e172596dcaa07e6b31899_hd.jpg" alt=""></p><p>H是Hessen矩阵，即：</p><p><img src="https://pic4.zhimg.com/80/v2-2891044fd02769c3148649e2a1a01fd5_hd.jpg" alt=""></p><h4 id="3-牛顿法的过程"><a href="#3-牛顿法的过程" class="headerlink" title="3.牛顿法的过程"></a>3.牛顿法的过程</h4><ol><li>给定初值$x_{0} $和精度阈值$\varepsilon $，并令$k=0$；</li><li>计算$x<em>{k} $和$H</em>{k}$ ；</li><li>若$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $则停止迭代；否则确定搜索方向：$d</em>{k} =-H<em>{k}^{-1} \cdot g</em>{k}$ ；</li><li>计算新的迭代点：$x<em>{k+1} =x</em>{k} +d_{k} $；</li><li>令$k=k+1$，转至$2$。</li></ol><h3 id="6-7-阻尼牛顿法"><a href="#6-7-阻尼牛顿法" class="headerlink" title="6.7 阻尼牛顿法"></a>6.7 阻尼牛顿法</h3><h4 id="1-引入-1"><a href="#1-引入-1" class="headerlink" title="1.引入"></a>1.引入</h4><p>注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现$f\left( x<em>{k+1} \right) &gt;f\left( x</em>{k} \right) $的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。</p><p>为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是$x<em>{k} $，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子$\lambda </em>{k}$ ，即：</p><p>$\lambda <em>{k} = minf\left( x</em>{k} +\lambda d_{k} \right) $</p><h4 id="2-算法过程"><a href="#2-算法过程" class="headerlink" title="2.算法过程"></a>2.算法过程</h4><ol><li>给定初值$x_{0}$ 和精度阈值$\varepsilon$ ，并令$k=0$；</li><li>计算$g<em>{k} （f\left( x \right) $在$x</em>{k}$ 处的梯度值）和$H_{k} $；</li><li>若$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $则停止迭代；否则确定搜索方向：$d</em>{k} =-H<em>{k}^{-1} \cdot g</em>{k}$ ；</li><li>利用$d<em>{k} =-H</em>{k}^{-1} \cdot g<em>{k} 得到步长\lambda </em>{k} $，并令$x<em>{k+1} =x</em>{k} +\lambda <em>{k} d</em>{k}$  </li><li>令$k=k+1$，转至$2$。</li></ol><h3 id="6-8-拟牛顿法"><a href="#6-8-拟牛顿法" class="headerlink" title="6.8 拟牛顿法"></a>6.8 拟牛顿法</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h4><p>由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵，计算量比较大（求矩阵的逆运算量比较大），因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。</p><h4 id="2-拟牛顿法的推导"><a href="#2-拟牛顿法的推导" class="headerlink" title="2.拟牛顿法的推导"></a>2.拟牛顿法的推导</h4><p>先将目标函数在$x_{k+1} $处展开，得到：  </p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k+1} \right) +f^{'} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{''}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}</script><p>两边同时取梯度，得：  </p><script type="math/tex; mode=display">f^{'}\left( x \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>取上式中的$x=x_{k}$ ，得：   </p><script type="math/tex; mode=display">f^{'}\left( x_{k} \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>即：   </p><script type="math/tex; mode=display">g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)</script><p>可得：  </p><script type="math/tex; mode=display">H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}</script><p>上面这个式子称为“拟牛顿条件”，由它来对$Hessen$矩阵做约束。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2-1创建图 启动图</title>
      <link href="/2017/12/26/2-1%E5%88%9B%E5%BB%BA%E5%9B%BE%20%E5%90%AF%E5%8A%A8%E5%9B%BE/"/>
      <url>/2017/12/26/2-1%E5%88%9B%E5%BB%BA%E5%9B%BE%20%E5%90%AF%E5%8A%A8%E5%9B%BE/</url>
      <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m1=tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">m2=tf.constant([[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">product=tf.matmul(m1,m2)</span><br><span class="line">print(product)</span><br></pre></td></tr></table></figure><pre><code>Tensor(&quot;MatMul_1:0&quot;, shape=(1, 1), dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.Session()</span><br><span class="line">result=sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result=sess.run(product)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Python学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>万万没想到：用理工科思维理解世界</title>
      <link href="/2017/12/13/%E4%B8%87%E4%B8%87%E6%B2%A1%E6%83%B3%E5%88%B0%EF%BC%9A%E7%94%A8%E7%90%86%E5%B7%A5%E7%A7%91%E6%80%9D%E7%BB%B4%E7%90%86%E8%A7%A3%E4%B8%96%E7%95%8C/"/>
      <url>/2017/12/13/%E4%B8%87%E4%B8%87%E6%B2%A1%E6%83%B3%E5%88%B0%EF%BC%9A%E7%94%A8%E7%90%86%E5%B7%A5%E7%A7%91%E6%80%9D%E7%BB%B4%E7%90%86%E8%A7%A3%E4%B8%96%E7%95%8C/</url>
      <content type="html"><![CDATA[<p>这是一篇读书笔记。<br><a id="more"></a></p><h2 id="《万万没想到：用理工科思维理解世界》"><a href="#《万万没想到：用理工科思维理解世界》" class="headerlink" title="《万万没想到：用理工科思维理解世界》"></a>《万万没想到：用理工科思维理解世界》</h2><ul><li>作者：万维钢（同人于野）<br><em>Unconventional wisdom</em></li></ul><h3 id="Part-one-反常识思维"><a href="#Part-one-反常识思维" class="headerlink" title="Part one 反常识思维"></a><strong>Part one 反常识思维</strong></h3><h4 id="1．“反常识”思维"><a href="#1．“反常识”思维" class="headerlink" title="1．“反常识”思维"></a>1．“反常识”思维</h4><p>当我们需要做决定的时候，我们考虑的是具体的事、具体的人和他们具体的表情。在这些具体例子的训练下，我们的潜意识早就学会了快速判断人的真诚度和事件的紧急程度。<br>这种“具体思维”做各种选择的首要标准，是<strong>道德</strong>。<br>我们首要学会的是分辨美丑。这也就是文人思维的起源，针对每个特定动作的美学评价。有时候他们管这种评价叫“价值观”，但所谓的价值观无非就是给人和事贴或好或坏的标签。<br>低端文人研究道德，高端文人研究美感。可是只有刚接触艺术的人才喜欢令人愉悦的东西，审美观成熟到一定程度后我们就觉得快乐是一种肤浅的感觉，改为欣赏愁苦了。<br>有时候文人把自己的价值观判断称为“常识”，因为这些判断本来就是从人的原始思维而来的，然而现代社会却产生了另一种思维，却是“反常识”的。<br>现代社会与古代最大的不同，是人们的生活变得越来越复杂。除了工作学习，我们还要娱乐。参加社交活动、学习和发展以及随时对遥远的公众事务发表意见。我们的每一个决定都可能以一种不直截了当的方式影响他人，然后再影响自己。面对这种复杂的局面，最基本的一个结果是好东西虽然多，你却不能都要。<br>取舍思维，英文有一个形神兼备的词“tradeoff”。它是“理工科思维”的起源。讨价还价一番后达成交易，这对文人来说是一个非常无语的情境！既不美也不丑，既不值得歌颂也不值得唾弃。斤斤计较地得到一个既谈不上实现了梦想也谈不上是悲剧的结果。完全不文艺。<br>“tradeoff”要求我们知道每一个事物的利弊。世界上并没有多少事情是“在没有使任何人境况没有变坏的前提下使得至少一个人变得更好”的所谓“帕累托改进”。<br>理工科思维要求妥协，而文人总爱不管不顾，喜欢说不惜一切代价，喜欢动不动就把全部筹码都押上去的剧情。理工科思维要求随时根据新情况调整策略。<br>《思考，快与慢》将人脑的两套思维系统称为“系统1”和“系统2”。前者自动起作用，能迅速对一个事物给出一个的很难改变的第一印象；而后者费力而缓慢，需要我们集中注意力进行复杂的计算甚至我们在系统2工作时连瞳孔都放大了。系统2根本不是计算机的对手，然而系统1却比计算机强大多了。文人思维显然是系统1的集大成者，而理工科思维则是系统2的产物。<br>“tradeoff”要求量化输入和预计输出，这也是理工思维的根本方法。但人脑天生不适应抽象数字。<br>在大多数公共问题上，常识是不好使的。资源调配即使做不到完全依赖市场，也不应该谁声音大就听谁的。<br>现在到了用理工科思维取代文人思维的时候，传统的文人腔已经越来越少出现在主流媒体上，一篇正经讨论现实的文章总要做点计算才说的过去。</p><h4 id="2-别想说服我"><a href="#2-别想说服我" class="headerlink" title="2.别想说服我"></a>2.别想说服我</h4><p>人做判断的时候有两种机制：一种是“科学家机制”，先有证据再下结论；另一种是“律师机制”，先有结论再去找证据。每个人都爱看能印证自己已有观念的东西，我们不但不爱看，而且还会直接忽略，那些不符合我们已有观念的证据。<br>这个毛病叫“确认偏误”（confirmation bias）。如果你已经开始相信一个东西了，那么你就会主动寻找能够增强这种相信的信息，乃至不顾事实。<br>在确认偏误的作用下，任何新的证据都有可能被忽略，甚至被对立的双方都用来加强自己的观点。可能有人认为只有文化程比较低的人才会陷入确认偏误，文化程度越高就越能客观判断。事实并非如此。在某些问题上，甚至是文化程度越高的人群，思想越容易两极化。<br>人们为了付出的沉没成本而不得不死命拥护自己的派别，也许就是为了表明自己的身份，也许是为了寻找一份归属感。<br>如果是两个理性而真诚的真理追求者讨论问题，争论的结果必然是这两个人达成一致。“why do humans reason?”甚至认为人的逻辑推理能力本来就不是用来追求真理的，而是用来说服别人的。也就是说我们天生就都是律师思维，我们的大脑本来就是个争论设备。这也就是因为进化总是奖励那些能说服别人的人，而不是那些能发现真理的人。<br>人人都只接收符合自己观点的信息，甚至只跟与自己志同道合的人交流，那么就会形成“回音室效应（echo chamber effect）”人们的观点将会变得越来越极端。<br>有鉴于此，约翰逊提出的核心建议是：要主动刻意地消费，吸收有可能修正我们观点的新信息，而不是吸收对我们现有观念的肯定。</p><h4 id="3-真理追求者"><a href="#3-真理追求者" class="headerlink" title="3.真理追求者"></a>3.真理追求者</h4><p>诺贝尔奖获得者罗伯特•奥曼指出，如果是两个理性而真诚的真理追求者争论问题，争论的结果必然是两人达成一致。换句话说如果争论不欢而散，那么必然有一方是虚伪的。</p><h4 id="4-坏比好重要"><a href="#4-坏比好重要" class="headerlink" title="4.坏比好重要"></a>4.坏比好重要</h4><p>损失厌恶：人们对负面感情的重视程度总是超过正面感情。心理学对这个更一般的现象也有个名词，叫“negativity bias”，姑且称之为负面偏见。<br>用核磁共振观察他们的大脑，当试验受试者说“损失”这个词的时候，他们大脑中的信任和（amygdala）兴奋了。这个区域会产生负面情绪。<br>恐惧和冒险是人的两种非常基本的感情。进化心理学认为恐惧来自人的自我保护本能，而冒险来自人的求偶本能。进化是的我们大脑中的恐惧优先级高于浪漫。<br>本能归本能，有人可以超越自己的本能。</p><h4 id="5-最简单概率论的五个智慧"><a href="#5-最简单概率论的五个智慧" class="headerlink" title="5.最简单概率论的五个智慧"></a>5.最简单概率论的五个智慧</h4><h5 id="1-随机"><a href="#1-随机" class="headerlink" title="1.随机"></a>1.随机</h5><p>概率论最基础的思想是，有些思想是无缘无故发生的。<br>大多数事情并不是完全的随机事件，却都有一定的随机因素。人们经常错误地理解偶然事件，总想用必然去解释偶然。<br>理解了随机性，我们就知道有些事情发生就发生了，没有太大可供解读的意义。<br>偶然的错误是不值得深究的，成绩也不值得深究。现代概率论的奠基人之一雅各布•伯努利，甚至认为我们根本就不应该基于一个人的成就去赞美他。用成绩评估一个人的能力，来决定是否让他入学、是否给他升职加薪，是社会的普遍做法，对此人人都服气，童叟无欺非常公平。这还有什么可说的。这还有什么可说的？问题在于，成绩很有可能有很大的偶然因素。失败者没必要要妄自菲薄，成功者也应该明白自己的成功中是有侥幸的。</p><h5 id="2-误差"><a href="#2-误差" class="headerlink" title="2.误差"></a>2.误差</h5><p>后来人们认识到偶然因素永远存在，即使实验条件再精确也无法完全避免随机干扰的影响。</p><h6 id="3-赌徒谬误"><a href="#3-赌徒谬误" class="headerlink" title="3.赌徒谬误"></a>3.赌徒谬误</h6><p>概率论中确实有一个“大数定律”说如果进行足够多次的抽奖，那么各种不同结果出现的频率就会等于它们的概率。<br>但是人们常常错误地理解随机性和大数定律——以为随机就意味着均匀。但大数定律的工作机制不是跟过去平均，它的真实意思是如果未来你再进行非常多次的抽奖，你就会得到非常多的“2”和“6”，以至于它们此前的一点点差异会变得微不足道。<br>“比如号码2已经出现了3期，而号码6已经出现了5期，则再下一次号码2再出现的概率明显大于6”，这完全错误，下一次出现2和6的概率是相等的。这就是一个著名错误“赌徒谬误”。</p><h5 id="4-在没有规律的地方发现规律"><a href="#4-在没有规律的地方发现规律" class="headerlink" title="4.在没有规律的地方发现规律"></a>4.在没有规律的地方发现规律</h5><p>理解随机性和独立随机事件，得出一个结论：独立随机事件的发生是没有规律和不可预测的。<br>未来是不可被精确预测的。这个世界并不像钟表那样运行。</p><h5 id="5-小数定律"><a href="#5-小数定律" class="headerlink" title="5.小数定律"></a>5.小数定律</h5><p>问题的关键是随机分布不等于均匀分布。人们往往认为如果是随机的，那就应该是均匀的，殊不知这一点仅在样本总数非常大的时候才有效。<br>如果统计数字很少，就很容易出现不均匀的情况。这个被戏称为“小数定律”。<br>大数定律是我们从统计数字中推测真相的理论基础。大数定律说如果统计样本足够大，那么事物出现的频率就能无限接近它的理论概率<br>小数定律说如果样本不够大，那么它就会变现为各种极端情况，而这种情况可能跟本性一点关系都没有。</p><h5 id="6-一颗阴谋论的心"><a href="#6-一颗阴谋论的心" class="headerlink" title="6.一颗阴谋论的心"></a>6.一颗阴谋论的心</h5><p>1.美国的阴谋<br>可以解释这些看似“自然”其实“不自然”的事件，我发现其背后有一个不可告人的目的。这种解读是阴谋论。<br>所有阴谋论都有一个共同的思维模式，就是不承认巧合，不承认有些事情是自然而然发生的，认为这一切都有联系、有目的。<br>2.合理性和可能性<br>想要对任何事情的真伪都给出正确的判断是不可能的，我们只能在有限的条件下合理地评估每件事的可能性。阴谋论之所以不足信，是因为其成立的可能性很低。<br>3.目的与科学<br>科学的标志，是对世界的运行给出一套纯机械的机制。<br>很多人研究为什么自然科学没有在中国发生。莫里斯（Ian Morris）在《西方将主宰多久》（Why The West Rules:For Now）说：有一个重要原因是在于中国的传统认为天道是有目的的。我们认为上天有道德观，它降下自然灾害是对皇帝的警告，或者对坏人的惩罚。<br>自然是没有目的，人类社会的很多现象往往也没有目的。<br>人在复杂的现代社会中运动，很大程度上类似于原子在电磁场中的运动，个人意愿能改变的事情很少，绝大多数人都是在随波逐流，复杂的系统也会出现非常激烈的“事件”。有人用计算机模拟发现，哪怕没有任何消息输入，仅仅是交易者之间的简单互动，也可能让股价产生很大波动。这些波动的发生并没有什么目的。每一次金融危机都会有阴谋论者站出来说这是谁谁为了某个目的故意制造的，但事实上，美联储对金融市场的控制手段非常有限。在正经的经济学家看来，把1997年亚洲金融危机归罪于索罗斯是非常可笑的事情。</p><h5 id="7-桥段会毁了你的生活"><a href="#7-桥段会毁了你的生活" class="headerlink" title="7.桥段会毁了你的生活"></a>7.桥段会毁了你的生活</h5><p>《连线》杂志的布朗（Scott Brown）在谈到“TV Tropes”时发出感慨，认为原创剧情已经消失了。其实也不至于。真正的原创剧情是高雅文学和文艺片的事情，流行文学和商业片只需要“好用的”剧情。评价严肃的作品，往往要看它是不是发明了独一无二的人物和剧情。所以严肃文学作家是科学家，通俗文学作家是工程师。</p><p>可是最好的程序员仍然可以把编程从技术上升到艺术的境界。</p><h5 id="8-健康的经济学"><a href="#8-健康的经济学" class="headerlink" title="8.健康的经济学"></a>8.健康的经济学</h5><p>如果没每加班1小时都一定能使寿命减少5分钟，恐怕就不会有这么多人加班了。但工作时间与健康并不是一个确定的关系，而是一个概率关系。<br>而且统计表明那些工作很轻松，生活无压力的人反而不如努力工作的人长寿。<br>一个选择了高风险高回报的人在健康出问题后应该愿赌服输——再给他们一次机会很可能还是这样选。</p><h5 id="9-核电站能出什么大事"><a href="#9-核电站能出什么大事" class="headerlink" title="9.核电站能出什么大事"></a>9.核电站能出什么大事</h5><p>1.核爆<br>要自爆核电站以报复人类，核电站也不会像原子弹一样爆炸。因为原材料纯度远远不够。事实上，维持核电站反应堆中的链式反应是很不容易的，以至于如果失控，链式反应会立即停止。燃料会继续变热，像日本这样需要灌水冷却，但这种变热不是链式反应，也就是说哪怕你不管了，让燃料自己变冷它也不会发生核爆。<br>核电事故的有害性在于辐射。核电站泄漏对于公众的危害是癌症。<br>2.癌症<br>3.哲学</p><h3 id="Part2-成功学的解药"><a href="#Part2-成功学的解药" class="headerlink" title="Part2 成功学的解药"></a><strong>Part2 成功学的解药</strong></h3><p>我们需要的是科学的励志，只有你的理论有意义，你的成功才可以复制。</p><h4 id="1-科学的励志和励志的科学"><a href="#1-科学的励志和励志的科学" class="headerlink" title="1.科学的励志和励志的科学"></a>1.科学的励志和励志的科学</h4><p>如今有了一些励志书，它们不再依赖名人轶事，而是借助试验和统计。这些书的理论背后都有严肃的学术论文作为依据，它们是几十年来心理学和认知科学的进步的结果。在科学家看来，乔布斯的个性管理也许不根本不值得推广，而扎克伯格的所谓天才霸业，远远比不上一群普通学生在几个月内的整体进步有研究价值。科学家，是励志领域一股拨乱反正的势力。<br>想知道什么品质对成功最重要，科学的办法不是看名人传记，而是进行大规模的统计。<br>真正能左右成绩的品质只有一个：自控。<br> 研究者普遍认为，排除智力因素，不管你心目中的成功是个人成就、家庭幸福还是人际关系，最能决定成功的只有自控。<br>自控需要意志力。意志力其实是一种生理机能。它就好像人的肌肉一样每次使用都需要消耗能量，而且用多了会疲倦。<br>意志力是一种有限资源。<br>如何提高意志力？研究者推断：人的意志力能量来自血液中的葡萄糖。 我们可以想办法合理支配这种资源，甚至像在锻炼肌肉一样增加意志力的容量。<br>除了好习惯可以减少意志力的消耗外，作者提到另外一种重要自控手段是自我监视。<br>真正有效的办法是“常立志”。意志力是一种通用资源，则意味着你可以通过做一些日常的小事来提高意志力，然后把它用在其他事情上。本书提出一个有效的练习办法是做自己不习惯做的事。<br>意志力显然不是人们喜欢自夸的能力。<br>尽管亚裔只占美国人口的4%，亚裔学生却占到斯坦福等顶级名校的25%。但统计表明同样是进入一个高智商的行业，白人需要智商110，而亚裔只需要103。<br>亚裔靠的是意志力。有实验发现，中国的小孩从两岁开始就比美国的小孩有更强的自控能力。不管什么中国文化虽然不怎么擅长科学思维，也不太明白意志力到底是什么，它却在意志力的实践上遥遥领先。</p><h4 id="2-匹夫怎样逆袭"><a href="#2-匹夫怎样逆袭" class="headerlink" title="2.匹夫怎样逆袭"></a>2.匹夫怎样逆袭</h4><p>关键在于两点：<br>第一，    你要知道你的不利条件，在某些情况下可能是你的有利条件；而巨人的所谓有利条件，在某些情况下可能是他的不利条件。<br>第二，    你绝对不能按照对手的打法去跟他玩，你有时候得使用非常规手段。<br>尼采说过，“凡不能使我毁灭的，必使我强大。”<br>既然优势和劣势可以互相转化，我们就不应该一味追求加强某一方面的优势，正所谓过犹不及。维护现有社会格局和强调遵守游戏规则，那是高富帅的事。而改变规则则是匹夫的特权。</p><h4 id="3-练习一万小时成天才？"><a href="#3-练习一万小时成天才？" class="headerlink" title="3.练习一万小时成天才？"></a>3.练习一万小时成天才？</h4><p>强调练习的同时绝对不能否定天赋的重要性。真正的关键根本不是训练时间的长短，而是训练方法。<br>练习，讲究的并不是谁练的最苦，或者谁的心最“诚”。<br>坏消息是高水平训练的成本很高。你需要一位掌握这个领域的先进知识的最好的教练，你需要一个有助于你提高能力的外部环境——这通常意味着加入一所好大学或者入选一个好的俱乐部，你要忍受一点都不舒服的训练方法，而且你需要投入非常多的时间。<br>好消息是各个领域的不同训练方法也都存在着一些共同特征。<br>科学的练习方法并不是从天而降的神秘招式，它一定程度上已经存在于我们的生活之中。它不是科学家的发明，而是科学家对各个领域高手训练方法的总结。人们一直在各个领域中不自觉地使用这些方法。<br>这一套练习方法，就是“刻意练习（deliberate practice）”。<br>去除一些不重要的，总结如下：</p><ol><li>只在“学习区”练习；</li><li>把要训练的内容分成有针对性的小块，对每一小块进行重复训练；</li><li>在整个练习过程中，随时能获得有效反馈；</li><li>练习时注意力必须高度集中。</li></ol><p>几点说明：许多人把困难事情干成认为是靠干事业人某种“内在”品质。比如归于拼搏精神，可是如果是怎么也干不成的事业，归于中国人的素质，作者不能赞同这种凡是往特别简单或者特别复杂了说的思维。首先，干事业不是靠拼命就行，其次，干事业就是干事业，没必要先把官场文化和春秋以来的儒家思想都研究、批判和改造一遍。<br>过分强调“功夫在诗外”这句陆游名言也是不对的。<br>想要成为某一领域的顶尖高手，关键在于“刻意”地在这个领域内，练习。</p><ol><li><p>只在“学习区”学习<br>心理学家把人的知识和技能分为层层嵌套的三个圆形区域：最内一层是“舒适区”，使我们已经熟练掌握的各种技能；最外一层是“恐慌区”，是我们暂时无法学会的技能，二者中间则是“学习区”。<br>有效的练习任务必须在受训者的学习区内进行，它具有高度的针对性。训练者必须随时了解自己最需要改进的地方。一旦学会某个东西，就不应该继续在上面花费时间，应该立即转入下一个困难点。<br>在舒适区做事，叫生活；在学习区做事，才叫练习。<br>脱离舒适区，需要强大的意志力，甚至是一种修炼。<br>每天都给自己定一个更远的目标。</p></li><li><p>掌握套路<br>刻意练习的基础部分：基础训练。也就是套路。<br>人所掌握的知识和技能绝非是零散的信息和随意的动作，它们大多数具有某种“结构”，这些“结构”就是套路。下棋用的定式，编程用的固定算法，这些都是套路。<br>心理学认为人的工作能力主要依靠两种记忆了：“短期记忆力”（short term working memory）和长期记忆力（long term working memory）短期工作记忆类似于电脑内存，是指人脑在同一时刻能够处理的事情的个数——一般来说四个，它与逻辑推理能力、创造性思维有关，也就是智商很有关系，它很难通过训练得到。<br>长期工作记忆储存了我们的知识和技能。它有点类似于计算机硬盘，但比硬盘高级多了。关键在于，长期工作记忆并非是杂乱无章。随便储存的，它是以神经网络的形式运作，必须通过训练才能储存，而且具有高度的结构性。心理学家把这种结构称为“块”（chunk）。<br>人的技能，取决于这两种工作记忆。专家做的事，就是使用有限的短期工作记忆，去调动自己几乎无限的长期工作记忆。而刻意练习，就是在大脑中建立长期工作记忆的过程。</p></li></ol><p>两种套路<br>对于脑力工作者，水平的高低关键要看掌握的套路有多少。<br>以量取胜的套路是容易掌握的。<br>但有些套路，比如那些非纯脑力劳动的非专业技能，想要掌握就没那么容易了。<br>人脑是如何掌握一个技能的，一个比较主流的结论是说这是神经元的作用。完成一个动作需要激发很多神经元，如果这个动作被反复做，那么这些神经元会被反复地一起激发。而神经元有个特点，就是如果经常被一起激发，它们最终会连在一起！！因为每个特定技能需要调动的神经元不同，不同技能在人的大脑就形成了不同的网络结构。另有一个理论则认为神经元的连接必然重要，但更重要的则是包裹在神经元伸出去的神经纤维（轴突）外面的一层髓磷脂组成的膜：髓鞘。如果我们把神经元想象成元器件，那么神经纤维就是连接元器件的导向，而髓鞘则相当与包在导线外面的胶皮、这样用胶皮把电线包起来防止电脉冲外泄，能够使得信号被传输地更强，更快，更准确。当我们正确地练习时，髓鞘就会越包越厚，每多一层就意味着更高的准确度和更快的速度。髓鞘，把小道变成告诉公路。<br>不论是哪种理论，最后我们都可以得出这样的结论：技能是人脑中的一种硬件结构，是“长”在人脑中的。这意味着如果你能打开大脑，你就会发现每个人脑中的神经网络结构都不一样。技能很不容易获得，一旦获得了也很难抹掉。<br>如此一来，高手与普通人就有了本质的区别。高手拥有长期训练获得的特殊神经脑结构，他的一举一动可能都带着不一般气质，连眼神都与众不同，简直是用特殊材料制成的人。练习，是对人体的改造。<br>用什么方法才能迅速把技能套路“长”在身上呢？关键在于两点：</p><ol><li>必须进行大量的重复训练；</li><li>训练必须有高度针对性。</li></ol><ul><li>基本功<br>磨刀不误砍柴工，基本功就是这么重要。不但体育和音乐需要练基本功，就连那些人们认为不存在基本功的领域，也要练基本功。</li><li>重复！重复！再重复！<br>  想把一个动作套路，一个技能，哪怕仅仅是一个生活习惯，甚至是一种心态，“长”在大脑之中，唯一的办法是不断重复。<br>  这种把不常见的高难度事件重复化也是MBA课程的精髓</li><li>高度针对性<br>  想要掌握一项技能，要像运动员一样，需要不停地练习实战动作，不停地比赛，而不是不停地看录像。</li><li>随时获得反馈<br>  从刻意练习角度，这就是即时的反馈（immediate feedback）在有即时反馈的情况下，一个人的进步速度非常之快，而且是实实在在的。</li><li>一定要有反馈<br>  想要真正的理解，唯一的办法是考试和测验。这就是反馈！没有测验，你的知识只是幻觉。</li><li>立即反馈<br>  老师的作用<br>  现代老师的最大作用是什么？及时反馈。一个动作好不好，最好有教练随时指出，本人必须能够随时了解练习结果。看不到结果的练习等于没有练习。在某种程度上，刻意练习是以错误为中心的练习。练习者必须要对错误及其敏感，一旦发现自己错了就会感到非常不舒服，一直练习到改正为止。</li><li>学徒制<br>  作者认为真正的人才不是靠课程、院系、考试大纲培养出来的。培养人才的有效办法只有一个，那就是学徒制。</li><li>刻意练习不好玩<br>  2005年，“刻意练习”概念的提出者埃里克森（K.Anders Ericeeon）领导的小组表明，决定性因素不是学习时间，而是学习环境。<br>  研究人员发现，排除以往成绩的话，只有一个因素能预测他成绩的变化，就是学习环境。</li><li>单独练习<br>  刻意练习需要练习者调动大量的身体和精神资源，全力投入。<br>  研究人员发现，所有学生都了解一个道理：真正决定你水平的不是全班一起上的音乐课，而是单独练习。<br>  所以我们再次发现所谓的“一万小时”实在是个误导人的概念。练习时间长短并不是最重要的，真正关键是你“刻意练习”——哪怕仅仅是“单独练习”——的时间。可见要想成为世界级高手，一定要尽早投入训练，这就是为什么天才音乐学家都是从很小的时候就开始苦练。</li><li>练习与娱乐<br>  在刻意练习中没有“寓教于乐”这个概念。<br>  如果你想学点知识，最好的办法是找本书——最好是正规的教科书或者专业著作——然后老老实实地找个没有人的地方坐下反复读，而且还有自己整理笔记，甚至做习题获得反馈。</li><li>练习需要重复，而重复一定不好玩。<br>  谁愿意练习一万小时？<br>  刻意练习需要学习者精神高度集中，是一种非常艰苦的练习，人的精力只能做这么多。<br>  世界就是属于这极少数人的。世界并不需要一千个钢琴大师或者一万个足球明星，这些少数的幸运儿已经把所有位置都占满了。如果你想要享受快乐童年，你的位置在观众席。<br>  刻意练习不好玩。伟大的成就需要放弃很多很多东西，而这种放弃并不是没有争议的。<br>  我不知道虎妈的育儿法是否对整个社会有力，但我相信虎妈一定明白一个道理：如果你想出类拔萃，那么你要参与的这场竞争很大程度上是个零和博弈——你想赢就意味着有人要输，你拿到这个位置就意味着有人拿不到这个位置。像这种博弈对社会有没有好处对你不重要，你关心的是怎么做对自己有好处。这个博弈没有双赢。<br>  这不是一般人玩的起的游戏。</li><li>孤注一掷<br>  体育、音乐和表演，都是高投入高风险的事情，明星的背后是无数个失败的垫背。想要成功，就得练习一万小时，但考虑到机遇因素，即使你练了这一万小时也未必成功，这其实是一场赌博。<br>  下这么大赌注练习，绝对不仅仅是为了博女朋友一笑，与之对等的回报是整个世界的认可。高水平的运动员有一个共同特点：他们非常，非常，非常非常想赢得比赛。<br>  真正使得乔丹成为巨星的“素质”，是对失败的痛恨。</li><li>奖励机制<br>  一般人当然用不着孤注一掷地刻意练习，但还是需要一点刺激才能练下去，因为只要是有用的练习都好玩。<br>  一个可能的结论似乎是奖励学习过程，比只看学习的结果效果更好。</li><li>兴趣和基因<br>  学习一个技能的初期，智商可能是决定性因素。但是随着学习的深入，兴趣的作用可能就越来越大了，因为兴趣可以相当的大的程度上决定谁能坚持下来。<br>  决定一个学生进步的幅度的不是智商，而是内在动力和学习方法。<br>  科学界的共识是，先天因素远远大于后天因素。<br>  寻找适合自己兴趣的环境，把自己的基因发扬光大——这难道不就是进化论告诉我们的人生的意义吗？</li></ul><h4 id="4-最高级的想象力是不自由的"><a href="#4-最高级的想象力是不自由的" class="headerlink" title="4.最高级的想象力是不自由的"></a>4.最高级的想象力是不自由的</h4><p>所以，最高级的想象力其实是不自由的。正是因为不自由，它的难度才大。自由的“what if”思维，只是高级想象力活动的第一步，其背后不自由的东西才是关键。</p><h4 id="5-思维密集度与牛人的反击"><a href="#5-思维密集度与牛人的反击" class="headerlink" title="5.思维密集度与牛人的反击"></a>5.思维密集度与牛人的反击</h4><p>“思维密集度”=准备这个读物需要的总时间/阅读这个读物需要的时间</p><h4 id="5-上网能避免浅薄么？"><a href="#5-上网能避免浅薄么？" class="headerlink" title="5.上网能避免浅薄么？"></a>5.上网能避免浅薄么？</h4><p>只有有意识的短期记忆，称为工作记忆，才有可能被转化为长期记忆。过去心理学家曾经认为人的工作记忆只能同时容纳7条信息，而最新的研究成果是最多只有2~4条。这样有限的容量非常容易被无关信息干扰导致过载。上网时分散的注意力，不停地为点还是不点做决定，都在阻碍我们把短期记忆升级为知识。<br>上网的关键态度是要成为网络的主人，而不做各种超链接的奴隶。高效率的上网应该像自闭症患者一样具有很强的目的性，以我为主，不被无关信息左右。就算是纯粹为了娱乐上网也无可厚非，这时候读的快就是优点。</p><h4 id="6-高效“冲浪”的办法"><a href="#6-高效“冲浪”的办法" class="headerlink" title="6.高效“冲浪”的办法"></a>6.高效“冲浪”的办法</h4><p>第一步，随便翻翻（toss）<br>效率的首要关键是集中。我们先做的不是读新闻，而是挑选新闻。<br>第二步，略读（skim）<br>第三步，精读（read）<br>这种先集中选择再采取行动的办法并不仅限于新闻阅读，其实有更为广泛的应用。诺贝尔经济学奖得主丹尼尔•卡尼曼在《思考，快与慢》一书中介绍了两个著名的心理学概念：“窄框架”（narrow framing）和“宽框架”（broad framing）。所谓窄框架，就是遇到一个东西做一次决策，一事一议；而宽框架则是把所有东西都摆在桌面上集中选择。</p><h4 id="7-笔记本就是力量"><a href="#7-笔记本就是力量" class="headerlink" title="7.笔记本就是力量"></a>7.笔记本就是力量</h4><p>记笔记，是一个被动的行为。你现在甚至不用自己记，软件的工具可以让你在几次点击之内记录下任何需要记下的信息。但真正的好笔记确实主动的，它不仅仅是对客观事物的记录，更是对自己思想的记录。<br>哪怕有一天维基百科、百度知道再加上人工智能可以向我们提供所有问题的答案，它们仍然不能取代人脑中的真正知识。这是因为真正的知识是分层的。你必须完全理解基础的一层，才能谈得上去看懂上面的一层。如果你没学过微积分，就算有人把人类历史上广义相对论的全部文本摆在你面前也没用。知识，不能仅仅机械地“存”在你脑子里，而必须以一种个性化的结构“长”在你的脑子里。通过个人笔记本来不断总结自己个性化的理解。恰恰可以帮助我们“长”知识。<br>理解知识需要笔记，使用知识也需要笔记。<br>真正的专家，都有自己一整套知识体系。这套体系就如同长在他们心中的一颗不断生枝长叶的树，又如同一张随时变大变复杂的网。每当有新的知识进来，他们都知道该把这个知识放到体系的什么位置上去。有人管这套体系叫做心智模式（mental model），有人管它叫矩阵（matrix）。有了这套体系，你才可能对相关事务作出出神入化的“眨眼判断”，而不是靠什么“灵感”或者“直觉”。<br>所以，记笔记的最直接的目的是为了形成自己的知识体系，改变自己看事物的眼光。<br>笔记系统的一个附带好处是它可以帮助我们把新的知识跟自己已有的知识联系起来。一般人善于发现新事物的不同点，而真正的高手则善于发现共同点。一旦发现新的知识和已有知识的共同点，这个知识就彻底“长”在我们身上了。而且这样带来的类比和联想，特别能刺激创造性思维。只有你把它们全部拆开、撕碎，再重新组合成你的东西，它们才真正属于你。<br>我们要做的就是“吃进”很多信息，然后生产笔记本。</p><p><strong>用强力读书</strong></p><p>读书却可以及大幅度地提升人的思想内力，这种内力是对世界的理解和见识。<br>《如何阅读一本书》这本书提出一种精神：为了娱乐而读，为了信息而读和为了理解而读。首先，只有为了理解某个我们原来不懂的东西而读书，才值得认真对待。其次，读书应该以我为主，而不是以书为主。<br>强力阅读<br>强力阅读更像是一种态度和心法。作者更想起名叫Deep Reading。<br>“强力阅读”并不是为了读《广义相对论》之类的专业著作，它面向的对象是《卧底经济学》之类写给非专业读者的非小说类书籍。称为“强力”，是因为它追求阅读的深度和效率，力图能在一本书中挖掘到最大限度的收获。<br>强力阅读跟“刻意练习”有三个共同点：<br>第一，    不好玩。用非常严肃认真的态度，非得把一本书融会贯通以至于“长”在自己的大脑里不可。<br>第二，    用时少。就如同在那种专门培养天才的最好的音乐学校里，孩子们每天真正练琴的时间绝对不超过2个小时一样。没人能长时间坚持那样的强度，而没有强度•的训练还不如不练。要把精力充沛而又不受打扰的时间段留给最好的书。<br>第三，    不追求块。读书的一个关键技术在于对于不同的读物采取不同的阅读速度。</p><p>以下是强力阅读的具体做法，它的核心技术是读书笔记。<br>新书要读两遍<br>一本书应该被读两遍，而且只读两遍，我们说的思想类书籍，不是什么学术著作。而且最优效率的办法是读完一遍马上再读一遍。<br>第一遍是正常通读，只要放松欣赏作者的精妙思想和有趣故事即可。<br>在读第二遍的同时写下读书笔记。要专注于思想脉络。读一章，记一章笔记。<br>什么是好的读书笔记？<br>强力研读要求读书笔记必须包括四方面内容：</p><ol><li>清晰地表现每一章的逻辑脉络</li><li>带走书中所有的亮点</li><li>有大量的自己的看法心得</li><li>发现这本书和以前读过的其他书或文章的联系<br>重要的是一定要看出作者的逻辑脉络。<br>读书笔记的第一作用就是抛开故事记住文章。让一本书从厚变薄，从具体的山川景色变成抽象的地图。只有当你跳出字里行间，以居高临下的姿态俯视全书，它的脉络才能变得清晰。<br>读书，在某种程度上就是在寻找能够刺激自己思维的那些亮点。强力研读是一种主动的读书方法。要在笔记中写下自己对此书的评论，好像跟作者对话一样。<br>你不可能对说的好的一段话无动于衷。你可以写下自己对这件事的理解，你还可以写下对作者的质疑或肯定。更高级的批注则是写下自己因为这段文字而产生的灵感。一本好书的每一章都能让人迸发出十个以上的灵感。也许它就解决了你之前一直关注的问题——尽管这个问题看似与此书无关；也许你会想把作者的理论往前推一步。这些想法未必有用，但是都非常宝贵，因为如果你不马上记下来，它们很快就会忘记。也许多年以后翻阅笔记的时候你会觉得自己的心得灵感比原书更有价值。</li></ol><p>如果你读的足够多，你会获得一种更难得的经历：感受人类知识的进步。<br>好书之所以读两遍，最重要的目的就是为了获得这些心得、灵感和联系。第一遍是为了陷进去，第二遍是为了跳出来。<br>记笔记是对一本书最大的敬意。读书笔记是一种非常个性化的写作，是个人知识的延伸。它不是书评，它完全是写给自己而不是为了公开发表——可以完全专注于意思，而不必关心文章。<br>所以“眼过千遍不如手过一遍”【心过一遍】，而且用思维导图做笔记是真的没用。</p><p>电子书<br>读书人的武功<br>强力研读要求慢读，但是我们知道很多著名的读书人的读书速度却都很快，这是为什么呢？这就是武功。他们读的快，是因为对他们来说一般的书里，新的东西已经非常有限。<br>比尔盖茨、查理芒格、沃伦巴菲特</p><p>【下面的文章有时间再写，缓慢一段时间】<br>创新是落后者的特权：三个竞争故事<br>————————————————</p>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>攻壳机动队</title>
      <link href="/2017/12/12/%E6%94%BB%E5%A3%B3%E6%9C%BA%E5%8A%A8%E9%98%9F/"/>
      <url>/2017/12/12/%E6%94%BB%E5%A3%B3%E6%9C%BA%E5%8A%A8%E9%98%9F/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/12/攻壳机动队/img12.png" alt=""><br>生死去来<br>棚头傀儡<br>一线断时<br>落落磊磊</p><p>若吾起舞时，<br>丽人亦沉醉。<br>若吾起舞时，<br>皓月亦鸣响。<br>神降合婚夜，<br>破晓虎鸫啼。<br>远神惠赐。</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>极简欧洲史</title>
      <link href="/2017/10/04/%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2/"/>
      <url>/2017/10/04/%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2/</url>
      <content type="html"><![CDATA[<p>“欧洲，为什么老是抢第一？”澳大利亚知名历史学家约翰•赫斯特在本书中的一场引人入胜的探索，为我们梳理出欧洲文明所以能改变全世界的各种特质。</p><p>作者从三大元素：古希腊罗马文化、基督教教义以及日耳曼战士文化开篇，描述了这三大元素如何彼此强化，又相互对立，最终形塑为欧洲文明的内 核；继而在诸多世纪以来催生帝国与城邦，激发征服与十字军东侵，造就出许多性格截然分明的人物──如仁慈的皇帝、好斗的教皇、侠义的骑士，乃至世上第一批享受繁荣和启蒙果实的公民。哲学思维、民主制度的渊源、政治权力的传递、甚至是医学、生物学使用拉丁文的源始，这种种欧洲因素不断形塑了现代文明的各种特质，使欧洲遥遥走在现代世界的前列。</p><p>本书以清晰、幽默、发人深省的笔调，杂以活泼的插图，叙述了一个不同凡响的文明，及其对人类社会的巨大冲击与巨大贡献。<br><a id="more"></a></p>]]></content>
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>星际穿越出现的诗</title>
      <link href="/2017/03/22/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E5%B0%8F%E8%AF%97/"/>
      <url>/2017/03/22/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E5%B0%8F%E8%AF%97/</url>
      <content type="html"><![CDATA[<p><strong>Do not go gentle into that good night</strong><br>-Dylan Thomas </p><p><em>Do not go gentle into that good night,<br>Old age should burn and rave at close of day;  　　<br>Rage, rage against the dying of the light.  　　  　　<br>Though wise men at their end know dark is right,  　　<br>Because their words had forked no lightning they  　　<br>Do not go gentle into that good night.  　　  　　<br>Good men, the last wave by, crying how bright  　　<br>Their frail deeds might have danced in a green bay,  　　<br>Rage, rage against the dying of the light.  　　  　　<br>Wild men who caught and sang the sun in flight,  　　<br>And learn, too late, they grieved it on its way,  　　<br>Do not go gentle into that good night.  　　  　　<br>Grave men, near death, who see with blinding sight  　　<br>Blind eyes could blaze like meteors and be gay,  　　<br>Rage, rage against the dying of the light.  　　  　　<br>And you, my father, there on the sad height,  　　<br>Curse, bless, me now with your fierce tears.,I pray.<br>Do not go gentle into that good night,    　<br>Rage, rage against the dying of the light. </em></p><p>《不要温和地走进那个良夜》</p><p>狄兰·托马斯</p><p>不要温和地走进那个良夜，<br>老年应当在日暮时燃烧咆哮；<br>怒斥，怒斥光明的消逝。</p><p>虽然智慧的人临终时懂得黑暗有理，<br>因为他们的话没有迸发出闪电，他们<br>也并不温和地走进那个良夜。</p><p>善良的人，当最后一浪过去，高呼他们脆弱的善行<br>可能曾会多么光辉地在绿色的海湾里舞蹈，<br>怒斥，怒斥光明的消逝。</p><p>狂暴的人抓住并歌唱过翱翔的太阳，<br>懂得，但为时太晚，他们使太阳在途中悲伤，<br>也并不温和地走进那个良夜。</p><p>严肃的人，接近死亡，用炫目的视觉看出<br>失明的眼睛可以像流星一样闪耀欢欣，<br>怒斥，怒斥光明的消逝。</p><p>你啊，我的父亲。在那悲哀的高处。<br>现在用您的热泪诅咒我，祝福我吧。我求您<br>不要温和地走进那个良夜。<br>怒斥，怒斥光明的消逝。</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>New test</title>
      <link href="/2016/04/25/New%20test/"/>
      <url>/2016/04/25/New%20test/</url>
      <content type="html"><![CDATA[<p>fefefefe<br>abcd这也是一篇测试文章</p><p>假设X是一个离散随机变量，其可能的取值有：$\left\{ x<em>1 ,x_2 ,……,x</em>{n} \right\}$，各个取值对应的概率取值为：$P\left( x_k \right) , k=1,2,……,n $，则其数学期望被定义为：</p><p>$\sum=er_dew11$</p><p>$\{ x_1 ,x_2 ,……,x_n \}$<br>efefewfwefwefwefweefe<br>$P( x_k) , k=1,2,……,n $</p><script type="math/tex; mode=display">efewfw\frac{1}{trhtr7}</script><p><img src="/2016/04/25/New test/img8.jpg" alt=""></p>]]></content>
      
      
        <tags>
            
            <tag> 主题测试 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
